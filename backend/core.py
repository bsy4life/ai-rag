import os
import hashlib

# =========== æ¨¡å¼åˆ¤æ–·èˆ‡ persist ç›®éŒ„ ===========
USE_OLLAMA = os.environ.get("USE_FREE_MODEL", "0") == "1"  # è¨­1å°±ç”¨ Ollama
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data/clear")

if USE_OLLAMA:
    VECTOR_DB_DIR = os.path.join(BASE_DIR, "data/chroma_db_ollama")
    print("ğŸš© [core.py] ä½¿ç”¨æœ¬åœ° Ollama Llama3ï¼Œpersist æ–¼ chroma_db_ollama", flush=True)
else:
    VECTOR_DB_DIR = os.path.join(BASE_DIR, "data/chroma_db_openai")
    print("ğŸš© [core.py] ä½¿ç”¨ OpenAI ä»˜è²»æ¨¡å‹ï¼Œpersist æ–¼ chroma_db_openai", flush=True)

HASH_FILE = os.path.join(VECTOR_DB_DIR, "last_hash.txt")

# =========== å…¨åŸŸè¨˜æ†¶é«”ï¼ˆç”± app ç®¡ç†ï¼‰ ===========
chat_memories = {}
qa_chain = None

# =========== Embedding & LLM å‹•æ…‹åˆ‡æ› ===========
if USE_OLLAMA:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from langchain_community.llms.ollama import Ollama
    OLLAMA_BASE_URL = os.environ.get("OLLAMA_BASE_URL", "http://127.0.0.1:11434")
    embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    llm = Ollama(model="llama3", base_url=OLLAMA_BASE_URL)
else:
    from langchain_openai import OpenAIEmbeddings, ChatOpenAI
    embedding = OpenAIEmbeddings()
    llm = ChatOpenAI(temperature=0)

from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever

def hash_dir(path):
    h = hashlib.md5()
    for dirpath, _, filenames in os.walk(path):
        for fname in sorted(filenames):
            fpath = os.path.join(dirpath, fname)
            try:
                with open(fpath, "rb") as f:
                    h.update(f.read())
            except:
                continue
    return h.hexdigest()

def load_last_hash():
    if not os.path.exists(HASH_FILE):
        return None
    with open(HASH_FILE, "r") as f:
        return f.read().strip()

def save_hash(h):
    os.makedirs(VECTOR_DB_DIR, exist_ok=True)
    with open(HASH_FILE, "w") as f:
        f.write(h)

def build_qa():
    dir_hash = hash_dir(DATA_DIR)
    last_hash = load_last_hash()
    if os.path.exists(VECTOR_DB_DIR) and last_hash == dir_hash:
        print(f"âœ… æª”æ¡ˆæœªç•°å‹•ï¼Œç›´æ¥è¼‰å…¥ Chroma å‘é‡åº« ({VECTOR_DB_DIR})", flush=True)
        vectordb = Chroma(persist_directory=VECTOR_DB_DIR, embedding_function=embedding)
    else:
        print(f"ğŸ”„ æª”æ¡ˆæœ‰ç•°å‹•æˆ–é¦–æ¬¡å•Ÿå‹•ï¼Œé‡å»º Chroma å‘é‡åº« ({VECTOR_DB_DIR})", flush=True)
        loader = DirectoryLoader(DATA_DIR, loader_cls=UnstructuredFileLoader)
        documents = loader.load()
        if not documents:
            print(f"âš ï¸ [build_qa] è³‡æ–™å¤¾ {DATA_DIR} åº•ä¸‹æ²’æœ‰ä»»ä½•æ–‡ä»¶ï¼Œè·³éçŸ¥è­˜åº«å»ºç½®ã€‚", flush=True)
            return None
        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        docs = splitter.split_documents(documents)
        if not docs:
            print(f"âš ï¸ [build_qa] ç¶“éæ‹†åˆ†å¾Œæ²’æœ‰ä»»ä½• chunkï¼Œè·³éçŸ¥è­˜åº«å»ºç½®ã€‚", flush=True)
            return None
        vectordb = Chroma.from_documents(
            docs,
            embedding=embedding,
            persist_directory=VECTOR_DB_DIR
        )
        vectordb.persist()
        save_hash(dir_hash)
        print(f"âœ… Chroma å‘é‡åº«é‡å»ºå®Œæˆä¸¦å·²æŒä¹…åŒ– ({VECTOR_DB_DIR})", flush=True)
    retriever = create_history_aware_retriever(
        llm=llm,
        retriever=vectordb.as_retriever(),
        prompt=ChatPromptTemplate.from_messages([
            ("system", "æ ¹æ“šå°è©±æ­·å²èˆ‡ç•¶å‰å•é¡Œï¼Œè«‹ç”¨å£èªåŒ–çš„æ–¹å¼é‡æ–°è¡¨è¿°ï¼š\n{chat_history}"),
            ("human", "{input}")
        ])
    )
    doc_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯å…§éƒ¨AIåŠ©ç†ï¼Œæ ¹æ“šæ–‡ä»¶å›ç­”å•é¡Œï¼Œè«‹å°‡å…§å®¹ç²¾ç°¡å¾Œå›è¦†ï¼š\n{context}"),
            ("human", "{input}")
        ])
    )
    return create_retrieval_chain(retriever=retriever, combine_docs_chain=doc_chain)

def reload_qa_chain():
    global qa_chain
    try:
        print("ğŸ“‚ åµæ¸¬åˆ°è³‡æ–™æ›´æ–°ï¼Œæº–å‚™é‡å»ºçŸ¥è­˜åº«â€¦", flush=True)
        new_chain = build_qa()
        if new_chain is None:
            print("âš ï¸ reload_qa_chain: build_qa() å›å‚³ Noneï¼Œè³‡æ–™å¤¾å¯èƒ½å°šç„¡æ–‡ä»¶ï¼Œè·³éé‡å»ºã€‚", flush=True)
            return
        qa_chain = new_chain
        chat_memories.clear()
        print("âœ… çŸ¥è­˜åº«é‡å»ºå®Œæˆ", flush=True)
    except Exception as e:
        print(f"âŒ çŸ¥è­˜åº«é‡å»ºå¤±æ•—ï¼š{e}", flush=True)
