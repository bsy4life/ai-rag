# core.py - SanShin AI æ ¸å¿ƒå¼•æ“ï¼ˆæ•´åˆç‰ˆï¼‰
"""
æ•´åˆåŠŸèƒ½ï¼š
- ä¸‰å±¤æª¢ç´¢ï¼šé—œéµå­—ç²¾ç¢ºåŒ¹é… + BM25 + å‘é‡
- åˆ†å±¤ LLMï¼šè¤‡é›œå•é¡Œç”¨ gpt-4oï¼Œç°¡å–®å•é¡Œç”¨ gpt-4o-mini
- Reranker æ”¯æ´ï¼šCohere / æœ¬åœ°æ¨¡å‹
- å®Œæ•´å¿«å–ï¼šTTL + æŒä¹…åŒ–
- æˆæœ¬ä¼°ç®—
- å€‹äººçŸ¥è­˜åº«æ•´åˆ
- æ™ºæ…§å‹è™Ÿè­˜åˆ¥ï¼ˆSMC, VALQUA, ç–åŸºç­‰ï¼‰
"""

import os
import re
import json
import hashlib
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, asdict, field
from datetime import datetime
from threading import Lock
from functools import lru_cache
from enum import Enum

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AI æ¥­å‹™å¼•æ“ï¼ˆç´” AI é©…å‹•çš„ BI æŸ¥è©¢ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from business_ai_engine import get_business_ai_engine
    _HAS_BUSINESS_AI = True
    logger.info("âœ… AI æ¥­å‹™å¼•æ“å·²è¼‰å…¥")
except ImportError:
    _HAS_BUSINESS_AI = False
    logger.warning("âš ï¸ business_ai_engine æœªè¼‰å…¥ï¼Œå°‡ä½¿ç”¨å‚³çµ±è¦å‰‡æŸ¥è©¢")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æŸ¥è©¢å¢å¼·å™¨ï¼ˆå¤šèªè¨€è¡“èªæ“´å±•ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
try:
    from query_enhancer import enhance_query, EnhancedQuery
    _HAS_QUERY_ENHANCER = True
    logger.info("âœ… æŸ¥è©¢å¢å¼·å™¨å·²è¼‰å…¥")
except ImportError:
    _HAS_QUERY_ENHANCER = False
    logger.warning("âš ï¸ query_enhancer æœªè¼‰å…¥ï¼Œå°‡ä½¿ç”¨åŸºæœ¬æŸ¥è©¢æ“´å±•")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM Provider / .env ç›¸å®¹å±¤ï¼ˆé¿å… .env èˆ‡ config.py è®Šæ•¸åä¸åŒæ­¥ï¼‰
# - ä½ çš„ .env ç›®å‰ç”¨ OPENAI_MODEL_* / ANTHROPIC_MODEL_*ï¼Œä½† config.py è®€ LLM_MODEL_*ã€‚
# - é€™è£¡åœ¨ import config ä¹‹å‰æŠŠç’°å¢ƒè®Šæ•¸è£œé½Šï¼Œè®“ config.py ä¸ç”¨æ”¹ä¹Ÿèƒ½åƒåˆ°æ­£ç¢ºæ¨¡å‹ã€‚
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _bootstrap_llm_env() -> None:
    """åœ¨ import config å‰ï¼ŒæŠŠ .env çš„è®Šæ•¸ååšä¸€æ¬¡ã€ç›¸å®¹åŒ–ã€ã€‚

    ç›®æ¨™ï¼š
    1) æ”¯æ´ LLM_PROVIDER=openai / anthropic / auto
    2) auto æ™‚ï¼Œç”¨ LLM_PRIMARY æ±ºå®šã€ä¸»è¦ä¾›æ‡‰å•†ã€ä¾†å¡« LLM_MODEL_*
       ï¼ˆé¿å… config.py è®€åˆ°éŒ¯çš„æ¨¡å‹ï¼‰
    3) docker compose up ä¸€çœ¼çœ‹å¾—å‡ºè·¯ç”±è¨­å®š
    """
    raw_provider = (os.getenv("LLM_PROVIDER") or os.getenv("AI_PROVIDER") or "").strip().lower()

    # å…è¨±çš„ provider: openai / anthropic / auto
    if not raw_provider:
        if os.getenv("ANTHROPIC_API_KEY"):
            raw_provider = "anthropic"
        elif os.getenv("OPENAI_API_KEY"):
            raw_provider = "openai"
        else:
            raw_provider = "openai"

    # çµ±ä¸€ provider åç¨±ï¼Œä½†ä¿ç•™ auto
    provider = raw_provider
    if provider in ("claude", "anthropic"):
        provider = "anthropic"
    elif provider in ("openai", "gpt"):
        provider = "openai"
    elif provider == "auto":
        provider = "auto"
    else:
        # ä¸èªè­˜å°±ç•¶ openaiï¼ˆä½†ä¹Ÿæœƒ log å‡ºä¾†æ–¹ä¾¿ä½ æŠ“ï¼‰
        provider = "openai"

    os.environ["LLM_PROVIDER"] = provider

    def _set_if_missing(k: str, v: str) -> None:
        if v is None:
            return
        v = str(v).strip()
        if v and not os.getenv(k):
            os.environ[k] = v

    def _norm_provider(p: str, default: str) -> str:
        p = (p or default).strip().lower()
        if p in ("claude", "anthropic"):
            return "anthropic"
        if p in ("openai", "gpt"):
            return "openai"
        return default

    # auto æ¨¡å¼ä¸‹ï¼Œç”¨ primary ä¾†æ±ºå®š LLM_MODEL_* æ‡‰è©²å¡«å“ªä¸€å®¶çš„æ¨¡å‹
    if provider == "auto":
        primary = _norm_provider(os.getenv("LLM_PRIMARY"), "anthropic" if os.getenv("ANTHROPIC_API_KEY") else "openai")
        fallback = _norm_provider(os.getenv("LLM_FALLBACK"), "openai" if primary == "anthropic" else "anthropic")
        os.environ["LLM_PRIMARY"] = primary
        os.environ["LLM_FALLBACK"] = fallback
        provider_for_models = primary
    else:
        provider_for_models = provider

    if provider_for_models == "anthropic":
        default_model = os.getenv("ANTHROPIC_MODEL_DEFAULT", "").strip()
        _set_if_missing("LLM_MODEL_DEFAULT", default_model)
        _set_if_missing("LLM_MODEL_SIMPLE", os.getenv("ANTHROPIC_MODEL_SIMPLE", default_model))
        _set_if_missing("LLM_MODEL_COMPLEX", os.getenv("ANTHROPIC_MODEL_COMPLEX", default_model))
        _set_if_missing("LLM_MODEL_BUSINESS", os.getenv("ANTHROPIC_MODEL_BUSINESS", default_model))
        _set_if_missing("LLM_MODEL_PERSONAL", os.getenv("ANTHROPIC_MODEL_PERSONAL", default_model))
    else:
        default_model = os.getenv("OPENAI_MODEL_DEFAULT", "").strip()
        _set_if_missing("LLM_MODEL_DEFAULT", default_model)
        _set_if_missing("LLM_MODEL_SIMPLE", os.getenv("OPENAI_MODEL_SIMPLE", default_model))
        _set_if_missing("LLM_MODEL_COMPLEX", os.getenv("OPENAI_MODEL_COMPLEX", default_model))
        _set_if_missing("LLM_MODEL_BUSINESS", os.getenv("OPENAI_MODEL_BUSINESS", default_model))
        _set_if_missing("LLM_MODEL_PERSONAL", os.getenv("OPENAI_MODEL_PERSONAL", default_model))

    # å•Ÿå‹•æ™‚å°ä¸€æ¬¡è·¯ç”±ç¸½è¡¨ï¼ˆdocker compose up æœƒçœ‹åˆ°ï¼‰
    logger.info(
        "ğŸ¤– LLM è·¯ç”±: provider=%s primary=%s fallback=%s | default=%s | simple=%s | complex=%s | business=%s | personal=%s",
        os.getenv("LLM_PROVIDER"),
        os.getenv("LLM_PRIMARY", "-"),
        os.getenv("LLM_FALLBACK", "-"),
        os.getenv("LLM_MODEL_DEFAULT"),
        os.getenv("LLM_MODEL_SIMPLE"),
        os.getenv("LLM_MODEL_COMPLEX"),
        os.getenv("LLM_MODEL_BUSINESS"),
        os.getenv("LLM_MODEL_PERSONAL"),
    )
_bootstrap_llm_env()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é…ç½®å°å…¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from config import (
    EMBEDDING_MODEL, LLM_CONFIGS, RETRIEVER_CONFIGS,
    PROMPTS, RERANKER_CONFIG, CACHE_CONFIG, SOURCE_TRACKING,
    KEYWORD_PATTERNS, CHINESE_STOPWORDS, COMPLEXITY_THRESHOLDS,
    COMPARISON_KEYWORDS, ANALYSIS_KEYWORDS,
    VECTOR_DB_DIR, BUSINESS_CSV_FILE, MARKDOWN_DIR,
    get_llm_config, get_retriever_config,
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è³‡æ–™çµæ§‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DocumentType(Enum):
    TECHNICAL = "technical"
    BUSINESS = "business"
    PERSONAL = "personal"
    MIXED = "mixed"

@dataclass
class SearchResult:
    """æœå°‹çµæœ"""
    content: str
    source: str
    doc_type: str
    score: float = 0.0
    metadata: Dict = field(default_factory=dict)

@dataclass
class QueryResult:
    """æŸ¥è©¢çµæœ"""
    answer: str
    sources: List[str]
    source_type: str
    images: List[Dict] = field(default_factory=list)
    metadata: Dict = field(default_factory=dict)
    cost_estimate: Dict = field(default_factory=dict)
    from_cache: bool = False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ç”¢å“å‹è™Ÿè­˜åˆ¥ï¼ˆé‡å°ä¸‰ä¿¡ç”¢å“å„ªåŒ–ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# SMC ç”¢å“å‹è™Ÿ
SMC_PATTERNS = [
    r'(?:^|\s)(MXJ\d+[A-Z]*[-\d]*)',
    r'(?:^|\s)(MXH\d+[A-Z]*[-\d]*)',
    r'(?:^|\s)(MXP\d+[A-Z]*[-\d]*)',
    r'(?:^|\s)(LES[A-Z]*\d*[-\d]*)',
    r'(?:^|\s)(LEHZ[A-Z]*\d*[-\d]*)',
    r'(?:^|\s)(LEHF\d+[A-Z]*[-\d]*)',
    r'(?:^|\s)(ACG[A-Z]*\d*[-\d]*)',
    r'(?:^|\s)(ARG[A-Z]*\d*[-\d]*)',
]

# VALQUA (è¯çˆ¾å¡) ç”¢å“å‹è™Ÿ
VALQUA_PATTERNS = [
    r'(?:ãƒãƒ«ã‚«ãƒ¼\s*)?No\.\s*(\d{4}[A-Z]*)',
    r'(?:ãƒãƒ«ã‚«ãƒ¼\s*)?No\.\s*([A-Z]+\d+)',
    r'(?:VALQUA|è¯çˆ¾å¡)\s*No\.\s*(\d+)',
    r'No\.\s*(\d{4,})',
]

# ç–åŸº/å”é‹¼ æ²¹å°
SEAL_PATTERNS = [
    r'(?:^|\s)(G[Ff][Oo][-\s]?\d+[A-Z]*)',
    r'(?:^|\s)(Gf[il][-\s]?\d*[A-Z]*)',
]

def extract_product_models(content: str) -> Dict[str, List[str]]:
    """å¾å…§å®¹ä¸­æå–ç”¢å“å‹è™Ÿï¼ŒæŒ‰å“ç‰Œåˆ†é¡"""
    results = {'smc': [], 'valqua': [], 'seal': [], 'other': []}
    
    for pattern in SMC_PATTERNS:
        matches = re.findall(pattern, content, re.IGNORECASE)
        results['smc'].extend(matches)
    
    for pattern in VALQUA_PATTERNS:
        matches = re.findall(pattern, content, re.IGNORECASE)
        results['valqua'].extend([f"No.{m}" if not m.startswith('No.') else m for m in matches])
    
    for pattern in SEAL_PATTERNS:
        matches = re.findall(pattern, content, re.IGNORECASE)
        results['seal'].extend(matches)
    
    # å»é‡
    for key in results:
        results[key] = list(set(results[key]))
    
    return results

def expand_technical_query(query: str) -> str:
    """æ“´å±•æŠ€è¡“æŸ¥è©¢ï¼Œå¢åŠ åŒç¾©è©å’Œå“ç‰Œé—œéµå­—"""
    expanded = query
    products = extract_product_models(query)
    
    # SMC ç”¢å“æ“´å±•
    if products.get('smc'):
        expanded += " SMC æ°£ç¼¸ é›»å‹•è‡´å‹•å™¨ ç©ºå£“"
    
    # VALQUA ç”¢å“æ“´å±•
    if products.get('valqua'):
        expanded += " VALQUA è¯çˆ¾å¡ å¢Šç‰‡ ãƒãƒ«ã‚«ãƒ¼ gasket"
    
    # æ²¹å°ç”¢å“æ“´å±•
    if products.get('seal'):
        expanded += " ç–åŸº å”é‹¼ æ²¹å° seal"
    
    # é—œéµå­—åŒç¾©è©
    synonyms = {
        'è¦æ ¼': 'ä»•æ§˜ spec specification',
        'å®‰è£': 'å–ä»˜ install mounting è¨­ç½®',
        'å°ºå¯¸': 'å¯¸æ³• dimension size',
        'å¢Šç‰‡': 'gasket ã‚¬ã‚¹ã‚±ãƒƒãƒˆ',
        'æè³ª': 'ææ–™ material',
        'è€ç†±': 'è€ç†±æ€§ heat resistant',
    }
    
    for key, syns in synonyms.items():
        if key in query:
            expanded += f" {syns}"
    
    return expanded

def identify_product_brand(model: str) -> Tuple[str, str]:
    """è­˜åˆ¥ç”¢å“å“ç‰Œå’Œé¡åˆ¥"""
    model_upper = model.upper()
    
    if model_upper.startswith(('MXJ', 'MXH', 'MXP')):
        return 'SMC', 'æ°£ç¼¸'
    if model_upper.startswith(('LES', 'LEH')):
        return 'SMC', 'é›»å‹•è‡´å‹•å™¨'
    if model_upper.startswith(('ACG', 'ARG', 'AWG')):
        return 'SMC', 'æ¿¾æ¸…å™¨/èª¿å£“é–¥'
    if 'NO.' in model_upper:
        return 'VALQUA', 'å¢Šç‰‡/å¡«æ–™'
    if model_upper.startswith('GF'):
        return 'ç–åŸº', 'æ²¹å°'
    
    return 'Unknown', 'Unknown'

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æŸ¥è©¢åˆ†é¡å™¨
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QueryClassifier:
    """æŸ¥è©¢é¡å‹åˆ†é¡å™¨"""
    
    BUSINESS_KEYWORDS = [
        'å®¢æˆ¶', 'æ¥­å‹™', 'æ‹œè¨ª', 'é€è²¨', 'è¨‚å–®', 'ç‡Ÿæ¥­æ‰€',
        'æ´»å‹•', 'æ—¥å ±', 'çµ±è¨ˆ', 'æ¥­ç¸¾', 'æ¥­å‹™å“¡',
    ]
    
    TECHNICAL_KEYWORDS = [
        'è¦æ ¼', 'å‹è™Ÿ', 'ç”¢å“', 'å®‰è£', 'ç¶­ä¿®', 'æ•…éšœ',
        'æ°£ç¼¸', 'æ²¹å£“', 'å¢Šç‰‡', 'é›»ç£é–¥', 'SMC', 'YUKEN',
        'VALQUA', 'è¯çˆ¾å¡', 'No.', 'ç–åŸº', 'å”é‹¼',
    ]
    
    @classmethod
    def classify_query(cls, query: str) -> DocumentType:
        """åˆ†é¡æŸ¥è©¢é¡å‹"""
        query_lower = query.lower()
        
        biz_score = sum(1 for kw in cls.BUSINESS_KEYWORDS if kw in query)
        tech_score = sum(1 for kw in cls.TECHNICAL_KEYWORDS if kw.lower() in query_lower)
        
        # ç”¢å“å‹è™Ÿæª¢æ¸¬
        products = extract_product_models(query)
        if any(products.values()):
            tech_score += 3
        
        if biz_score > tech_score:
            return DocumentType.BUSINESS
        elif tech_score > 0:
            return DocumentType.TECHNICAL
        else:
            return DocumentType.MIXED

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¤‡é›œåº¦è©•ä¼°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def estimate_complexity(query: str, doc_count: int = 0) -> str:
    """è©•ä¼°æŸ¥è©¢è¤‡é›œåº¦ï¼Œæ±ºå®šä½¿ç”¨å“ªå€‹ LLM"""
    is_complex = False
    
    # å•é¡Œé•·åº¦
    if len(query) > COMPLEXITY_THRESHOLDS.get("query_length", 100):
        is_complex = True
    
    # å¤šç”¢å“æ¯”è¼ƒ
    products = extract_product_models(query)
    total_products = sum(len(v) for v in products.values())
    if total_products >= COMPLEXITY_THRESHOLDS.get("model_count", 2):
        is_complex = True
    
    # æ¯”è¼ƒé¡å•é¡Œ
    if any(kw in query for kw in COMPARISON_KEYWORDS):
        is_complex = True
    
    # åˆ†æé¡å•é¡Œ
    if any(kw in query for kw in ANALYSIS_KEYWORDS):
        is_complex = True
    
    # æ–‡æª”æ•¸é‡å¤š
    if doc_count > COMPLEXITY_THRESHOLDS.get("doc_count", 5):
        is_complex = True
    
    return "complex" if is_complex else "simple"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é—œéµå­—ç´¢å¼•ï¼ˆç²¾ç¢ºåŒ¹é…å±¤ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class KeywordIndex:
    """å€’æ’é—œéµå­—ç´¢å¼•ï¼Œç”¨æ–¼ç²¾ç¢ºåŒ¹é…"""
    
    def __init__(self, index_path: str = None):
        self.index: Dict[str, List[Tuple[str, float]]] = {}
        self.doc_keywords: Dict[str, List[str]] = {}
        self.index_path = index_path
        self._lock = Lock()
        
        if index_path and os.path.exists(index_path):
            self.load()
    
    def add_document(self, doc_id: str, text: str, weight: float = 1.0):
        """æ·»åŠ æ–‡ä»¶åˆ°ç´¢å¼•"""
        keywords = self._extract_keywords(text)
        
        with self._lock:
            self.doc_keywords[doc_id] = keywords
            for kw in keywords:
                kw_lower = kw.lower()
                if kw_lower not in self.index:
                    self.index[kw_lower] = []
                
                existing = [i for i, (d, _) in enumerate(self.index[kw_lower]) if d == doc_id]
                if existing:
                    self.index[kw_lower][existing[0]] = (doc_id, weight)
                else:
                    self.index[kw_lower].append((doc_id, weight))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–é—œéµå­—"""
        keywords = set()
        
        # ä½¿ç”¨é…ç½®çš„æ­£å‰‡æ¨¡å¼
        for pattern in KEYWORD_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            keywords.update(m.upper() if len(m) <= 10 else m for m in matches)
        
        # ä¸­æ–‡è©å½™
        chinese = re.findall(r'[\u4e00-\u9fa5]{2,6}', text)
        keywords.update(w for w in chinese if w not in CHINESE_STOPWORDS)
        
        # ç”¢å“å‹è™Ÿ
        products = extract_product_models(text)
        for brand_products in products.values():
            keywords.update(brand_products)
        
        return list(keywords)
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """æœå°‹é—œéµå­—"""
        query_keywords = self._extract_keywords(query)
        query_words = set(query.lower().split())
        all_terms = set(kw.lower() for kw in query_keywords) | query_words
        
        doc_scores: Dict[str, float] = {}
        
        for term in all_terms:
            # ç²¾ç¢ºåŒ¹é…ï¼ˆæ¬Šé‡ 2ï¼‰
            if term in self.index:
                for doc_id, weight in self.index[term]:
                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 2 * weight
            
            # éƒ¨åˆ†åŒ¹é…ï¼ˆæ¬Šé‡ 1ï¼‰
            for indexed_kw, doc_list in self.index.items():
                if len(term) >= 3 and (term in indexed_kw or indexed_kw in term):
                    for doc_id, weight in doc_list:
                        doc_scores[doc_id] = doc_scores.get(doc_id, 0) + weight
        
        results = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def save(self):
        """å„²å­˜ç´¢å¼•"""
        if self.index_path:
            try:
                os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
                with self._lock:
                    data = {
                        "index": {k: list(v) for k, v in self.index.items()},
                        "doc_keywords": self.doc_keywords,
                    }
                    with open(self.index_path, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False)
            except Exception as e:
                logger.warning(f"å„²å­˜é—œéµå­—ç´¢å¼•å¤±æ•—: {e}")
    
    def load(self):
        """è¼‰å…¥ç´¢å¼•"""
        if self.index_path and os.path.exists(self.index_path):
            try:
                with open(self.index_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.index = {k: [tuple(x) for x in v] for k, v in data.get("index", {}).items()}
                    self.doc_keywords = data.get("doc_keywords", {})
            except Exception as e:
                logger.warning(f"è¼‰å…¥é—œéµå­—ç´¢å¼•å¤±æ•—: {e}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æŸ¥è©¢å¿«å–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QueryCache:
    """æŸ¥è©¢å¿«å–ï¼ˆæ”¯æ´ TTL å’ŒæŒä¹…åŒ–ï¼‰"""
    
    def __init__(self, config=CACHE_CONFIG):
        self.config = config
        self.cache: Dict[str, Tuple[Any, float]] = {}
        self._lock = Lock()
        
        if config.backend == "file" and config.file_path:
            os.makedirs(os.path.dirname(config.file_path), exist_ok=True)
            self._load_file_cache()
    
    def _make_key(self, query: str, mode: str, user_id: str = "") -> str:
        return hashlib.md5(f"{query}:{mode}:{user_id}".encode()).hexdigest()
    
    def get(self, query: str, mode: str, user_id: str = "") -> Optional[QueryResult]:
        if not self.config.enabled:
            return None
        
        key = self._make_key(query, mode, user_id)
        
        with self._lock:
            if key in self.cache:
                result, timestamp = self.cache[key]
                if datetime.now().timestamp() - timestamp < self.config.ttl:
                    result.from_cache = True
                    return result
                else:
                    del self.cache[key]
        
        return None
    
    def set(self, query: str, mode: str, result: QueryResult, user_id: str = ""):
        if not self.config.enabled:
            return
        
        key = self._make_key(query, mode, user_id)
        
        with self._lock:
            if len(self.cache) >= self.config.max_size:
                oldest = min(self.cache.items(), key=lambda x: x[1][1])
                del self.cache[oldest[0]]
            
            self.cache[key] = (result, datetime.now().timestamp())
        
        if self.config.backend == "file":
            self._save_file_cache()
    
    def _load_file_cache(self):
        if os.path.exists(self.config.file_path):
            try:
                with open(self.config.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for key, item in data.items():
                        if isinstance(item, list) and len(item) == 2:
                            result_dict, ts = item
                            self.cache[key] = (QueryResult(**result_dict), ts)
            except Exception as e:
                logger.warning(f"è¼‰å…¥å¿«å–å¤±æ•—: {e}")
    
    def _save_file_cache(self):
        try:
            data = {}
            for key, (result, ts) in self.cache.items():
                data[key] = [asdict(result), ts]
            with open(self.config.file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"å„²å­˜å¿«å–å¤±æ•—: {e}")
    
    def clear(self):
        with self._lock:
            self.cache.clear()
        if self.config.backend == "file" and os.path.exists(self.config.file_path):
            try:
                os.remove(self.config.file_path)
            except:
                pass

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Rerankerï¼ˆçµæœé‡æ’ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Reranker:
    """çµæœé‡æ’å™¨ï¼ˆCohere / æœ¬åœ°æ¨¡å‹ï¼‰"""
    
    def __init__(self):
        self._reranker = None
        self._type = None
        self._init_reranker()
    
    def _init_reranker(self):
        if not RERANKER_CONFIG.enabled:
            logger.info("Reranker æœªå•Ÿç”¨")
            return
        
        if RERANKER_CONFIG.type == "cohere":
            try:
                from langchain_cohere import CohereRerank
                api_key = os.getenv(RERANKER_CONFIG.cohere_api_key_env)
                if api_key:
                    self._reranker = CohereRerank(
                        model=RERANKER_CONFIG.cohere_model,
                        top_n=RERANKER_CONFIG.top_n
                    )
                    self._type = "cohere"
                    logger.info("âœ… Cohere Reranker å·²å•Ÿç”¨")
                else:
                    logger.warning("Cohere API Key æœªè¨­å®š")
            except ImportError:
                logger.warning("éœ€è¦å®‰è£ langchain-cohere: pip install langchain-cohere")
        
        elif RERANKER_CONFIG.type == "local":
            try:
                from sentence_transformers import CrossEncoder
                self._reranker = CrossEncoder(RERANKER_CONFIG.local_model)
                self._type = "local"
                logger.info(f"âœ… æœ¬åœ° Reranker å·²å•Ÿç”¨: {RERANKER_CONFIG.local_model}")
            except ImportError:
                logger.warning("éœ€è¦å®‰è£ sentence-transformers")
    
    def rerank(self, query: str, results: List[SearchResult]) -> List[SearchResult]:
        """é‡æ’çµæœ"""
        if not results:
            return results
        
        if not self._reranker:
            return results[:RERANKER_CONFIG.top_n]
        
        try:
            if self._type == "cohere":
                from langchain_core.documents import Document
                docs = [Document(page_content=r.content, metadata=r.metadata) for r in results]
                reranked_docs = self._reranker.compress_documents(docs, query)
                
                reranked = []
                for doc in reranked_docs[:RERANKER_CONFIG.top_n]:
                    for r in results:
                        if r.content == doc.page_content:
                            reranked.append(r)
                            break
                return reranked
            
            else:  # local
                pairs = [(query, r.content) for r in results]
                scores = self._reranker.predict(pairs)
                
                scored = list(zip(results, scores))
                scored.sort(key=lambda x: x[1], reverse=True)
                
                return [r for r, _ in scored[:RERANKER_CONFIG.top_n]]
        
        except Exception as e:
            logger.warning(f"Rerank å¤±æ•—: {e}")
            return results[:RERANKER_CONFIG.top_n]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»è¦ RAG å¼•æ“
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CategorizedQASystem:
    """ä¸‰æ–° AI å•ç­”ç³»çµ±ï¼ˆæ•´åˆç‰ˆï¼‰"""
    
    def __init__(self):
        self._lock = Lock()
        self._initialized = False
        
        # çµ„ä»¶
        self._vectordb = None
        self._bm25 = None
        self._keyword_index = None
        self._llm_cache = {}
        
        # å¿«å–å’Œé‡æ’
        self.cache = QueryCache()
        self.reranker = Reranker()
        
        # å€‹äººçŸ¥è­˜åº«
        self._personal_kb_module = None
        self._load_personal_kb_module()
        
        # æ¥­å‹™æŸ¥è©¢
        self._business_module = None
        self._load_business_module()
        
        # çµ±è¨ˆ
        self.doc_count = 0
        self.file_count = 0
        
        # åˆå§‹åŒ–
        self.initialize()
    
    def _load_personal_kb_module(self):
        """è¼‰å…¥å€‹äººçŸ¥è­˜åº«æ¨¡çµ„"""
        try:
            from personal_kb import get_personal_kb, search_personal
            self._personal_kb_module = {
                'get_kb': get_personal_kb,
                'search': search_personal,
            }
            logger.info("âœ… å€‹äººçŸ¥è­˜åº«æ¨¡çµ„å·²è¼‰å…¥")
        except ImportError as e:
            logger.warning(f"å€‹äººçŸ¥è­˜åº«æ¨¡çµ„æœªè¼‰å…¥: {e}")
    
    def _load_business_module(self):
        """è¼‰å…¥æ¥­å‹™æŸ¥è©¢æ¨¡çµ„"""
        try:
            from business_csv import _direct_business_query_text
            self._business_module = {
                'query': _direct_business_query_text,
            }
            logger.info("âœ… æ¥­å‹™æŸ¥è©¢æ¨¡çµ„å·²è¼‰å…¥")
        except ImportError as e:
            logger.warning(f"æ¥­å‹™æŸ¥è©¢æ¨¡çµ„æœªè¼‰å…¥: {e}")
    
    def initialize(self):
        """åˆå§‹åŒ–ç³»çµ±"""
        if self._initialized:
            return
        
        with self._lock:
            if self._initialized:
                return
            
            logger.info("ğŸ”§ åˆå§‹åŒ– RAG å¼•æ“...")
            
            try:
                self._init_vectordb()
                self._init_bm25()
                self._init_keyword_index()
                
                self._initialized = True
                logger.info("âœ… RAG å¼•æ“åˆå§‹åŒ–å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ RAG å¼•æ“åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_vectordb(self):
        """åˆå§‹åŒ–å‘é‡åº«ï¼ˆè‡ªå‹•å¾ markdown ç›®éŒ„å»ºç«‹ï¼Œæ”¯æ´å¢é‡æ›´æ–°ï¼‰"""
        try:
            from langchain_chroma import Chroma
            from langchain_openai import OpenAIEmbeddings
            from langchain.text_splitter import RecursiveCharacterTextSplitter
            from langchain_core.documents import Document
            
            embedding = OpenAIEmbeddings(model=EMBEDDING_MODEL)
            
            os.makedirs(VECTOR_DB_DIR, exist_ok=True)
            
            self._vectordb = Chroma(
                persist_directory=VECTOR_DB_DIR,
                embedding_function=embedding,
                collection_name="tech_docs",
            )
            
            self.doc_count = self._vectordb._collection.count()
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°å‘é‡åº«
            if os.path.exists(MARKDOWN_DIR):
                if self.doc_count == 0:
                    # å‘é‡åº«ç‚ºç©ºï¼Œå®Œå…¨é‡å»º
                    logger.info(f"ğŸ“‚ å‘é‡åº«ç‚ºç©ºï¼Œé–‹å§‹å¾ {MARKDOWN_DIR} å»ºç«‹...")
                    self._build_vectordb_from_markdown(embedding)
                    self.doc_count = self._vectordb._collection.count()
                else:
                    # å‘é‡åº«ä¸ç‚ºç©ºï¼Œæª¢æŸ¥æ˜¯å¦æœ‰æ–°æª”æ¡ˆéœ€è¦åŠ å…¥
                    self._sync_vectordb_with_markdown(embedding)
                    self.doc_count = self._vectordb._collection.count()
            
            self.file_count = self.doc_count
            logger.info(f"ğŸ“š å‘é‡åº«å·²è¼‰å…¥: {self.doc_count} æ–‡æª”")
            
        except Exception as e:
            logger.error(f"å‘é‡åº«åˆå§‹åŒ–å¤±æ•—: {e}")
            import traceback
            logger.error(traceback.format_exc())
    
    def _sync_vectordb_with_markdown(self, embedding):
        """åŒæ­¥å‘é‡åº«èˆ‡ markdown ç›®éŒ„ï¼ˆå¢é‡æ›´æ–°ï¼‰"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_core.documents import Document
        
        # å–å¾—å‘é‡åº«ä¸­å·²æœ‰çš„æª”æ¡ˆ
        try:
            existing_data = self._vectordb.get()
            existing_sources = set()
            if existing_data and existing_data.get('metadatas'):
                for meta in existing_data['metadatas']:
                    if meta and meta.get('source'):
                        existing_sources.add(meta['source'])
        except Exception as e:
            logger.warning(f"ç„¡æ³•è®€å–ç¾æœ‰å‘é‡åº«: {e}")
            existing_sources = set()
        
        # å–å¾— markdown ç›®éŒ„ä¸­çš„æª”æ¡ˆ
        markdown_files = set()
        for filename in os.listdir(MARKDOWN_DIR):
            if filename.endswith(('.md', '.txt', '.markdown')):
                markdown_files.add(filename)
        
        # æ‰¾å‡ºéœ€è¦æ–°å¢çš„æª”æ¡ˆ
        new_files = markdown_files - existing_sources
        
        if not new_files:
            logger.info("ğŸ“š å‘é‡åº«å·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°")
            return
        
        logger.info(f"ğŸ“‚ ç™¼ç¾ {len(new_files)} å€‹æ–°æª”æ¡ˆéœ€è¦åŠ å…¥å‘é‡åº«")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # å¢åŠ åˆ° 1000ï¼Œä¿ç•™å®Œæ•´æ®µè½
            chunk_overlap=150,    # å¢åŠ é‡ç–Š
            separators=[
                "\n## ", "\n### ", "\n#### ",
                "\nç¬¬ ", "\nä¸€ã€", "\näºŒã€", "\nä¸‰ã€", "\nå››ã€", "\näº”ã€",
                "\n\n", "\n", "ã€‚", ".", " "
            ]
        )
        
        all_docs = []
        
        for filename in new_files:
            filepath = os.path.join(MARKDOWN_DIR, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if not content.strip():
                    continue
                
                chunks = text_splitter.split_text(content)
                
                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "source": filename,
                            "chunk_idx": i,
                            "doc_type": "technical"
                        }
                    )
                    all_docs.append(doc)
                
                logger.info(f"  ğŸ“„ {filename}: {len(chunks)} chunks")
                
            except Exception as e:
                logger.error(f"  âŒ è®€å– {filename} å¤±æ•—: {e}")
        
        if all_docs:
            # åˆ†æ‰¹è™•ç†ï¼Œæ¯æ‰¹ 500 å€‹ chunks
            batch_size = 500
            total = len(all_docs)
            logger.info(f"ğŸ“¤ é–‹å§‹ embedding {total} å€‹æ–° chunks...")
            
            for i in range(0, total, batch_size):
                batch = all_docs[i:i + batch_size]
                self._vectordb.add_documents(batch)
            
            logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: æ–°å¢ {total} chunks")
    
    def _build_vectordb_from_markdown(self, embedding):
        """å¾ markdown ç›®éŒ„å»ºç«‹å‘é‡åº«"""
        from langchain.text_splitter import RecursiveCharacterTextSplitter
        from langchain_core.documents import Document
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,      # å¢åŠ åˆ° 1000ï¼Œä¿ç•™å®Œæ•´æ®µè½
            chunk_overlap=150,    # å¢åŠ é‡ç–Š
            separators=[
                "\n## ", "\n### ", "\n#### ",
                "\nç¬¬ ", "\nä¸€ã€", "\näºŒã€", "\nä¸‰ã€", "\nå››ã€", "\näº”ã€",
                "\n\n", "\n", "ã€‚", ".", " "
            ]
        )
        
        all_docs = []
        
        for filename in os.listdir(MARKDOWN_DIR):
            if not filename.endswith(('.md', '.txt', '.markdown')):
                continue
            
            filepath = os.path.join(MARKDOWN_DIR, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                if not content.strip():
                    continue
                
                chunks = text_splitter.split_text(content)
                
                for i, chunk in enumerate(chunks):
                    doc = Document(
                        page_content=chunk,
                        metadata={
                            "source": filename,
                            "chunk_idx": i,
                            "doc_type": "technical"
                        }
                    )
                    all_docs.append(doc)
                
                logger.info(f"  ğŸ“„ {filename}: {len(chunks)} chunks")
                
            except Exception as e:
                logger.error(f"  âŒ è®€å– {filename} å¤±æ•—: {e}")
        
        if all_docs:
            # åˆ†æ‰¹è™•ç†ï¼Œæ¯æ‰¹ 500 å€‹ chunksï¼ˆé¿å…è¶…é OpenAI token é™åˆ¶ï¼‰
            batch_size = 500
            total = len(all_docs)
            logger.info(f"ğŸ“¤ é–‹å§‹ embedding {total} å€‹ chunksï¼ˆåˆ† {(total + batch_size - 1) // batch_size} æ‰¹ï¼‰...")
            
            for i in range(0, total, batch_size):
                batch = all_docs[i:i + batch_size]
                batch_num = i // batch_size + 1
                logger.info(f"  ğŸ“¦ è™•ç†ç¬¬ {batch_num} æ‰¹: {len(batch)} chunks...")
                self._vectordb.add_documents(batch)
            
            logger.info(f"âœ… å‘é‡åº«å»ºç«‹å®Œæˆ: {total} chunks")
    
    def _init_bm25(self):
        """åˆå§‹åŒ– BM25 æª¢ç´¢å™¨"""
        try:
            from langchain_community.retrievers import BM25Retriever
            
            if self._vectordb:
                docs = self._vectordb.get()
                if docs and docs.get('documents'):
                    from langchain_core.documents import Document
                    bm25_docs = [
                        Document(page_content=d, metadata=m or {})
                        for d, m in zip(
                            docs['documents'], 
                            docs.get('metadatas', [{}] * len(docs['documents']))
                        )
                    ]
                    self._bm25 = BM25Retriever.from_documents(bm25_docs)
                    self._bm25.k = 10
                    logger.info(f"ğŸ“š BM25 æª¢ç´¢å™¨å·²å»ºç«‹: {len(bm25_docs)} æ–‡æª”")
        except ImportError:
            logger.warning("BM25 æœªå•Ÿç”¨ï¼ˆéœ€è¦ rank_bm25ï¼‰")
        except Exception as e:
            logger.warning(f"BM25 åˆå§‹åŒ–å¤±æ•—: {e}")
    
    def _init_keyword_index(self):
        """åˆå§‹åŒ–é—œéµå­—ç´¢å¼•"""
        index_path = os.path.join(VECTOR_DB_DIR, "keyword_index.json")
        self._keyword_index = KeywordIndex(index_path)
        
        # å¦‚æœç´¢å¼•ç‚ºç©ºï¼Œå¾å‘é‡åº«å»ºç«‹
        if not self._keyword_index.index and self._vectordb:
            try:
                docs = self._vectordb.get()
                if docs and docs.get('documents'):
                    for i, (doc_id, content) in enumerate(zip(
                        docs.get('ids', []),
                        docs['documents']
                    )):
                        self._keyword_index.add_document(doc_id, content)
                    self._keyword_index.save()
                    logger.info(f"ğŸ“š é—œéµå­—ç´¢å¼•å·²å»ºç«‹: {len(docs['documents'])} æ–‡æª”")
            except Exception as e:
                logger.warning(f"å»ºç«‹é—œéµå­—ç´¢å¼•å¤±æ•—: {e}")
    @lru_cache(maxsize=12)
    def _get_llm(self, model: str, temperature: float = 0.1, max_tokens: Optional[int] = None):
        """å–å¾— LLMï¼ˆå¿«å–ï¼‰

        éœ€æ±‚ï¼š
        - æ”¯æ´ LLM_PROVIDER=openai / anthropic / auto
        - auto æ™‚ï¼šå…ˆèµ° primaryï¼Œå¤±æ•—å†èµ° fallback
        - æ¯æ¬¡é¸ LLM éƒ½æœƒåœ¨ docker compose log å°ã€èµ°å“ªä¸€æ¢ã€èˆ‡ model
        """

        def _infer_slot(m: str) -> str:
            # ç›¡é‡æŠŠç•¶å‰ model å°æ‡‰åˆ°å“ªå€‹ç”¨é€”ï¼Œè®“ fallback ä¹Ÿèƒ½ç”¨åŒä¸€ç”¨é€”çš„ model
            mapping = [
                ("COMPLEX", os.getenv("LLM_MODEL_COMPLEX")),
                ("SIMPLE", os.getenv("LLM_MODEL_SIMPLE")),
                ("BUSINESS", os.getenv("LLM_MODEL_BUSINESS")),
                ("PERSONAL", os.getenv("LLM_MODEL_PERSONAL")),
                ("DEFAULT", os.getenv("LLM_MODEL_DEFAULT")),
            ]
            for slot, mv in mapping:
                if mv and m == mv:
                    return slot
            return "DEFAULT"

        def _pick_model_for(provider_name: str, slot: str, primary_model: str) -> str:
            # primary ç”¨å‚³é€²ä¾†çš„ modelï¼›fallback ä¾ slot å–å°æ‡‰å®¶çš„ model
            if provider_name == "anthropic":
                return (
                    os.getenv(f"ANTHROPIC_MODEL_{slot}")
                    or os.getenv("ANTHROPIC_MODEL_DEFAULT")
                    or primary_model
                ).strip()
            return (
                os.getenv(f"OPENAI_MODEL_{slot}")
                or os.getenv("OPENAI_MODEL_DEFAULT")
                or primary_model
            ).strip()

        def _build(provider_name: str, model_name: str):
            # æª¢æ¸¬æ˜¯å¦ç‚º o ç³»åˆ—æ¨ç†æ¨¡å‹ï¼ˆo1, o3, gpt-5 ç­‰ï¼‰
            model_lower = model_name.lower()
            is_reasoning_model = any(x in model_lower for x in ['o1', 'o3', 'gpt-5', 'o4'])
            
            if provider_name == "anthropic":
                try:
                    from langchain_anthropic import ChatAnthropic
                except Exception as e:
                    raise RuntimeError("æ‰¾ä¸åˆ° langchain_anthropicï¼Œè«‹åœ¨ requirements å®‰è£ langchain-anthropic") from e

                logger.info("ğŸ§  LLM é¸æ“‡: provider=anthropic model=%s temperature=%s", model_name, temperature)
                kwargs = {"model": model_name, "temperature": temperature}
                if max_tokens is not None:
                    kwargs["max_tokens"] = max_tokens
                return ChatAnthropic(**kwargs)

            # default openai
            try:
                from langchain_openai import ChatOpenAI
            except Exception as e:
                raise RuntimeError("æ‰¾ä¸åˆ° langchain_openaiï¼Œè«‹åœ¨ requirements å®‰è£ langchain-openai") from e

            logger.info("ğŸ§  LLM é¸æ“‡: provider=openai model=%s temperature=%s reasoning=%s", model_name, temperature, is_reasoning_model)
            
            if is_reasoning_model:
                # o ç³»åˆ—æ¨¡å‹ä¸æ”¯æ´ temperature å’Œ max_tokens
                # ä½¿ç”¨ model_kwargs å‚³é max_completion_tokens
                kwargs = {"model": model_name}
                if max_tokens is not None:
                    kwargs["model_kwargs"] = {"max_completion_tokens": max_tokens}
            else:
                kwargs = {"model": model_name, "temperature": temperature}
                if max_tokens is not None:
                    kwargs["max_tokens"] = max_tokens
            
            return ChatOpenAI(**kwargs)

        provider = (os.getenv("LLM_PROVIDER") or "openai").strip().lower()
        # å…è¨±ä½  .env å¯« claude
        if provider == "claude":
            provider = "anthropic"

        slot = _infer_slot(model)

        if provider == "auto":
            primary = (os.getenv("LLM_PRIMARY") or "anthropic").strip().lower()
            fallback = (os.getenv("LLM_FALLBACK") or "openai").strip().lower()
            if primary == "claude":
                primary = "anthropic"
            if fallback == "claude":
                fallback = "anthropic"

            primary_model = _pick_model_for(primary, slot, model)
            fallback_model = _pick_model_for(fallback, slot, model)

            logger.info(
                "ğŸ§­ LLM AUTO: slot=%s primary=%s(%s) fallback=%s(%s)",
                slot, primary, primary_model, fallback, fallback_model
            )

            try:
                llm = _build(primary, primary_model)
                logger.info("âœ… LLM AUTO å‘½ä¸­: primary=%s model=%s", primary, primary_model)
                return llm
            except Exception as e:
                logger.warning("âš ï¸ LLM AUTO primary å¤±æ•—ï¼Œæ”¹èµ° fallbackã€‚primary=%s err=%s", primary, repr(e))
                llm = _build(fallback, fallback_model)
                logger.info("âœ… LLM AUTO å‘½ä¸­: fallback=%s model=%s", fallback, fallback_model)
                return llm

        # é autoï¼šç›´æ¥ä¾ provider å»ºç«‹
        if provider in ("anthropic", "openai"):
            model_name = _pick_model_for(provider, slot, model) if provider != "openai" else model
            # æ³¨æ„ï¼šopenai çš„ model ç›´æ¥ç”¨å‚³é€²ä¾†çš„ï¼ˆå·²ç”± config / LLM_MODEL_* æ±ºå®šï¼‰
            return _build(provider, model_name)

        # ä¸èªè­˜å°±ç•¶ openaiï¼ˆä½†ä¹Ÿæœƒå°å‡ºä¾†ï¼‰
        logger.warning("Unknown LLM_PROVIDER=%s, fallback to openai", provider)
        return _build("openai", model)
    
    def _search(
        self,
        query: str,
        doc_type: str = "technical",
        top_k: int = 10,
    ) -> List[SearchResult]:
        """
        å¢å¼·ç‰ˆä¸‰å±¤æª¢ç´¢ï¼šæŸ¥è©¢å¢å¼· + é—œéµå­— + BM25 + å‘é‡
        
        æ–°åŠŸèƒ½ï¼š
        - ä½¿ç”¨ AI æŸ¥è©¢å¢å¼·å™¨é€²è¡Œå¤šèªè¨€æ“´å±•
        - å°å¤šå€‹æŸ¥è©¢è®Šé«”é€²è¡Œæœç´¢
        - åˆä½µå»é‡å¾Œæ’åº
        """
        results: List[SearchResult] = []
        seen_contents = set()
        
        # ä½¿ç”¨æŸ¥è©¢å¢å¼·å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        search_queries = [query]
        enhanced = None
        
        if doc_type == "technical" and _HAS_QUERY_ENHANCER:
            try:
                # ä½¿ç”¨ LLM å¢å¼·ï¼ˆå¯é…ç½®é—œé–‰ï¼‰
                use_llm = os.getenv("QUERY_ENHANCE_LLM", "true").lower() == "true"
                enhanced = enhance_query(query, use_llm=use_llm)
                
                # ç²å–æ‰€æœ‰æŸ¥è©¢è®Šé«”ï¼ˆé™åˆ¶æ•¸é‡é¿å…å¤ªæ…¢ï¼‰
                all_queries = enhanced.get_all_queries()
                search_queries = all_queries[:5]  # æœ€å¤š 5 å€‹è®Šé«”
                
                logger.info(f"ğŸ” æŸ¥è©¢å¢å¼·: {len(search_queries)} å€‹è®Šé«”")
                
            except Exception as e:
                logger.warning(f"æŸ¥è©¢å¢å¼·å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹æŸ¥è©¢: {e}")
                search_queries = [expand_technical_query(query)]
        else:
            # é™ç´šåˆ°èˆŠçš„æ“´å±•æ–¹å¼
            if doc_type == "technical":
                search_queries = [expand_technical_query(query)]
        
        # å°æ¯å€‹æŸ¥è©¢è®Šé«”é€²è¡Œæœç´¢
        for search_query in search_queries:
            # 1. é—œéµå­—ç²¾ç¢ºåŒ¹é…ï¼ˆåªå°åŸå§‹æŸ¥è©¢å’Œç”¢å“å‹è™Ÿï¼‰
            if self._keyword_index and self._keyword_index.index:
                kw_results = self._keyword_index.search(search_query, top_k=top_k)
                for doc_id, score in kw_results:
                    try:
                        doc_data = self._vectordb._collection.get(ids=[doc_id])
                        if doc_data['documents']:
                            content = doc_data['documents'][0]
                            content_hash = hash(content[:200])
                            if content_hash not in seen_contents:
                                # ç”¢å“å‹è™Ÿç²¾ç¢ºåŒ¹é…åŠ åˆ†
                                bonus = 1.0
                                if enhanced and enhanced.extracted_models:
                                    for model in enhanced.extracted_models:
                                        if model.upper() in content.upper():
                                            bonus = 2.0
                                            break
                                
                                results.append(SearchResult(
                                    content=content,
                                    source=doc_data['metadatas'][0].get('source', '') if doc_data['metadatas'] else '',
                                    doc_type=doc_type,
                                    score=score * 2 * bonus,
                                    metadata={"match_type": "keyword", "doc_id": doc_id}
                                ))
                                seen_contents.add(content_hash)
                    except:
                        pass
            
            # 2. BM25 æœå°‹
            if self._bm25:
                try:
                    bm25_docs = self._bm25.invoke(search_query)
                    for doc in bm25_docs[:top_k]:
                        content_hash = hash(doc.page_content[:200])
                        if content_hash not in seen_contents:
                            results.append(SearchResult(
                                content=doc.page_content,
                                source=doc.metadata.get("source", ""),
                                doc_type=doc_type,
                                score=1.5,
                                metadata={"match_type": "bm25", "query_variant": search_query[:50]}
                            ))
                            seen_contents.add(content_hash)
                except Exception as e:
                    logger.warning(f"BM25 æœå°‹å¤±æ•—: {e}")
            
            # 3. å‘é‡æœå°‹
            if self._vectordb:
                try:
                    config = get_retriever_config(doc_type)
                    vector_docs = self._vectordb.similarity_search_with_score(
                        search_query, k=config.k
                    )
                    for doc, score in vector_docs:
                        content_hash = hash(doc.page_content[:200])
                        if content_hash not in seen_contents:
                            results.append(SearchResult(
                                content=doc.page_content,
                                source=doc.metadata.get("source", ""),
                                doc_type=doc_type,
                                score=1.0 / (1.0 + score),
                                metadata={"match_type": "vector", **doc.metadata}
                            ))
                            seen_contents.add(content_hash)
                except Exception as e:
                    logger.warning(f"å‘é‡æœå°‹å¤±æ•—: {e}")
        
        # 4. Rerankï¼ˆä½¿ç”¨åŸå§‹æŸ¥è©¢ï¼‰
        if results:
            results = self.reranker.rerank(query, results)
        
        logger.info(f"ğŸ“Š æœç´¢çµæœ: {len(results)} ç­†ï¼ˆå»é‡å¾Œï¼‰")
        
        return results[:top_k]
    
    def _generate_answer(
        self,
        query: str,
        context: List[SearchResult],
        doc_type: str = "technical",
    ) -> Tuple[str, Dict]:
        """ç”Ÿæˆå›ç­”"""
        # è©•ä¼°è¤‡é›œåº¦ï¼Œé¸æ“‡ LLM
        complexity = estimate_complexity(query, len(context))
        llm_config = get_llm_config(doc_type, complexity)
        llm = self._get_llm(llm_config.model, llm_config.temperature)
        
        # æº–å‚™ä¸Šä¸‹æ–‡ï¼ˆæ¸…ç† HTML æ¨™ç±¤å’Œåœ–ç‰‡è·¯å¾‘ï¼‰
        def clean_content(content: str) -> str:
            """æ¸…ç†æ–‡æª”å…§å®¹ï¼Œç§»é™¤å¹²æ“¾ LLM çš„å…ƒç´ """
            import re
            # ç§»é™¤ img æ¨™ç±¤
            content = re.sub(r'<img[^>]*>', '', content)
            # ç§»é™¤ style å±¬æ€§
            content = re.sub(r'\s*style="[^"]*"', '', content)
            # ç§»é™¤ç©ºçš„ HTML æ¨™ç±¤
            content = re.sub(r'<(\w+)[^>]*>\s*</\1>', '', content)
            # ç§»é™¤ blockquote æ¨™ç±¤ä½†ä¿ç•™å…§å®¹
            content = re.sub(r'</?blockquote>', '', content)
            # ç§»é™¤ table ç›¸é—œæ¨™ç±¤ä½†å˜—è©¦ä¿ç•™çµæ§‹
            content = re.sub(r'</?table[^>]*>', '\n', content)
            content = re.sub(r'</?thead[^>]*>', '', content)
            content = re.sub(r'</?tbody[^>]*>', '', content)
            content = re.sub(r'</?colgroup[^>]*>', '', content)
            content = re.sub(r'<col[^>]*/?>', '', content)
            content = re.sub(r'<tr[^>]*>', '\n', content)
            content = re.sub(r'</tr>', '', content)
            content = re.sub(r'<t[hd][^>]*>', ' | ', content)
            content = re.sub(r'</t[hd]>', '', content)
            # ç§»é™¤å…¶ä»–å¸¸è¦‹ HTML æ¨™ç±¤
            content = re.sub(r'</?p>', '\n', content)
            content = re.sub(r'</?div[^>]*>', '\n', content)
            content = re.sub(r'</?span[^>]*>', '', content)
            # ç§»é™¤é€£çºŒçš„ ### æ¨™è¨˜
            content = re.sub(r'#{4,}', '', content)
            # ç§»é™¤åœ–ç‰‡è·¯å¾‘
            content = re.sub(r'/home/aiuser/[^\s]+\.(png|jpg|jpeg|gif)', '[åœ–ç‰‡]', content)
            # æ¸…ç†å¤šé¤˜ç©ºè¡Œ
            content = re.sub(r'\n{3,}', '\n\n', content)
            content = re.sub(r'[ \t]+', ' ', content)
            return content.strip()
        
        context_text = "\n\n---\n\n".join([
            f"ã€ä¾†æº: {os.path.basename(r.source) if r.source else 'æœªçŸ¥'}ã€‘\n{clean_content(r.content)}"
            for r in context
        ])
        
        # é¸æ“‡ Prompt
        prompt_template = PROMPTS.get(doc_type, PROMPTS["technical"])
        prompt = prompt_template.format(context=context_text, input=query)
        
        # ç”Ÿæˆå›ç­”ï¼ˆå¸¶ fallbackï¼‰
        used_model = llm_config.model
        try:
            response = llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
        except Exception as e:
            error_msg = str(e)
            # æª¢æŸ¥æ˜¯å¦æ˜¯ API é™é¡æˆ–é…é¡éŒ¯èª¤
            if "usage limits" in error_msg or "quota" in error_msg.lower() or "rate limit" in error_msg.lower():
                logger.warning(f"âš ï¸ LLM èª¿ç”¨å¤±æ•—ï¼ˆé™é¡ï¼‰ï¼Œå˜—è©¦ fallback: {e}")
                # å˜—è©¦ fallback åˆ°å¦ä¸€å€‹æä¾›å•†
                try:
                    provider = (os.getenv("LLM_PROVIDER") or "openai").strip().lower()
                    if provider == "auto":
                        fallback_provider = (os.getenv("LLM_FALLBACK") or "openai").strip().lower()
                    else:
                        fallback_provider = "openai" if provider in ("anthropic", "claude") else "anthropic"
                    
                    # é¸æ“‡ fallback æ¨¡å‹
                    if fallback_provider == "openai":
                        fallback_model = os.getenv("OPENAI_MODEL_COMPLEX", "gpt-4o")
                    else:
                        fallback_model = os.getenv("ANTHROPIC_MODEL_COMPLEX", "claude-sonnet-4-20250514")
                    
                    logger.info(f"ğŸ”„ Fallback åˆ° {fallback_provider}: {fallback_model}")
                    fallback_llm = self._get_llm(fallback_model, llm_config.temperature)
                    response = fallback_llm.invoke(prompt)
                    answer = response.content if hasattr(response, 'content') else str(response)
                    used_model = fallback_model
                    logger.info(f"âœ… Fallback æˆåŠŸ: {fallback_provider}")
                except Exception as fallback_e:
                    logger.error(f"âŒ Fallback ä¹Ÿå¤±æ•—: {fallback_e}")
                    answer = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼šä¸»è¦ LLM é™é¡ï¼Œå‚™ç”¨ LLM ä¹Ÿå¤±æ•—ã€‚è«‹ç¨å¾Œé‡è©¦ã€‚"
            else:
                logger.error(f"LLM ç”Ÿæˆå¤±æ•—: {e}")
                answer = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
        
        # æ·»åŠ ä¾†æº
        if SOURCE_TRACKING.enabled and SOURCE_TRACKING.show_in_response:
            sources = list(set(
                os.path.basename(r.source) for r in context if r.source
            ))[:SOURCE_TRACKING.max_sources]
            if sources:
                answer += "\n\n---\nğŸ“š **åƒè€ƒä¾†æº**ï¼š" + "ã€".join(sources)
        
        # æˆæœ¬ä¼°ç®—
        from config import TOKEN_PRICES
        input_tokens = len(prompt) // 2
        output_tokens = len(answer) // 2
        prices = TOKEN_PRICES.get(llm_config.model, {"input": 0.15, "output": 0.6})
        
        # ğŸ†• Debugï¼šå›å‚³æœ¬æ¬¡å¯¦éš›ä½¿ç”¨çš„ provider / modelï¼ˆæ–¹ä¾¿ç¢ºèª Claude vs OpenAIï¼‰
        used_provider = os.getenv("LLM_PROVIDER", "openai").strip().lower() or "openai"

        cost = {
            "model": llm_config.model,
            "used_provider": used_provider,
            "used_model": llm_config.model,
            "complexity": complexity,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "estimated_cost_usd": round(
                (input_tokens / 1_000_000) * prices.get("input", 0.15) +
                (output_tokens / 1_000_000) * prices.get("output", 0.6),
                6
            ),
        }
        
        return answer, cost
    
    def ask(
        self,
        query: str,
        mode: str = "smart",
        user_id: str = "default",
    ) -> Tuple[str, str, Dict]:
        """
        ä¸»è¦å•ç­”æ¥å£
        
        Args:
            query: æŸ¥è©¢å…§å®¹
            mode: smart, technical, business, personal
            user_id: ç”¨æˆ¶ ID
        
        Returns:
            (answer, source_type, info_dict)
        """
        if not query or not query.strip():
            return "è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œã€‚", "error", {}
        
        query = query.strip()
        
        # æª¢æŸ¥å¿«å–
        cached = self.cache.get(query, mode, user_id)
        if cached:
            return cached.answer, cached.source_type, {
                "images": cached.images,
                "sources": cached.sources,
                "cost_estimate": cached.cost_estimate,
                "from_cache": True,
            }
        
        # Smart æ¨¡å¼ï¼šå…ˆæª¢æŸ¥å€‹äººçŸ¥è­˜åº«
        if mode == "smart" and user_id and user_id != "default":
            personal_result = self._ask_personal(query, user_id)
            if personal_result and "æœªæ‰¾åˆ°" not in personal_result[0] and "å°šç„¡æ–‡ä»¶" not in personal_result[0]:
                # å¿«å–çµæœ
                result = QueryResult(
                    answer=personal_result[0],
                    sources=personal_result[2].get("sources", []),
                    source_type="personal",
                    images=personal_result[2].get("images", []),
                )
                self.cache.set(query, mode, result, user_id)
                return personal_result
        
        # åˆ¤æ–·æŸ¥è©¢é¡å‹
        if mode == "smart":
            doc_type = QueryClassifier.classify_query(query)
        elif mode == "personal":
            return self._ask_personal(query, user_id)
        else:
            doc_type = DocumentType(mode) if mode in ["technical", "business"] else DocumentType.TECHNICAL
        
        # æ¥­å‹™æŸ¥è©¢
        if doc_type == DocumentType.BUSINESS:
            return self._ask_business(query)
        
        # æŠ€è¡“æŸ¥è©¢ï¼šä¸‰å±¤æª¢ç´¢
        search_results = self._search(query, doc_type.value)
        
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚è«‹å˜—è©¦ä¸åŒçš„é—œéµå­—ã€‚", doc_type.value, {}
        
        # ç”Ÿæˆå›ç­”
        answer, cost = self._generate_answer(query, search_results, doc_type.value)
        
        info = {
            "sources": [r.source for r in search_results if r.source],
            "cost_estimate": cost,
            "from_cache": False,
        }
        
        # å¿«å–çµæœ
        result = QueryResult(
            answer=answer,
            sources=info["sources"],
            source_type=doc_type.value,
            cost_estimate=cost,
        )
        self.cache.set(query, mode, result, user_id)
        
        return answer, doc_type.value, info
    
    def _ask_business(self, query: str) -> Tuple[str, str, Dict]:
        """
        æ¥­å‹™æŸ¥è©¢ï¼ˆAI é©…å‹• + å‚³çµ±å›é€€ï¼‰
        
        æ¨¡å¼é¸æ“‡ï¼š
        - BUSINESS_QUERY_MODE=ai  â†’ ä½¿ç”¨ AI å¼•æ“ï¼ˆé è¨­ï¼‰
        - BUSINESS_QUERY_MODE=legacy â†’ ä½¿ç”¨å‚³çµ±è¦å‰‡
        """
        use_ai = os.getenv("BUSINESS_QUERY_MODE", "ai").lower() == "ai"
        
        # å˜—è©¦ AI æ¨¡å¼
        if use_ai and _HAS_BUSINESS_AI:
            try:
                engine = get_business_ai_engine()
                result = engine.query(query)
                
                if result.get("success"):
                    return (
                        result.get("answer", "æŸ¥è©¢å®Œæˆ"),
                        "business",
                        {
                            "sources": ["business_ai"],
                            "insights": result.get("insights", []),
                            "recommendations": result.get("recommendations", []),
                            "visualizations": result.get("visualizations", []),
                            "data_summary": result.get("data_summary", {}),
                        }
                    )
                else:
                    # AI è¿”å›å¤±æ•—ï¼Œå›é€€åˆ°å‚³çµ±
                    logger.warning("AI æ¥­å‹™æŸ¥è©¢è¿”å›å¤±æ•—ï¼Œå˜—è©¦å‚³çµ±æ–¹å¼")
            except Exception as e:
                logger.error(f"AI æ¥­å‹™æŸ¥è©¢ç•°å¸¸: {e}ï¼Œå›é€€åˆ°å‚³çµ±æ–¹å¼")
        
        # å‚³çµ±æ¨¡å¼ï¼ˆå›é€€ï¼‰
        return self._ask_business_legacy(query)
    
    def _ask_business_legacy(self, query: str) -> Tuple[str, str, Dict]:
        """æ¥­å‹™æŸ¥è©¢ - å‚³çµ±è¦å‰‡ç‰ˆæœ¬"""
        if not self._business_module:
            return "æ¥­å‹™æŸ¥è©¢æ¨¡çµ„æœªè¼‰å…¥ã€‚", "error", {}
        
        try:
            answer = self._business_module['query'](query)
            if answer:
                return answer, "business", {"sources": ["business_csv"]}
            else:
                return "æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ¥­å‹™è¨˜éŒ„ã€‚", "business", {}
        except Exception as e:
            logger.error(f"æ¥­å‹™æŸ¥è©¢å¤±æ•—: {e}")
            return f"æ¥­å‹™æŸ¥è©¢å¤±æ•—ï¼š{e}", "error", {}
    
    def _ask_personal(self, query: str, user_id: str) -> Tuple[str, str, Dict]:
        """å€‹äººçŸ¥è­˜åº«æŸ¥è©¢"""
        if not self._personal_kb_module:
            return "å€‹äººçŸ¥è­˜åº«åŠŸèƒ½æœªå•Ÿç”¨ã€‚", "error", {}
        
        try:
            kb = self._personal_kb_module['get_kb'](user_id)
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶
            stats = kb.metadata.get("stats", {})
            if stats.get("total_documents", 0) == 0:
                return "æ‚¨çš„å€‹äººçŸ¥è­˜åº«å°šç„¡æ–‡ä»¶ã€‚è«‹å…ˆä¸Šå‚³æ–‡ä»¶ã€‚", "personal", {}
            
            # æœå°‹
            results = self._personal_kb_module['search'](user_id, query, top_k=5)
            
            if not results:
                return "åœ¨å€‹äººçŸ¥è­˜åº«ä¸­æœªæ‰¾åˆ°ç›¸é—œå…§å®¹ã€‚", "personal", {}
            
            # çµ„è£çµæœ
            all_images = []
            context_parts = []
            
            for r in results:
                context_parts.append(f"ã€{r.filename}ã€‘\n{r.content}")
                
                for img in getattr(r, 'images', [])[:2]:
                    all_images.append({
                        "doc_id": r.doc_id,
                        "filename": r.filename,
                        "image_name": img.get("name"),
                        "url": f"/kb/personal/{user_id}/images/{r.doc_id}/{img.get('name')}",
                    })

            # ğŸ§  åœ–ç‰‡å›å‚³ç­–ç•¥ï¼šåªæœ‰ã€Œéœ€è¦åœ–ã€æ‰å›å‚³ï¼Œé¿å…åƒæŸ¥åˆ†æ©Ÿè¡¨å»è¢«å¡åœ–
            def _should_return_images(q: str) -> bool:
                q = (q or "").lower()
                keywords = [
                    "åœ–", "åœ–ç‰‡", "æˆªåœ–", "ç•«é¢", "ä»‹é¢", "ç•«é¢é•·ä»€éº¼æ¨£", "ç¤ºæ„", "ç¯„ä¾‹", "æ­¥é©Ÿ", "è¨­å®š", "æ€éº¼åš",
                    "screenshot", "image", "diagram", "ui", "gui",
                ]
                return any(k.lower() in q for k in keywords)

            return_images = bool(all_images) and _should_return_images(query)
            
            # ç”Ÿæˆå›ç­”
            context = "\n\n---\n\n".join(context_parts)
            # ğŸ†• Personal KB ç›®å‰æ²¿ç”¨æ—¢æœ‰é è¨­æ¨¡å‹ï¼›ä¸¦å›å‚³ used_provider/used_model ä¾›å‰ç«¯é¡¯ç¤º
            personal_model = os.getenv("LLM_MODEL_PERSONAL", "gpt-4o-mini").strip() or "gpt-4o-mini"
            used_provider = os.getenv("LLM_PROVIDER", "openai").strip().lower() or "openai"

            llm = self._get_llm(personal_model, 0.1)
            
            prompt = PROMPTS.get("personal", PROMPTS["technical"]).format(
                context=context, input=query
            )
            response = llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
            
            if return_images:
                answer += f"\n\nğŸ“· æ‰¾åˆ° {len(all_images)} å¼µç›¸é—œåœ–ç‰‡ã€‚"
            
            return answer, "personal", {
                "sources": [r.filename for r in results],
                "images": all_images if return_images else [],
                "used_provider": used_provider,
                "used_model": personal_model,
            }
            
        except Exception as e:
            logger.error(f"å€‹äººçŸ¥è­˜åº«æŸ¥è©¢å¤±æ•—: {e}")
            return f"æŸ¥è©¢å¤±æ•—ï¼š{e}", "error", {}
    
    def reload(self) -> bool:
        """é‡æ–°è¼‰å…¥ç³»çµ±"""
        with self._lock:
            self._initialized = False
            self._vectordb = None
            self._bm25 = None
            self._keyword_index = None
            self._get_llm.cache_clear()
        
        self.initialize()
        self.cache.clear()
        logger.info("âœ… ç³»çµ±å·²é‡æ–°è¼‰å…¥")
        return True
    
    def get_stats(self) -> Dict:
        """å–å¾—ç³»çµ±çµ±è¨ˆ"""
        return {
            "initialized": self._initialized,
            "vectordb_loaded": self._vectordb is not None,
            "vectordb_count": self.doc_count,
            "bm25_loaded": self._bm25 is not None,
            "keyword_index_loaded": self._keyword_index is not None and bool(self._keyword_index.index),
            "reranker_enabled": RERANKER_CONFIG.enabled,
            "cache_enabled": CACHE_CONFIG.enabled,
            "cache_size": len(self.cache.cache),
            "personal_kb_available": self._personal_kb_module is not None,
            "business_query_available": self._business_module is not None,
        }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¨åŸŸå¯¦ä¾‹å’Œä¾¿æ·å‡½æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_qa_system: Optional[CategorizedQASystem] = None
_qa_system_lock = Lock()

def get_qa_system() -> CategorizedQASystem:
    """å–å¾—å•ç­”ç³»çµ±å¯¦ä¾‹ï¼ˆå–®ä¾‹ï¼‰"""
    global _qa_system
    
    if _qa_system is None:
        with _qa_system_lock:
            if _qa_system is None:
                _qa_system = CategorizedQASystem()
    
    return _qa_system

def reload_qa_system() -> bool:
    """é‡æ–°è¼‰å…¥å•ç­”ç³»çµ±"""
    return get_qa_system().reload()

# å‘å¾Œå…¼å®¹
def get_engine():
    return get_qa_system()

def reload_engine():
    return reload_qa_system()