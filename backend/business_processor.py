#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
business_processor.py - æ¥­å‹™æ—¥å ±è™•ç†å™¨ï¼ˆæ•´åˆç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. è§£æ Lotus Notes åŒ¯å‡ºçš„æ¥­å‹™æ—¥å ± TXT
2. æ—¥æœŸéæ¿¾ï¼šåªä¿ç•™æœ€è¿‘ N å€‹æœˆçš„è³‡æ–™
3. å¢é‡æ›´æ–°ï¼šèˆ‡ç¾æœ‰ CSV åˆä½µï¼Œé¿å…é‡è¤‡
4. è‡ªå‹•è§¸ç™¼å‘é‡åº«é‡å»º

ä½¿ç”¨æ–¹å¼ï¼š
- CLI: python business_processor.py input.txt -m 12 -o ./data/business/
- API: é€é /knowledge/upload-business ç«¯é»
"""

from __future__ import annotations
import argparse
import re
import os
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta
from dateutil.relativedelta import relativedelta

try:
    import pandas as pd
except ImportError:
    raise ImportError("éœ€è¦å¥—ä»¶ pandas: pip install pandas")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¨­å®š
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# é è¨­ä¿ç•™æœˆæ•¸
DEFAULT_MONTHS_TO_KEEP = 12

# æ¬„ä½æ˜ å°„
KEY_MAP = {
    "date": "Date", "æ—¥æœŸ": "Date",
    "worker": "Worker", "å“¡å·¥": "Worker",
    "customer": "Customer", "å®¢æˆ¶": "Customer",
    "class": "Class", "æ´»å‹•é¡å‹": "Class",
    "content": "Content", "æ´»å‹•å…§å®¹": "Content",
    "depart": "Depart", "éƒ¨é–€": "Depart",
    "manager": "Manager", "ä¸»ç®¡": "Manager",
    "level": "Level",
    "doc_st": "Doc_Status", "æ–‡ä»¶ç‹€æ…‹": "Doc_Status",
    "timecreated": "TimeCreated",
    "doc_time": "Doc_Time",
    "$updatedby": "$UpdatedBy",
}

TARGET_COLS = [
    "Date", "Worker", "Customer", "Class", "Content", "Depart",
    "Manager", "Level", "Doc_Status", "TimeCreated", "Doc_Time"
]

RE_KEY_VALUE = re.compile(r"^([A-Za-z0-9_\-$\u4e00-\u9fa5]+)\s*[:ï¼š]\s*(.*)$")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è§£æå·¥å…·
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def normalize_key(k: str) -> str:
    """æ­£è¦åŒ–æ¬„ä½åç¨±"""
    k = k.strip()
    k_l = k.lower()
    if k_l in KEY_MAP:
        return KEY_MAP[k_l]
    if k in KEY_MAP:
        return KEY_MAP[k]
    if k in TARGET_COLS or k in ["$UpdatedBy", "SubManager"]:
        return k
    return k

def normalize_date(s: str) -> str:
    """å°‡æ—¥æœŸæ­£è¦åŒ–ç‚º YYYY/MM/DD"""
    s = (s or "").strip()
    if not s:
        return s
    m = re.search(r"(\d{4})[/-](\d{1,2})[/-](\d{1,2})", s)
    if m:
        y, mo, d = m.groups()
        return f"{int(y):04d}/{int(mo):02d}/{int(d):02d}"
    return s

def parse_date(s: str) -> Optional[datetime]:
    """è§£ææ—¥æœŸå­—ä¸²ç‚º datetime"""
    s = normalize_date(s)
    if not s:
        return None
    try:
        return datetime.strptime(s, "%Y/%m/%d")
    except:
        return None

def normalize_class(s: str) -> str:
    """æ¸…æ´—æ´»å‹•é¡å‹"""
    s = (s or "").strip()
    if not s:
        return s
    s = s.replace("ï¼Œ", ",").replace("ã€", ",").replace(" ", "")
    parts = [p for p in re.split(r"[,\s]+", s) if p]
    return ", ".join(parts)

def extract_cn_name(s: str) -> str:
    """å¾ CN=åå­—/O=Org æ ¼å¼æŠ½å‡ºåå­—"""
    if not isinstance(s, str):
        return s
    m = re.search(r"CN=([^/ï¼Œ,ï¼›;\s]+)", s)
    return m.group(1) if m else s

def compute_record_hash(rec: Dict[str, str]) -> str:
    """è¨ˆç®—è¨˜éŒ„çš„å”¯ä¸€ hashï¼ˆç”¨æ–¼å»é‡ï¼‰"""
    key_fields = ["Date", "Worker", "Customer", "Content", "TimeCreated"]
    key_str = "|".join(str(rec.get(f, "")) for f in key_fields)
    return hashlib.md5(key_str.encode()).hexdigest()[:12]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¨˜éŒ„åˆ‡åˆ†èˆ‡è§£æ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def split_records(text: str) -> List[str]:
    """å°‡æ–‡å­—åˆ‡åˆ†ç‚ºå¤šç­†è¨˜éŒ„"""
    # å„ªå…ˆä½¿ç”¨æ›é ç¬¦
    if "\f" in text:
        blocks = re.split(r"\f+", text)
        return [b for b in blocks if b.strip()]
    
    # é€€è·¯ï¼šä»¥ Doc_Time æˆ– Date è¡Œç‚ºåˆ†æ®µ
    lines = text.splitlines()
    blocks, curr = [], []
    rec_start_re = re.compile(r"^\s*(Doc_Time|Date)\s*[:ï¼š]")
    
    for ln in lines:
        if rec_start_re.search(ln) and curr:
            blocks.append("\n".join(curr))
            curr = [ln]
        else:
            curr.append(ln)
    if curr:
        blocks.append("\n".join(curr))
    
    return [b for b in blocks if b.strip()]

def parse_block(block: str) -> Dict[str, str]:
    """è§£æå–®ä¸€å€å¡Šç‚ºæ¬„ä½ dict"""
    data = {col: "" for col in TARGET_COLS}
    lines = [ln.strip() for ln in block.splitlines() if ln.strip()]
    raw_map: Dict[str, str] = {}
    
    for ln in lines:
        m = RE_KEY_VALUE.match(ln)
        if not m:
            continue
        k, v = m.group(1).strip(), m.group(2).strip()
        norm_k = normalize_key(k)
        if norm_k in raw_map:
            raw_map[norm_k] = f"{raw_map[norm_k]}; {v}"
        else:
            raw_map[norm_k] = v
    
    # æ˜ å°„åˆ°æ¨™æº–æ¬„ä½
    for tgt in data.keys():
        if tgt in raw_map:
            data[tgt] = raw_map[tgt]
    
    # å¾ $UpdatedBy æ¨ Worker
    if not data["Worker"] and "$UpdatedBy" in raw_map:
        data["Worker"] = extract_cn_name(raw_map["$UpdatedBy"])
    
    # Manager æŠ½åå­—
    if data["Manager"]:
        data["Manager"] = extract_cn_name(data["Manager"])
    
    # æ­£è¦åŒ–
    data["Date"] = normalize_date(data["Date"])
    data["Class"] = normalize_class(data["Class"])
    
    return data

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»è™•ç†é‚è¼¯
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_business_file(
    input_path: str,
    months_to_keep: int = DEFAULT_MONTHS_TO_KEEP,
    existing_csv: Optional[str] = None,
    output_csv: Optional[str] = None
) -> Tuple[pd.DataFrame, Dict]:
    """
    è™•ç†æ¥­å‹™æ—¥å ±æª”æ¡ˆ
    
    Args:
        input_path: è¼¸å…¥çš„ TXT æª”æ¡ˆè·¯å¾‘
        months_to_keep: ä¿ç•™æœ€è¿‘å¹¾å€‹æœˆçš„è³‡æ–™
        existing_csv: ç¾æœ‰çš„ CSV æª”æ¡ˆï¼ˆç”¨æ–¼å¢é‡æ›´æ–°ï¼‰
        output_csv: è¼¸å‡ºçš„ CSV æª”æ¡ˆè·¯å¾‘
    
    Returns:
        (DataFrame, çµ±è¨ˆè³‡è¨Š)
    """
    stats = {
        "input_file": input_path,
        "raw_records": 0,
        "filtered_by_date": 0,
        "duplicates_removed": 0,
        "final_records": 0,
        "date_range": {"min": None, "max": None},
        "cutoff_date": None,
    }
    
    # è¨ˆç®—æˆªæ­¢æ—¥æœŸ
    cutoff_date = datetime.now() - relativedelta(months=months_to_keep)
    stats["cutoff_date"] = cutoff_date.strftime("%Y/%m/%d")
    
    # è®€å–è¼¸å…¥æª”æ¡ˆ
    input_path = Path(input_path)
    text = None
    for enc in ("utf-8", "utf-8-sig", "cp950", "big5"):
        try:
            text = input_path.read_text(encoding=enc, errors="ignore")
            break
        except:
            continue
    
    if text is None:
        raise ValueError(f"ç„¡æ³•è®€å–æª”æ¡ˆ: {input_path}")
    
    # è§£æè¨˜éŒ„
    blocks = split_records(text)
    rows = []
    
    for b in blocks:
        rec = parse_block(b)
        if not any(rec.values()) or not (rec.get("Date") or rec.get("Content")):
            continue
        
        stats["raw_records"] += 1
        
        # æ—¥æœŸéæ¿¾
        rec_date = parse_date(rec["Date"])
        if rec_date and rec_date < cutoff_date:
            stats["filtered_by_date"] += 1
            continue
        
        # åŠ å…¥ hash ç”¨æ–¼å»é‡
        rec["_hash"] = compute_record_hash(rec)
        rows.append(rec)
    
    # å»ºç«‹ DataFrame
    df_new = pd.DataFrame(rows)
    
    if df_new.empty:
        return pd.DataFrame(columns=TARGET_COLS), stats
    
    # åˆä½µç¾æœ‰è³‡æ–™ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
    if existing_csv and Path(existing_csv).exists():
        try:
            df_existing = pd.read_csv(existing_csv, encoding="utf-8-sig")
            
            # ç¢ºä¿ç¾æœ‰è³‡æ–™ä¹Ÿæœ‰ hash
            if "_hash" not in df_existing.columns:
                df_existing["_hash"] = df_existing.apply(
                    lambda r: compute_record_hash(r.to_dict()), axis=1
                )
            
            # ç¾æœ‰è³‡æ–™ä¹Ÿè¦éæ¿¾æ—¥æœŸ
            df_existing["_parsed_date"] = df_existing["Date"].apply(parse_date)
            df_existing = df_existing[
                df_existing["_parsed_date"].isna() | 
                (df_existing["_parsed_date"] >= cutoff_date)
            ]
            df_existing = df_existing.drop(columns=["_parsed_date"])
            
            # åˆä½µä¸¦å»é‡
            df_combined = pd.concat([df_existing, df_new], ignore_index=True)
            before_dedup = len(df_combined)
            df_combined = df_combined.drop_duplicates(subset=["_hash"], keep="last")
            stats["duplicates_removed"] = before_dedup - len(df_combined)
            
            df_final = df_combined
        except Exception as e:
            print(f"âš ï¸ è®€å–ç¾æœ‰ CSV å¤±æ•—ï¼Œå°‡ä½¿ç”¨æ–°è³‡æ–™: {e}")
            df_final = df_new.drop_duplicates(subset=["_hash"], keep="last")
    else:
        df_final = df_new.drop_duplicates(subset=["_hash"], keep="last")
    
    # ç§»é™¤ hash æ¬„ä½ï¼Œæ•´ç†è¼¸å‡º
    if "_hash" in df_final.columns:
        df_final = df_final.drop(columns=["_hash"])
    
    # ç¢ºä¿æ¬„ä½é †åº
    for col in TARGET_COLS:
        if col not in df_final.columns:
            df_final[col] = ""
    df_final = df_final[TARGET_COLS]
    
    # ä¾æ—¥æœŸæ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    df_final = df_final.sort_values("Date", ascending=False).reset_index(drop=True)
    
    # çµ±è¨ˆ
    stats["final_records"] = len(df_final)
    if not df_final.empty:
        dates = df_final["Date"].dropna()
        if len(dates) > 0:
            stats["date_range"]["min"] = dates.min()
            stats["date_range"]["max"] = dates.max()
    
    # è¼¸å‡º CSV
    if output_csv:
        output_path = Path(output_csv)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df_final.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"âœ… å·²è¼¸å‡º: {output_path}")
    
    return df_final, stats

def generate_summary(df: pd.DataFrame, stats: Dict) -> str:
    """ç”¢ç”Ÿè™•ç†æ‘˜è¦"""
    lines = [
        "# æ¥­å‹™æ—¥å ±è™•ç†æ‘˜è¦",
        "",
        f"- è™•ç†æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"- åŸå§‹è¨˜éŒ„: {stats['raw_records']} ç­†",
        f"- æ—¥æœŸéæ¿¾: {stats['filtered_by_date']} ç­†ï¼ˆæ—©æ–¼ {stats['cutoff_date']}ï¼‰",
        f"- å»é‡: {stats['duplicates_removed']} ç­†",
        f"- æœ€çµ‚è¨˜éŒ„: {stats['final_records']} ç­†",
        "",
    ]
    
    if stats["date_range"]["min"]:
        lines.append(f"- è³‡æ–™ç¯„åœ: {stats['date_range']['min']} ~ {stats['date_range']['max']}")
    
    if not df.empty:
        # å„æ¥­å‹™æ´»å‹•æ•¸çµ±è¨ˆ
        lines.append("")
        lines.append("## å„æ¥­å‹™æ´»å‹•æ•¸ï¼ˆæœ€è¿‘ï¼‰")
        worker_counts = df.groupby("Worker").size().sort_values(ascending=False).head(10)
        for worker, count in worker_counts.items():
            lines.append(f"- {worker}: {count} ç­†")
        
        # æ´»å‹•é¡å‹çµ±è¨ˆ
        lines.append("")
        lines.append("## æ´»å‹•é¡å‹åˆ†å¸ƒ")
        class_counts = df["Class"].value_counts().head(10)
        for cls, count in class_counts.items():
            lines.append(f"- {cls}: {count} ç­†")
    
    return "\n".join(lines)


def cleanup_old_business_files(business_dir: str, keep_count: int = 3) -> List[str]:
    """
    æ¸…ç†èˆŠçš„æ¥­å‹™è³‡æ–™æª”æ¡ˆï¼Œåªä¿ç•™æœ€æ–°çš„å¹¾å€‹
    
    Args:
        business_dir: æ¥­å‹™è³‡æ–™ç›®éŒ„
        keep_count: ä¿ç•™çš„æª”æ¡ˆæ•¸é‡
    
    Returns:
        è¢«åˆªé™¤çš„æª”æ¡ˆåˆ—è¡¨
    """
    import glob
    
    # æ‰¾å‡ºæ‰€æœ‰å¸¶æ™‚é–“æˆ³çš„æ¥­å‹™æª”æ¡ˆ
    pattern = os.path.join(business_dir, "business_*.csv")
    files = glob.glob(pattern)
    
    # æ’é™¤å›ºå®šæª”å
    files = [f for f in files if not f.endswith("clean_business.csv")]
    
    if len(files) <= keep_count:
        return []
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    files.sort(key=lambda x: os.path.getmtime(x), reverse=True)
    
    # åˆªé™¤èˆŠæª”æ¡ˆ
    deleted = []
    for f in files[keep_count:]:
        try:
            os.remove(f)
            deleted.append(os.path.basename(f))
            print(f"ğŸ—‘ï¸ å·²åˆªé™¤èˆŠæª”æ¡ˆ: {os.path.basename(f)}")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•åˆªé™¤ {f}: {e}")
    
    # åŒæ™‚æ¸…ç†èˆŠçš„ summary æª”æ¡ˆ
    summary_files = glob.glob(os.path.join(business_dir, "business_summary*.md"))
    for sf in summary_files:
        try:
            os.remove(sf)
            deleted.append(os.path.basename(sf))
            print(f"ğŸ—‘ï¸ å·²åˆªé™¤æ‘˜è¦æª”æ¡ˆ: {os.path.basename(sf)}")
        except:
            pass
    
    return deleted

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# API æ•´åˆå‡½æ•¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def process_and_update_knowledge_base(
    input_path: str,
    business_dir: str,
    months_to_keep: int = DEFAULT_MONTHS_TO_KEEP,
    trigger_reload: bool = True
) -> Dict:
    """
    è™•ç†æ¥­å‹™æ—¥å ±ä¸¦æ›´æ–°çŸ¥è­˜åº«ï¼ˆä¾› API å‘¼å«ï¼‰
    
    Args:
        input_path: ä¸Šå‚³çš„ TXT æª”æ¡ˆè·¯å¾‘
        business_dir: æ¥­å‹™è³‡æ–™ç›®éŒ„
        months_to_keep: ä¿ç•™æœˆæ•¸
        trigger_reload: æ˜¯å¦è§¸ç™¼å‘é‡åº«é‡å»º
    
    Returns:
        è™•ç†çµæœ
    """
    # ç”¢ç”Ÿå¸¶æ—¥æœŸçš„æª”åï¼šbusiness_YYYYMMDD_HHMM.csv
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    new_csv_name = f"business_{timestamp}.csv"
    new_csv_path = os.path.join(business_dir, new_csv_name)
    
    # å›ºå®šæª”åï¼ˆä¾›ç³»çµ±ä½¿ç”¨ï¼‰
    fixed_csv_path = os.path.join(business_dir, "clean_business.csv")
    
    # è™•ç†æª”æ¡ˆ
    df, stats = process_business_file(
        input_path=input_path,
        months_to_keep=months_to_keep,
        existing_csv=fixed_csv_path if os.path.exists(fixed_csv_path) else None,
        output_csv=new_csv_path
    )
    
    # åŒæ™‚æ›´æ–°å›ºå®šæª”åï¼ˆä¾›ç³»çµ±æŸ¥è©¢ç”¨ï¼‰
    if os.path.exists(new_csv_path):
        import shutil
        shutil.copy2(new_csv_path, fixed_csv_path)
    
    # æ¸…ç†èˆŠçš„æ™‚é–“æˆ³æª”æ¡ˆï¼ˆåªä¿ç•™æœ€æ–° 3 å€‹ï¼‰
    cleanup_old_business_files(business_dir, keep_count=3)
    
    result = {
        "success": True,
        "stats": stats,
        "csv_path": new_csv_path,
        "csv_name": new_csv_name,
        "fixed_csv_path": fixed_csv_path,
    }
    
    # è§¸ç™¼é‡å»ºï¼ˆå¦‚æœéœ€è¦ï¼‰
    if trigger_reload:
        try:
            from core import reload_qa_system
            reload_qa_system()
            result["reloaded"] = True
        except Exception as e:
            result["reloaded"] = False
            result["reload_error"] = str(e)
    
    return result

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="æ¥­å‹™æ—¥å ±è™•ç†å™¨ - æ”¯æ´æ—¥æœŸéæ¿¾å’Œå¢é‡æ›´æ–°"
    )
    parser.add_argument("input", help="è¼¸å…¥çš„ TXT æª”æ¡ˆ")
    parser.add_argument(
        "-m", "--months", type=int, default=DEFAULT_MONTHS_TO_KEEP,
        help=f"ä¿ç•™æœ€è¿‘å¹¾å€‹æœˆçš„è³‡æ–™ï¼ˆé è¨­: {DEFAULT_MONTHS_TO_KEEP}ï¼‰"
    )
    parser.add_argument(
        "-o", "--output-dir", default="./",
        help="è¼¸å‡ºç›®éŒ„ï¼ˆé è¨­: ç•¶å‰ç›®éŒ„ï¼‰"
    )
    parser.add_argument(
        "--merge", action="store_true",
        help="èˆ‡ç¾æœ‰ CSV åˆä½µï¼ˆå¢é‡æ›´æ–°ï¼‰"
    )
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”¢ç”Ÿå¸¶æ™‚é–“æˆ³çš„æª”å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    csv_path = output_dir / f"business_{timestamp}.csv"
    fixed_csv_path = output_dir / "clean_business.csv"
    
    existing = str(fixed_csv_path) if args.merge and fixed_csv_path.exists() else None
    
    df, stats = process_business_file(
        input_path=args.input,
        months_to_keep=args.months,
        existing_csv=existing,
        output_csv=str(csv_path)
    )
    
    # åŒæ™‚æ›´æ–°å›ºå®šæª”å
    import shutil
    shutil.copy2(str(csv_path), str(fixed_csv_path))
    
    # æ¸…ç†èˆŠæª”æ¡ˆ
    cleanup_old_business_files(str(output_dir), keep_count=3)
    
    # è¼¸å‡ºæ‘˜è¦åˆ°çµ‚ç«¯
    summary = generate_summary(df, stats)
    print("\n" + summary)
    print(f"\nâœ… å·²å„²å­˜: {csv_path}")
    print(f"âœ… å·²æ›´æ–°: {fixed_csv_path}")

if __name__ == "__main__":
    main()
