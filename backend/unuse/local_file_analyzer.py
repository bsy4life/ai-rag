#!/usr/bin/env python3
"""
æœ¬åœ° OCR æª”æ¡ˆåˆ†æå·¥å…·
åˆ†æ data/ocr_txt ä¸­çš„æª”æ¡ˆï¼Œè©•ä¼°æ˜¯å¦éœ€è¦æ¸…ç†
"""
import os
import re
import json
from pathlib import Path
from typing import Dict, List, Tuple
from collections import Counter

class LocalFileAnalyzer:
    def __init__(self, data_dir: str = "/home/aiuser/ai-rag/backend/data/ocr_txt"):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            data_dir: è³‡æ–™ç›®éŒ„è·¯å¾‘
        """
        self.data_dir = Path(data_dir)
        self.report_dir = self.data_dir.parent / "analysis_reports"
        self.report_dir.mkdir(parents=True, exist_ok=True)
        
        # é›œè¨Šæ¨¡å¼
        self.noise_patterns = {
            'image_refs': r'!\[.*?\]\(.*?\)',
            'width_height': r'\{(?:width|height)=.*?\}',
            'format_marks': r'\[\.underline\]|\[\.bold\]|\[\.italic\]',
            'quote_marks': r'^>\s+',
            'empty_brackets': r'\[\s*\]|\(\s*\)',
            'media_paths': r'media/image\d+\.\w+',
            'excessive_spaces': r' {3,}',
            'excessive_newlines': r'\n{4,}'
        }
        
        # æœ‰ç”¨å…§å®¹æ¨¡å¼
        self.useful_patterns = {
            'model_numbers': r'LES[A-Z]*\d+|LECP\d+|LEC[A-Z]*\d+',
            'specifications': r'\d+(?:\.\d+)?\s*(?:mm|kg|MPa|Pa|Â°C|â„ƒ|N|W|V|A)',
            'part_numbers': r'[A-Z]{2,}\d{2,}',
            'tables': r'(?:\|.*\|.*\n)+',
            'japanese_text': r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]+',
            'technical_terms': r'(?:é€Ÿåº¦|ç²¾åº¦|è¡Œç¨‹|è² è¼‰|å£“åŠ›|æº«åº¦|é›»å£“|é›»æµ|åŠŸç‡)'
        }
    
    def analyze_file(self, file_path: Path) -> Dict:
        """
        åˆ†æå–®å€‹æª”æ¡ˆ
        
        Args:
            file_path: æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åˆ†æçµæœå­—å…¸
        """
        try:
            # å˜—è©¦ä¸åŒç·¨ç¢¼è®€å–
            content = None
            for encoding in ['utf-8', 'big5', 'gb2312', 'shift_jis', 'cp950']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        content = f.read()
                    break
                except UnicodeDecodeError:
                    continue
            
            if content is None:
                # æœ€å¾Œå˜—è©¦å¿½ç•¥éŒ¯èª¤
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            
            # åŸºæœ¬çµ±è¨ˆ
            stats = {
                'filename': file_path.name,
                'total_chars': len(content),
                'total_lines': content.count('\n'),
                'file_size_kb': file_path.stat().st_size / 1024,
                'encoding_detected': 'utf-8'  # ç°¡åŒ–é¡¯ç¤º
            }
            
            # è¨ˆç®—é›œè¨Š
            noise_counts = {}
            total_noise_chars = 0
            for name, pattern in self.noise_patterns.items():
                matches = re.findall(pattern, content, re.MULTILINE)
                noise_counts[name] = len(matches)
                total_noise_chars += sum(len(m) for m in matches)
            
            # è¨ˆç®—æœ‰ç”¨å…§å®¹
            useful_counts = {}
            for name, pattern in self.useful_patterns.items():
                matches = re.findall(pattern, content)
                useful_counts[name] = len(matches)
            
            # è¨ˆç®—æ¯”ä¾‹
            noise_ratio = total_noise_chars / len(content) if len(content) > 0 else 0
            
            # æå–ç¯„ä¾‹å…§å®¹
            clean_sample = re.sub(r'!\[.*?\]\(.*?\)', '', content[:500])
            clean_sample = re.sub(r'\{.*?\}', '', clean_sample)
            clean_sample = re.sub(r'\n{3,}', '\n\n', clean_sample)
            
            return {
                'stats': stats,
                'noise': noise_counts,
                'useful': useful_counts,
                'noise_ratio': noise_ratio,
                'total_noise_chars': total_noise_chars,
                'sample_original': content[:500],
                'sample_cleaned': clean_sample,
                'recommendation': self.get_recommendation(noise_ratio, useful_counts)
            }
            
        except Exception as e:
            return {
                'stats': {'filename': file_path.name, 'error': str(e)},
                'noise': {},
                'useful': {},
                'noise_ratio': 0,
                'recommendation': 'ERROR'
            }
    
    def get_recommendation(self, noise_ratio: float, useful_counts: Dict) -> str:
        """
        æ ¹æ“šåˆ†æçµæœçµ¦å‡ºå»ºè­°
        
        Args:
            noise_ratio: é›œè¨Šæ¯”ä¾‹
            useful_counts: æœ‰ç”¨å…§å®¹çµ±è¨ˆ
            
        Returns:
            å»ºè­°å­—ä¸²
        """
        # è¨ˆç®—æœ‰ç”¨å…§å®¹åˆ†æ•¸
        useful_score = (
            useful_counts.get('model_numbers', 0) * 10 +
            useful_counts.get('specifications', 0) * 5 +
            useful_counts.get('tables', 0) * 20 +
            useful_counts.get('technical_terms', 0) * 3
        )
        
        if noise_ratio > 0.3:
            return "HEAVY_CLEAN"  # éœ€è¦é‡åº¦æ¸…ç†
        elif noise_ratio > 0.1:
            return "LIGHT_CLEAN"  # éœ€è¦è¼•åº¦æ¸…ç†
        elif useful_score < 10:
            return "LOW_VALUE"    # å…§å®¹åƒ¹å€¼ä½
        else:
            return "DIRECT_USE"   # å¯ç›´æ¥ä½¿ç”¨
    
    def analyze_all_files(self) -> Dict:
        """
        åˆ†ææ‰€æœ‰æª”æ¡ˆ
        
        Returns:
            ç¸½é«”åˆ†æçµæœ
        """
        # æ‰¾å‡ºæ‰€æœ‰æ–‡å­—æª”æ¡ˆ
        text_files = []
        for ext in ['*.txt', '*.docx', '*.doc']:
            text_files.extend(self.data_dir.glob(ext))
        
        if not text_files:
            print(f"âš ï¸ åœ¨ {self.data_dir} ä¸­æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆ")
            return {}
        
        print(f"ğŸ“‚ åˆ†æç›®éŒ„: {self.data_dir}")
        print(f"ğŸ“„ æ‰¾åˆ° {len(text_files)} å€‹æª”æ¡ˆ")
        print("=" * 60)
        
        # åˆ†ææ¯å€‹æª”æ¡ˆ
        all_results = []
        recommendations_count = Counter()
        
        for i, file_path in enumerate(text_files, 1):
            print(f"\n[{i}/{len(text_files)}] åˆ†æ: {file_path.name}")
            
            result = self.analyze_file(file_path)
            all_results.append(result)
            recommendations_count[result['recommendation']] += 1
            
            # é¡¯ç¤ºç°¡è¦çµæœ
            if 'error' not in result['stats']:
                print(f"  ğŸ“Š å¤§å°: {result['stats']['file_size_kb']:.1f} KB")
                print(f"  ğŸ“ å­—å…ƒ: {result['stats']['total_chars']:,}")
                print(f"  ğŸ—‘ï¸ é›œè¨Š: {result['noise_ratio']:.1%}")
                print(f"  ğŸ’¡ å»ºè­°: {result['recommendation']}")
                
                # é¡¯ç¤ºç™¼ç¾çš„æœ‰ç”¨å…§å®¹
                if result['useful']['model_numbers'] > 0:
                    print(f"  âœ“ ç™¼ç¾ {result['useful']['model_numbers']} å€‹å‹è™Ÿ")
                if result['useful']['specifications'] > 0:
                    print(f"  âœ“ ç™¼ç¾ {result['useful']['specifications']} å€‹è¦æ ¼æ•¸æ“š")
                if result['useful']['tables'] > 0:
                    print(f"  âœ“ ç™¼ç¾è¡¨æ ¼çµæ§‹")
            else:
                print(f"  âŒ éŒ¯èª¤: {result['stats']['error']}")
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        summary = self.generate_summary(all_results, recommendations_count)
        
        # ä¿å­˜è©³ç´°å ±å‘Š
        report_path = self.report_dir / "analysis_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                'summary': summary,
                'details': all_results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*60}")
        print("ğŸ“Š ç¸½é«”åˆ†æçµæœ")
        print(f"{'='*60}")
        print(f"æª”æ¡ˆç¸½æ•¸: {summary['total_files']}")
        print(f"ç¸½å¤§å°: {summary['total_size_mb']:.1f} MB")
        print(f"å¹³å‡é›œè¨Š: {summary['average_noise_ratio']:.1%}")
        print(f"\nå»ºè­°åˆ†å¸ƒ:")
        for rec, count in summary['recommendations'].items():
            print(f"  {rec}: {count} å€‹æª”æ¡ˆ")
        
        print(f"\nğŸ’¡ ç¸½é«”å»ºè­°:")
        for suggestion in summary['suggestions']:
            print(f"  â€¢ {suggestion}")
        
        print(f"\nğŸ“ è©³ç´°å ±å‘Šå·²ä¿å­˜: {report_path}")
        
        return summary
    
    def generate_summary(self, results: List[Dict], rec_count: Counter) -> Dict:
        """
        ç”Ÿæˆç¸½çµå ±å‘Š
        
        Args:
            results: æ‰€æœ‰æª”æ¡ˆçš„åˆ†æçµæœ
            rec_count: å»ºè­°çµ±è¨ˆ
            
        Returns:
            ç¸½çµå­—å…¸
        """
        total_size = sum(r['stats'].get('file_size_kb', 0) for r in results)
        noise_ratios = [r['noise_ratio'] for r in results if r['noise_ratio'] > 0]
        
        summary = {
            'total_files': len(results),
            'total_size_mb': total_size / 1024,
            'average_noise_ratio': sum(noise_ratios) / len(noise_ratios) if noise_ratios else 0,
            'recommendations': dict(rec_count),
            'suggestions': []
        }
        
        # ç”Ÿæˆå»ºè­°
        if rec_count['HEAVY_CLEAN'] > len(results) * 0.3:
            summary['suggestions'].append("è¶…é 30% çš„æª”æ¡ˆéœ€è¦é‡åº¦æ¸…ç†ï¼Œå»ºè­°ä½¿ç”¨æ¸…ç†å·¥å…·è™•ç†æ‰€æœ‰æª”æ¡ˆ")
        elif rec_count['DIRECT_USE'] > len(results) * 0.7:
            summary['suggestions'].append("å¤§éƒ¨åˆ†æª”æ¡ˆå“è³ªè‰¯å¥½ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨")
        else:
            summary['suggestions'].append("æª”æ¡ˆå“è³ªåƒå·®ä¸é½Šï¼Œå»ºè­°å°éœ€è¦æ¸…ç†çš„æª”æ¡ˆå€‹åˆ¥è™•ç†")
        
        if summary['average_noise_ratio'] > 0.2:
            summary['suggestions'].append("å¹³å‡é›œè¨Šæ¯”ä¾‹åé«˜ï¼Œæ¸…ç†å¾Œå¯æå‡æœç´¢æº–ç¢ºåº¦")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰æŠ€è¡“æ–‡æª”
        total_specs = sum(r['useful'].get('specifications', 0) for r in results)
        if total_specs > 100:
            summary['suggestions'].append(f"ç™¼ç¾å¤§é‡æŠ€è¡“è¦æ ¼æ•¸æ“š ({total_specs} å€‹)ï¼Œé€™äº›æ˜¯é‡è¦å…§å®¹ï¼Œæ¸…ç†æ™‚è¦å°å¿ƒä¿ç•™")
        
        return summary
    
    def clean_files(self, level: str = "light") -> None:
        """
        æ ¹æ“šåˆ†æçµæœæ¸…ç†æª”æ¡ˆ
        
        Args:
            level: æ¸…ç†ç­‰ç´š ("light", "heavy", "auto")
        """
        print(f"\nğŸ§¹ é–‹å§‹æ¸…ç†æª”æ¡ˆ (ç­‰ç´š: {level})")
        
        clean_dir = self.data_dir.parent / "ocr_txt_cleaned"
        clean_dir.mkdir(parents=True, exist_ok=True)
        
        text_files = list(self.data_dir.glob("*.txt"))
        
        for file_path in text_files:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            if level == "auto":
                # è‡ªå‹•åˆ¤æ–·æ¸…ç†ç­‰ç´š
                result = self.analyze_file(file_path)
                if result['recommendation'] == "DIRECT_USE":
                    cleaned = content
                elif result['recommendation'] == "HEAVY_CLEAN":
                    cleaned = self.heavy_clean(content)
                else:
                    cleaned = self.light_clean(content)
            elif level == "heavy":
                cleaned = self.heavy_clean(content)
            else:
                cleaned = self.light_clean(content)
            
            # ä¿å­˜æ¸…ç†å¾Œçš„æª”æ¡ˆ
            output_path = clean_dir / file_path.name
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(cleaned)
            
            print(f"  âœ“ {file_path.name} -> {output_path}")
        
        print(f"\nâœ… æ¸…ç†å®Œæˆï¼æª”æ¡ˆå·²ä¿å­˜åˆ°: {clean_dir}")
    
    def light_clean(self, text: str) -> str:
        """è¼•åº¦æ¸…ç†ï¼šåªç§»é™¤æ˜é¡¯çš„é›œè¨Š"""
        text = re.sub(r'!\[.*?\]\(.*?\)', '', text)  # åœ–ç‰‡å¼•ç”¨
        text = re.sub(r'\{(?:width|height)=.*?\}', '', text)  # å°ºå¯¸æ¨™è¨˜
        text = re.sub(r'\n{4,}', '\n\n\n', text)  # éå¤šæ›è¡Œ
        text = re.sub(r' {3,}', '  ', text)  # éå¤šç©ºæ ¼
        return text.strip()
    
    def heavy_clean(self, text: str) -> str:
        """é‡åº¦æ¸…ç†ï¼šç§»é™¤æ‰€æœ‰é›œè¨Š"""
        # å…ˆåšè¼•åº¦æ¸…ç†
        text = self.light_clean(text)
        # é¡å¤–æ¸…ç†
        text = re.sub(r'\[\.underline\]|\[\.bold\]|\[\.italic\]', '', text)
        text = re.sub(r'^>\s+', '', text, flags=re.MULTILINE)
        text = re.sub(r'media/image\d+\.\w+', '', text)
        text = re.sub(r'\[\s*\]|\(\s*\)', '', text)
        return text.strip()

def main():
    """ä¸»ç¨‹å¼"""
    import sys
    
    # æª¢æŸ¥åƒæ•¸
    if len(sys.argv) > 1:
        data_dir = sys.argv[1]
    else:
        data_dir = "/home/aiuser/ai-rag/backend/data/ocr_txt"
    
    analyzer = LocalFileAnalyzer(data_dir)
    
    # åˆ†ææª”æ¡ˆ
    summary = analyzer.analyze_all_files()
    
    if not summary:
        return
    
    # è©¢å•æ˜¯å¦è¦æ¸…ç†
    print("\n" + "="*60)
    print("æ˜¯å¦è¦æ¸…ç†æª”æ¡ˆï¼Ÿ")
    print("1. ä¸æ¸…ç† (åˆ†æå®Œæˆ)")
    print("2. è¼•åº¦æ¸…ç† (åªç§»é™¤åœ–ç‰‡å¼•ç”¨å’Œæ ¼å¼æ¨™è¨˜)")
    print("3. é‡åº¦æ¸…ç† (ç§»é™¤æ‰€æœ‰é›œè¨Š)")
    print("4. è‡ªå‹•æ¸…ç† (æ ¹æ“šåˆ†æçµæœè‡ªå‹•é¸æ“‡)")
    
    choice = input("\nè«‹é¸æ“‡ (1-4): ").strip()
    
    if choice == "2":
        analyzer.clean_files("light")
    elif choice == "3":
        analyzer.clean_files("heavy")
    elif choice == "4":
        analyzer.clean_files("auto")
    else:
        print("âœ… åˆ†æå®Œæˆï¼Œæœªé€²è¡Œæ¸…ç†")

if __name__ == "__main__":
    main()