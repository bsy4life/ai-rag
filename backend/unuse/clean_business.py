#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
clean_business.py
å°‡ã€Šæ¥­å‹™æ—¥å ±æ ¼å¼.txtã€‹é¡åŸå§‹æ–‡å­—æª”æ¸…æ´—ç‚ºçµæ§‹åŒ– CSVï¼Œä¸¦è¼¸å‡ºç°¡æ˜“ Markdown é è¦½ã€‚

ç‰¹é»ï¼š
- ä»¥æ›é ç¬¦ \f è¦–ç‚ºå„ç­†ç´€éŒ„åˆ†éš”ï¼ˆè‹¥ç„¡æ›é ï¼Œä¹Ÿæœƒå˜—è©¦ä»¥ Doc_Time/Date ç­‰é—œéµæ¬„ä½åˆ‡åˆ†ï¼‰
- è§£æå¸¸è¦‹æ¬„ä½ï¼šDate, Worker, Customer, Class, Content, Depart, Manager, Level, Doc_Status, TimeCreated, Doc_Time
- æ­£è¦åŒ–æ—¥æœŸç‚º YYYY/MM/DDã€æ´»å‹•é¡å‹ç‚ºã€Œé€—è™Ÿ+ç©ºç™½ã€æ¸…å–®
- è‡ªå‹•å¾ $UpdatedBy / Manager ä¸­æŠ½å‡º CN=ä¸­æ–‡å
- å¯ä¸€æ¬¡è™•ç†å¤šå€‹æª”æ¡ˆ
- ç”¢å‡ºï¼šclean_business.csvï¼ˆUTF-8 BOMï¼‰èˆ‡ clean_business_preview.md

éœ€æ±‚ï¼š
- Python 3.8+
- pandas
"""

from __future__ import annotations
import argparse
import re
import sys
from pathlib import Path
from typing import Dict, List
from datetime import datetime

try:
    import pandas as pd
except ImportError:
    print("âŒ éœ€è¦å¥—ä»¶ pandasï¼Œè«‹å…ˆå®‰è£ï¼š pip install pandas", file=sys.stderr)
    sys.exit(1)

# -----------------------
# è§£æèˆ‡æ­£è¦åŒ–å·¥å…·
# -----------------------

KEY_MAP = {
    # æ­£è¦åŒ–æ¬„ä½åç¨±æ˜ å°„
    "date": "Date",
    "æ—¥æœŸ": "Date",
    "worker": "Worker",
    "å“¡å·¥": "Worker",
    "customer": "Customer",
    "å®¢æˆ¶": "Customer",
    "class": "Class",
    "æ´»å‹•é¡å‹": "Class",
    "content": "Content",
    "æ´»å‹•å…§å®¹": "Content",
    "depart": "Depart",
    "éƒ¨é–€": "Depart",
    "manager": "Manager",
    "ä¸»ç®¡": "Manager",
    "submanager": "SubManager",
    "level": "Level",
    "doc_st": "Doc_Status",
    "æ–‡ä»¶ç‹€æ…‹": "Doc_Status",
    "timecreated": "TimeCreated",
    "doc_time": "Doc_Time",
    "$updatedby": "$UpdatedBy",
}

TARGET_COLS = [
    "Date","Worker","Customer","Class","Content","Depart",
    "Manager","Level","Doc_Status","TimeCreated","Doc_Time"
]

RE_KEY_VALUE = re.compile(r"^([A-Za-z0-9_\-$\u4e00-\u9fa5]+)\s*[:ï¼š]\s*(.*)$")

def normalize_key(k: str) -> str:
    k = k.strip()
    k_l = k.lower()
    # å…ˆ map è‹±/ä¸­ â†’ æ¨™æº– key
    if k_l in KEY_MAP:
        return KEY_MAP[k_l]
    # ç‰¹æ®Šéµï¼ˆå¤§å°å¯«ï¼ä¸­è‹±æ–‡æ··æ­ï¼‰
    if k in KEY_MAP:
        return KEY_MAP[k]
    # å·²ç¶“æ˜¯æ¨™æº–ç›®æ¨™æ¬„ä½
    if k in TARGET_COLS or k in ["$UpdatedBy", "SubManager"]:
        return k
    # fallbackï¼šç¶­æŒåŸæ¨£ï¼ˆé¿å…éºå¤±è³‡è¨Šï¼‰
    return k

def normalize_date(s: str) -> str:
    """å°‡æ—¥æœŸæ­£è¦åŒ–ç‚º YYYY/MM/DDï¼›è‹¥ç„¡æ³•è§£æå‰‡åŸæ¨£è¿”å›ã€‚"""
    s = (s or "").strip()
    if not s:
        return s
    m = re.match(r"(\d{4})[/-](\d{1,2})[/-](\d{1,2})", s)
    if m:
        y, mo, d = m.groups()
        return f"{int(y):04d}/{int(mo):02d}/{int(d):02d}"
    # æœ‰æ™‚ Date ä¸ç´”ï¼Œå˜—è©¦åœ¨å­—ä¸²ä¸­æŠ“ç¬¬ä¸€å€‹ YYYY-MM-DD/ YYYY/MM/DD
    m = re.search(r"(\d{4})[/-](\d{1,2})[/-](\d{1,2})", s)
    if m:
        y, mo, d = m.groups()
        return f"{int(y):04d}/{int(mo):02d}/{int(d):02d}"
    return s

def normalize_class(s: str) -> str:
    """å°‡æ´»å‹•é¡å‹å­—ä¸²æ¸…æ´—æˆä»¥é€—è™Ÿ+ç©ºç™½åˆ†éš”çš„æ¸…å–®æ–‡å­—ã€‚"""
    s = (s or "").strip()
    if not s:
        return s
    s = s.replace("ï¼Œ", ",").replace("ã€", ",")
    s = s.replace(" ", "")
    parts = [p for p in re.split(r"[,\s]+", s) if p]
    return ", ".join(parts)

def extract_cn_name(s: str) -> str:
    """
    å¾é¡ä¼¼ 'CN=é»ƒå¾·éœ–/O=Sanshin' æŠ½å‡º 'é»ƒå¾·éœ–'ã€‚
    è‹¥æ²’æœ‰ CN=ï¼Œå‰‡å›å‚³åŸå­—ä¸²ã€‚
    """
    if not isinstance(s, str):
        return s
    m = re.search(r"CN=([^/ï¼Œ,ï¼›;\s]+)", s)
    return m.group(1) if m else s

def guess_record_splits(text: str) -> List[str]:
    """
    åˆæ­¥åˆ‡åˆ†ç´€éŒ„ï¼š
    - å„ªå…ˆä½¿ç”¨ \f
    - è‹¥ç„¡ \fï¼Œå‰‡ä»¥å‡ºç¾ 'Doc_Time:' æˆ– 'Date:' çš„è¡Œç‚ºåˆ†æ®µèµ·å§‹ï¼Œèšåˆåˆ°ä¸‹ä¸€æ®µé–‹å§‹å‰
    """
    if "\f" in text:
        blocks = re.split(r"\f+", text)
        return [b for b in blocks if b.strip()]
    # é€€è·¯ï¼šä»¥é—œéµæ¬„ä½è¡Œç‚ºåˆ†æ®µ
    lines = text.splitlines()
    blocks, curr = [], []
    rec_start_re = re.compile(r"^\s*(Doc_Time|Date)\s*[:ï¼š]")
    for ln in lines:
        if rec_start_re.search(ln) and curr:
            blocks.append("\n".join(curr))
            curr = [ln]
        else:
            curr.append(ln)
    if curr:
        blocks.append("\n".join(curr))
    return [b for b in blocks if b.strip()]

def parse_block(block: str) -> Dict[str, str]:
    """
    è§£æå–®ä¸€å€å¡Šç‚ºæ¬„ä½ dictï¼Œåƒ…ä¿ç•™ TARGET_COLS ï¼‹å°‘é‡é—œéµè¼”åŠ©æ¬„ä½ã€‚
    """
    data: Dict[str, str] = {
        "Date": "",
        "Worker": "",
        "Customer": "",
        "Class": "",
        "Content": "",
        "Depart": "",
        "Manager": "",
        "Level": "",
        "Doc_Status": "",
        "TimeCreated": "",
        "Doc_Time": "",
    }

    # è¡Œæ¸…æ´—
    lines = [ln.strip() for ln in block.splitlines() if ln.strip()]

    # æš«å­˜åŸå§‹éµå€¼ï¼ˆé˜²æ­¢åŒéµå¤šæ¬¡å‡ºç¾æ™‚çš„è¦†è“‹ï¼‰
    raw_map: Dict[str, str] = {}

    for ln in lines:
        m = RE_KEY_VALUE.match(ln)
        if not m:
            continue
        k, v = m.group(1).strip(), m.group(2).strip()
        norm_k = normalize_key(k)
        if norm_k in raw_map:
            # åˆä½µï¼ˆç”¨åˆ†è™Ÿé€£æ¥ï¼‰ï¼Œé¿å…è¦†è“‹æ‰
            raw_map[norm_k] = f"{raw_map[norm_k]}; {v}"
        else:
            raw_map[norm_k] = v

    # æ˜ å°„åˆ°æ¨™æº–æ¬„ä½
    for tgt in data.keys():
        if tgt in raw_map:
            data[tgt] = raw_map[tgt]

    # å¾ $UpdatedBy æ¨ Workerï¼Œè‹¥ Worker ç©º
    if not data["Worker"] and "$UpdatedBy" in raw_map:
        data["Worker"] = extract_cn_name(raw_map["$UpdatedBy"])

    # Manager ä¹ŸæŠ½ CN å
    if data["Manager"]:
        data["Manager"] = extract_cn_name(data["Manager"])

    # æ­£è¦åŒ–æ—¥æœŸèˆ‡æ´»å‹•é¡å‹
    data["Date"] = normalize_date(data["Date"])
    data["Class"] = normalize_class(data["Class"])

    return data

def clean_files(input_paths: List[Path]) -> pd.DataFrame:
    """è®€å–å¤šå€‹æª”æ¡ˆï¼Œæ¸…æ´—ä¸¦å½™æ•´ç‚ºå–®ä¸€ DataFrameã€‚"""
    rows: List[Dict[str, str]] = []

    for p in input_paths:
        if not p.exists() or not p.is_file():
            print(f"âš ï¸ è·³éä¸å­˜åœ¨çš„æª”æ¡ˆï¼š{p}", file=sys.stderr)
            continue
        # å˜—è©¦ä»¥ UTF-8 è®€å–ï¼Œä¸è¡Œå°±ç”¨ cp950/big5 ä½œç‚ºé€€è·¯
        for enc in ("utf-8", "cp950", "big5", "utf-8-sig"):
            try:
                text = p.read_text(encoding=enc, errors="ignore")
                break
            except Exception:
                text = None
        if text is None:
            print(f"âŒ ç„¡æ³•è®€å–æª”æ¡ˆï¼š{p}", file=sys.stderr)
            continue

        blocks = guess_record_splits(text)
        for b in blocks:
            rec = parse_block(b)
            # è‡³å°‘è¦æœ‰ Date æˆ– Content æ‰è¦–ç‚ºæœ‰æ•ˆ
            if any(rec.values()) and (rec.get("Date") or rec.get("Content")):
                rows.append(rec)

    if not rows:
        return pd.DataFrame(columns=TARGET_COLS)

    df = pd.DataFrame(rows, columns=TARGET_COLS).drop_duplicates().reset_index(drop=True)

    # ç”¢ç”Ÿæœˆä»½æ¬„ä½ YYYY/MM
    df["_Month"] = df["Date"].astype(str).str.slice(0, 7)
    return df

def save_outputs(df: pd.DataFrame, out_dir: Path) -> None:
    out_dir.mkdir(parents=True, exist_ok=True)
    csv_path = out_dir / "clean_business.csv"
    md_path  = out_dir / "clean_business_preview.md"

    # CSVï¼ˆBOM ä»¥åˆ© Excel é–‹å•Ÿä¸­æ–‡ä¸äº‚ç¢¼ï¼‰
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")

    # Markdown ç°¡å ±ï¼ˆæ¯æœˆ/æ¥­å‹™æ´»å‹•æ•¸ï¼‰
    md_lines = ["# æ¥­å‹™æ—¥å ±æ¸…æ´—é è¦½", ""]
    if not df.empty:
        piv = (df.groupby(["_Month","Worker"])
                 .size()
                 .rename("æ´»å‹•æ•¸")
                 .reset_index()
                 .sort_values(["_Month","æ´»å‹•æ•¸"], ascending=[True, False]))
        try:
            md_lines += ["## æ¯æœˆå„æ¥­å‹™æ´»å‹•æ•¸", "", piv.to_markdown(index=False), ""]
        except Exception:
            # æŸäº›ç’°å¢ƒç„¡ tabulate æ”¯æ´æ™‚é€€è·¯
            md_lines += ["## æ¯æœˆå„æ¥­å‹™æ´»å‹•æ•¸", "", str(piv), ""]

        # ç¯„ä¾‹å‰ 30 ç­†
        sample = df[TARGET_COLS].head(30)
        try:
            md_lines += ["## æ¸…æ´—å¾Œæ¨£ä¾‹ï¼ˆå‰ 30 ç­†ï¼‰", "", sample.to_markdown(index=False), ""]
        except Exception:
            md_lines += ["## æ¸…æ´—å¾Œæ¨£ä¾‹ï¼ˆå‰ 30 ç­†ï¼‰", "", str(sample), ""]

    md_path.write_text("\n".join(md_lines), encoding="utf-8")

    print(f"âœ… å·²è¼¸å‡º CSVï¼š{csv_path}")
    print(f"âœ… å·²è¼¸å‡ºé è¦½ï¼š{md_path}")
    print(f"ğŸ“Š è³‡æ–™ç­†æ•¸ï¼š{len(df)}")

# -----------------------
# CLI
# -----------------------

def main():
    ap = argparse.ArgumentParser(
        description="æ¸…æ´—ã€Šæ¥­å‹™æ—¥å ±ã€‹åŸå§‹æ–‡å­—æª”ï¼Œè¼¸å‡ºçµæ§‹åŒ– CSV èˆ‡ Markdown é è¦½"
    )
    ap.add_argument(
        "inputs",
        nargs="+",
        help="è¼¸å…¥è·¯å¾‘ï¼ˆæª”æ¡ˆæˆ–è³‡æ–™å¤¾ï¼Œæ”¯æ´å¤šå€‹ï¼‰ã€‚è‹¥ç‚ºè³‡æ–™å¤¾ï¼ŒæœƒæŠ“å–å…¶ä¸­æ‰€æœ‰ .txt æª”"
    )
    ap.add_argument(
        "-o", "--out-dir",
        default="./",
        help="è¼¸å‡ºè³‡æ–™å¤¾ï¼ˆé è¨­ç‚ºç›®å‰ç›®éŒ„ï¼‰"
    )
    args = ap.parse_args()

    in_paths: List[Path] = []
    for item in args.inputs:
        p = Path(item)
        if p.is_dir():
            in_paths += list(p.glob("*.txt"))
        elif p.is_file():
            in_paths.append(p)
        else:
            print(f"âš ï¸ è·³éç„¡æ•ˆè·¯å¾‘ï¼š{p}", file=sys.stderr)

    if not in_paths:
        print("âŒ æ‰¾ä¸åˆ°å¯è™•ç†çš„è¼¸å…¥æª”æ¡ˆï¼ˆ.txtï¼‰", file=sys.stderr)
        sys.exit(2)

    df = clean_files(in_paths)
    save_outputs(df, Path(args.out_dir))


if __name__ == "__main__":
    main()
