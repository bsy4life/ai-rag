# rag_optimizer.py - RAG å„ªåŒ–è¼”åŠ©æ¨¡çµ„
"""
æä¾›ï¼š
1. åˆ†å±¤ LLM é¸æ“‡
2. Reranker æ”¯æ´
3. ä¾†æºè¿½è¹¤
4. è¤‡é›œåº¦è©•ä¼°
"""

import os
import re
from typing import List, Dict, Optional, Tuple
from langchain_core.documents import Document

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åˆ†å±¤ LLM
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_llm_for_query(query_type: str, query: str, docs: List[Document] = None):
    """
    æ ¹æ“šæŸ¥è©¢é¡å‹å’Œè¤‡é›œåº¦é¸æ“‡é©ç•¶çš„ LLM
    
    Args:
        query_type: "technical", "business", "mixed"
        query: æŸ¥è©¢å…§å®¹
        docs: æª¢ç´¢åˆ°çš„æ–‡æª”ï¼ˆç”¨æ–¼è©•ä¼°è¤‡é›œåº¦ï¼‰
    
    Returns:
        ChatOpenAI å¯¦ä¾‹
    """
    from langchain_openai import ChatOpenAI
    
    try:
        from config_optimized import LLM_CONFIG
    except ImportError:
        # é è¨­é…ç½®
        LLM_CONFIG = {
            "technical_complex": {"model": "gpt-4o", "temperature": 0.1},
            "technical_simple": {"model": "gpt-4o-mini", "temperature": 0.1},
            "business": {"model": "gpt-4o-mini", "temperature": 0},
            "default": {"model": "gpt-4o-mini", "temperature": 0.1},
        }
    
    # æ¥­å‹™æŸ¥è©¢ä¸€å¾‹ç”¨ä¾¿å®œæ¨¡å‹
    if query_type == "business":
        config = LLM_CONFIG.get("business", LLM_CONFIG["default"])
        return ChatOpenAI(**config)
    
    # æŠ€è¡“æŸ¥è©¢æ ¹æ“šè¤‡é›œåº¦é¸æ“‡
    if query_type == "technical":
        complexity = estimate_query_complexity(query, docs)
        if complexity == "complex":
            config = LLM_CONFIG.get("technical_complex", LLM_CONFIG["default"])
        else:
            config = LLM_CONFIG.get("technical_simple", LLM_CONFIG["default"])
        return ChatOpenAI(**config)
    
    # é è¨­
    return ChatOpenAI(**LLM_CONFIG["default"])


def estimate_query_complexity(query: str, docs: List[Document] = None) -> str:
    """
    è©•ä¼°æŸ¥è©¢è¤‡é›œåº¦
    
    Returns:
        "simple" æˆ– "complex"
    """
    # è¤‡é›œåº¦æŒ‡æ¨™
    is_complex = False
    
    # 1. å•é¡Œé•·åº¦
    if len(query) > 100:
        is_complex = True
    
    # 2. å¤šå€‹ç”¢å“/å‹è™Ÿ
    model_patterns = [
        r'[A-Z]{2,}\d+',           # SMC123
        r'No\.\s*\d+',             # No.6500
        r'\d+[A-Z]+\d*',           # 7010A
    ]
    model_count = sum(len(re.findall(p, query)) for p in model_patterns)
    if model_count >= 2:
        is_complex = True
    
    # 3. æ¯”è¼ƒé¡å•é¡Œ
    comparison_keywords = ['æ¯”è¼ƒ', 'å·®ç•°', 'ä¸åŒ', 'å“ªå€‹', 'vs', 'å°æ¯”', 'å„ªç¼ºé»']
    if any(kw in query for kw in comparison_keywords):
        is_complex = True
    
    # 4. æ–‡æª”æ•¸é‡å¤š
    if docs and len(docs) > 5:
        is_complex = True
    
    # 5. éœ€è¦è¨ˆç®—æˆ–åˆ†æ
    analysis_keywords = ['è¨ˆç®—', 'ä¼°ç®—', 'åˆ†æ', 'çµ±è¨ˆ', 'è¶¨å‹¢', 'é æ¸¬']
    if any(kw in query for kw in analysis_keywords):
        is_complex = True
    
    return "complex" if is_complex else "simple"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Reranker
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_reranker = None

def get_reranker():
    """å–å¾— Reranker å¯¦ä¾‹ï¼ˆå–®ä¾‹ï¼‰"""
    global _reranker
    
    if _reranker is not None:
        return _reranker
    
    try:
        from config_optimized import RERANKER_CONFIG
    except ImportError:
        return None
    
    if not RERANKER_CONFIG.get("enabled", False):
        return None
    
    reranker_type = RERANKER_CONFIG.get("type", "local")
    
    if reranker_type == "cohere":
        try:
            from langchain_cohere import CohereRerank
            api_key = os.getenv(RERANKER_CONFIG["cohere"]["api_key_env"])
            if api_key:
                _reranker = CohereRerank(
                    model=RERANKER_CONFIG["cohere"]["model"],
                    top_n=RERANKER_CONFIG.get("top_n", 5)
                )
                print("âœ… Cohere Reranker å·²å•Ÿç”¨")
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£ langchain-cohere: pip install langchain-cohere")
    
    elif reranker_type == "local":
        try:
            from sentence_transformers import CrossEncoder
            model_name = RERANKER_CONFIG["local"]["model"]
            _reranker = CrossEncoder(model_name)
            print(f"âœ… æœ¬åœ° Reranker å·²å•Ÿç”¨: {model_name}")
        except ImportError:
            print("âš ï¸ éœ€è¦å®‰è£ sentence-transformers: pip install sentence-transformers")
    
    return _reranker


def rerank_documents(query: str, docs: List[Document], top_n: int = 5) -> List[Document]:
    """
    ä½¿ç”¨ Reranker é‡æ–°æ’åºæ–‡æª”
    
    Args:
        query: æŸ¥è©¢
        docs: åŸå§‹æ–‡æª”åˆ—è¡¨
        top_n: ä¿ç•™å‰ N å€‹
    
    Returns:
        é‡æ’å¾Œçš„æ–‡æª”åˆ—è¡¨
    """
    if not docs:
        return docs
    
    reranker = get_reranker()
    if reranker is None:
        return docs[:top_n]
    
    try:
        from config_optimized import RERANKER_CONFIG
        reranker_type = RERANKER_CONFIG.get("type", "local")
    except ImportError:
        reranker_type = "local"
    
    if reranker_type == "cohere":
        # Cohere Reranker è¿”å› Document åˆ—è¡¨
        try:
            return reranker.compress_documents(docs, query)[:top_n]
        except Exception as e:
            print(f"âš ï¸ Cohere Rerank å¤±æ•—: {e}")
            return docs[:top_n]
    
    else:
        # æœ¬åœ° CrossEncoder
        try:
            pairs = [(query, doc.page_content) for doc in docs]
            scores = reranker.predict(pairs)
            
            # æ’åº
            doc_scores = list(zip(docs, scores))
            doc_scores.sort(key=lambda x: x[1], reverse=True)
            
            return [doc for doc, _ in doc_scores[:top_n]]
        except Exception as e:
            print(f"âš ï¸ æœ¬åœ° Rerank å¤±æ•—: {e}")
            return docs[:top_n]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¾†æºè¿½è¹¤
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_sources(docs: List[Document], max_sources: int = 3) -> List[str]:
    """
    å¾æ–‡æª”ä¸­æå–ä¾†æºè³‡è¨Š
    
    Args:
        docs: æ–‡æª”åˆ—è¡¨
        max_sources: æœ€å¤šè¿”å›å¹¾å€‹ä¾†æº
    
    Returns:
        ä¾†æºåˆ—è¡¨
    """
    sources = []
    seen = set()
    
    for doc in docs:
        source = doc.metadata.get('source', '')
        if source and source not in seen:
            # ç°¡åŒ–è·¯å¾‘
            source_name = os.path.basename(source)
            sources.append(source_name)
            seen.add(source)
        
        if len(sources) >= max_sources:
            break
    
    return sources


def append_sources_to_answer(answer: str, sources: List[str]) -> str:
    """
    åœ¨å›ç­”æœ«å°¾é™„åŠ ä¾†æºè³‡è¨Š
    
    Args:
        answer: åŸå§‹å›ç­”
        sources: ä¾†æºåˆ—è¡¨
    
    Returns:
        é™„åŠ ä¾†æºå¾Œçš„å›ç­”
    """
    if not sources:
        return answer
    
    source_text = "\n\n---\nğŸ“š **åƒè€ƒä¾†æº**ï¼š" + "ã€".join(sources)
    return answer + source_text


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å„ªåŒ–å¾Œçš„æª¢ç´¢æµç¨‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def optimized_retrieve_and_generate(
    query: str,
    retriever,
    chain,
    query_type: str = "technical",
    use_rerank: bool = True,
    track_sources: bool = True
) -> Tuple[str, List[str]]:
    """
    å„ªåŒ–å¾Œçš„æª¢ç´¢å’Œç”Ÿæˆæµç¨‹
    
    Args:
        query: æŸ¥è©¢
        retriever: æª¢ç´¢å™¨
        chain: LLM éˆ
        query_type: æŸ¥è©¢é¡å‹
        use_rerank: æ˜¯å¦ä½¿ç”¨ Reranker
        track_sources: æ˜¯å¦è¿½è¹¤ä¾†æº
    
    Returns:
        (å›ç­”, ä¾†æºåˆ—è¡¨)
    """
    # 1. æª¢ç´¢
    docs = retriever.invoke(query)
    
    # 2. Rerankï¼ˆå¯é¸ï¼‰
    if use_rerank:
        docs = rerank_documents(query, docs)
    
    # 3. æå–ä¾†æº
    sources = []
    if track_sources:
        sources = extract_sources(docs)
    
    # 4. é¸æ“‡ LLM
    llm = get_llm_for_query(query_type, query, docs)
    
    # 5. ç”Ÿæˆå›ç­”
    # æ³¨æ„ï¼šé€™è£¡å‡è¨­ chain å¯ä»¥æ¥å—è‡ªå®šç¾© LLM
    # å¯¦éš›ä½¿ç”¨æ™‚å¯èƒ½éœ€è¦èª¿æ•´
    try:
        result = chain.invoke({"input": query, "context": docs})
        answer = result if isinstance(result, str) else result.get("answer", "")
    except Exception as e:
        answer = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
    
    # 6. é™„åŠ ä¾†æº
    if track_sources and sources:
        answer = append_sources_to_answer(answer, sources)
    
    return answer, sources


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æˆæœ¬ä¼°ç®—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def estimate_query_cost(query: str, docs: List[Document], model: str = "gpt-4o") -> Dict:
    """
    ä¼°ç®—æŸ¥è©¢æˆæœ¬
    
    Returns:
        {"input_tokens": int, "output_tokens": int, "estimated_cost": float}
    """
    # ä¼°ç®— token æ•¸ï¼ˆç²—ç•¥ï¼‰
    query_tokens = len(query) // 2  # ä¸­æ–‡ç´„ 2 å­—å…ƒ = 1 token
    
    context_tokens = 0
    for doc in docs:
        context_tokens += len(doc.page_content) // 2
    
    input_tokens = query_tokens + context_tokens
    output_tokens = 500  # å‡è¨­è¼¸å‡ºç´„ 500 tokens
    
    # åƒ¹æ ¼ï¼ˆæ¯ 1M tokensï¼‰
    prices = {
        "gpt-4o": {"input": 2.5, "output": 10.0},
        "gpt-4o-mini": {"input": 0.15, "output": 0.6},
    }
    
    price = prices.get(model, prices["gpt-4o"])
    
    input_cost = (input_tokens / 1_000_000) * price["input"]
    output_cost = (output_tokens / 1_000_000) * price["output"]
    
    return {
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "estimated_cost_usd": round(input_cost + output_cost, 6),
        "model": model,
    }
