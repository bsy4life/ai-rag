# incremental_embedding.py - å¢é‡ Embedding ç®¡ç†æ¨¡çµ„
"""
å¯¦ç¾ CSV æ¥­å‹™è³‡æ–™çš„å¢é‡ embeddingï¼š
- åµæ¸¬æ–°å¢è¨˜éŒ„ â†’ åª embed æ–°è¨˜éŒ„
- åµæ¸¬åˆªé™¤è¨˜éŒ„ â†’ å¾å‘é‡åº«ç§»é™¤
- åµæ¸¬ä¿®æ”¹è¨˜éŒ„ â†’ æ›´æ–° embedding
"""

import os
import json
import hashlib
from datetime import datetime
from typing import Dict, List, Set, Tuple, Optional

try:
    import pandas as pd
    _HAS_PANDAS = True
except ImportError:
    _HAS_PANDAS = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¸¸æ•¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# ID ç´¢å¼•æª”æ¡ˆåç¨±
INDEX_FILE_NAME = "business_record_index.json"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è¨˜éŒ„ ID ç”Ÿæˆ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_record_id(row: dict) -> str:
    """
    ç‚ºæ¯ç­†æ¥­å‹™è¨˜éŒ„ç”Ÿæˆå”¯ä¸€ ID
    ä½¿ç”¨ Date + Worker + Customer + Content çš„ hash
    """
    # çµ„åˆé—œéµæ¬„ä½
    key_parts = [
        str(row.get('Date', '')).strip(),
        str(row.get('Worker', '')).strip(),
        str(row.get('Customer', '')).strip(),
        str(row.get('Content', ''))[:100].strip(),  # åªå–å‰ 100 å­—å…ƒé¿å…å¤ªé•·
    ]
    key_str = '|'.join(key_parts)
    
    # ç”Ÿæˆ hash ä½œç‚º ID
    return hashlib.md5(key_str.encode('utf-8')).hexdigest()[:16]


def generate_content_hash(row: dict) -> str:
    """
    ç‚ºè¨˜éŒ„å…§å®¹ç”Ÿæˆ hashï¼Œç”¨æ–¼åµæ¸¬å…§å®¹è®Šæ›´
    """
    content_parts = [
        str(row.get('Date', '')),
        str(row.get('Worker', '')),
        str(row.get('Customer', '')),
        str(row.get('Class', '')),
        str(row.get('Content', '')),
        str(row.get('Depart', '')),
        str(row.get('Manager', '')),
    ]
    content_str = '|'.join(content_parts)
    return hashlib.md5(content_str.encode('utf-8')).hexdigest()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ç´¢å¼•ç®¡ç†
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class RecordIndex:
    """æ¥­å‹™è¨˜éŒ„ç´¢å¼•ç®¡ç†å™¨"""
    
    def __init__(self, index_path: str):
        self.index_path = index_path
        self.records: Dict[str, dict] = {}  # record_id -> {content_hash, row_num, date, ...}
        self._load()
    
    def _load(self):
        """è¼‰å…¥ç´¢å¼•æª”æ¡ˆ"""
        if os.path.exists(self.index_path):
            try:
                with open(self.index_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.records = data.get('records', {})
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥ç´¢å¼•å¤±æ•—: {e}")
                self.records = {}
    
    def save(self):
        """å„²å­˜ç´¢å¼•æª”æ¡ˆ"""
        try:
            os.makedirs(os.path.dirname(self.index_path), exist_ok=True)
            data = {
                'updated_at': datetime.now().isoformat(),
                'total_records': len(self.records),
                'records': self.records
            }
            with open(self.index_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"âŒ å„²å­˜ç´¢å¼•å¤±æ•—: {e}")
    
    def get_all_ids(self) -> Set[str]:
        """å–å¾—æ‰€æœ‰å·²ç´¢å¼•çš„è¨˜éŒ„ ID"""
        return set(self.records.keys())
    
    def get_record(self, record_id: str) -> Optional[dict]:
        """å–å¾—è¨˜éŒ„è³‡è¨Š"""
        return self.records.get(record_id)
    
    def add_record(self, record_id: str, content_hash: str, metadata: dict = None):
        """æ–°å¢è¨˜éŒ„åˆ°ç´¢å¼•"""
        self.records[record_id] = {
            'content_hash': content_hash,
            'indexed_at': datetime.now().isoformat(),
            **(metadata or {})
        }
    
    def remove_record(self, record_id: str):
        """å¾ç´¢å¼•ç§»é™¤è¨˜éŒ„"""
        self.records.pop(record_id, None)
    
    def update_record(self, record_id: str, content_hash: str, metadata: dict = None):
        """æ›´æ–°è¨˜éŒ„"""
        if record_id in self.records:
            self.records[record_id]['content_hash'] = content_hash
            self.records[record_id]['updated_at'] = datetime.now().isoformat()
            if metadata:
                self.records[record_id].update(metadata)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¢é‡å·®ç•°è¨ˆç®—
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compute_diff(csv_path: str, index: RecordIndex) -> Tuple[List[dict], List[str], List[dict]]:
    """
    è¨ˆç®— CSV èˆ‡ç¾æœ‰ç´¢å¼•çš„å·®ç•°
    
    Returns:
        (to_add, to_delete, to_update)
        - to_add: éœ€è¦æ–°å¢çš„è¨˜éŒ„ [{'record_id': ..., 'row': ..., 'content_hash': ...}, ...]
        - to_delete: éœ€è¦åˆªé™¤çš„è¨˜éŒ„ ID [record_id, ...]
        - to_update: éœ€è¦æ›´æ–°çš„è¨˜éŒ„ [{'record_id': ..., 'row': ..., 'content_hash': ...}, ...]
    """
    if not _HAS_PANDAS:
        return [], [], []
    
    if not os.path.exists(csv_path):
        return [], [], []
    
    # è®€å– CSV
    try:
        df = pd.read_csv(csv_path, encoding='utf-8')
    except Exception as e:
        print(f"âŒ è®€å– CSV å¤±æ•—: {e}")
        return [], [], []
    
    # è¨ˆç®— CSV ä¸­æ¯ç­†è¨˜éŒ„çš„ ID å’Œ hash
    csv_records: Dict[str, dict] = {}
    for idx, row in df.iterrows():
        row_dict = row.to_dict()
        record_id = generate_record_id(row_dict)
        content_hash = generate_content_hash(row_dict)
        csv_records[record_id] = {
            'row': row_dict,
            'row_num': idx,
            'content_hash': content_hash
        }
    
    csv_ids = set(csv_records.keys())
    indexed_ids = index.get_all_ids()
    
    # è¨ˆç®—å·®ç•°
    new_ids = csv_ids - indexed_ids  # CSV æœ‰ï¼Œç´¢å¼•æ²’æœ‰ â†’ æ–°å¢
    deleted_ids = indexed_ids - csv_ids  # ç´¢å¼•æœ‰ï¼ŒCSV æ²’æœ‰ â†’ åˆªé™¤
    common_ids = csv_ids & indexed_ids  # å…©é‚Šéƒ½æœ‰ â†’ æª¢æŸ¥æ˜¯å¦æ›´æ–°
    
    # éœ€è¦æ–°å¢çš„è¨˜éŒ„
    to_add = []
    for rid in new_ids:
        info = csv_records[rid]
        to_add.append({
            'record_id': rid,
            'row': info['row'],
            'content_hash': info['content_hash']
        })
    
    # éœ€è¦åˆªé™¤çš„è¨˜éŒ„
    to_delete = list(deleted_ids)
    
    # éœ€è¦æ›´æ–°çš„è¨˜éŒ„ï¼ˆå…§å®¹ hash è®Šäº†ï¼‰
    to_update = []
    for rid in common_ids:
        csv_hash = csv_records[rid]['content_hash']
        indexed_record = index.get_record(rid)
        if indexed_record and indexed_record.get('content_hash') != csv_hash:
            to_update.append({
                'record_id': rid,
                'row': csv_records[rid]['row'],
                'content_hash': csv_hash
            })
    
    return to_add, to_delete, to_update

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¢é‡ Embedding åŸ·è¡Œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_document_from_row(row: dict, record_id: str) -> 'Document':
    """å¾ CSV è¡Œå»ºç«‹ LangChain Document"""
    from langchain_core.documents import Document
    from utils import DocumentType
    
    # å»ºæ§‹å…§å®¹
    parts = [f"**è¨˜éŒ„ID**: {record_id}"]
    
    field_map = {
        'Date': 'æ—¥æœŸ',
        'Worker': 'æ¥­å‹™äººå“¡',
        'Customer': 'å®¢æˆ¶',
        'Class': 'æ´»å‹•é¡å‹',
        'Content': 'æ´»å‹•å…§å®¹',
        'Depart': 'éƒ¨é–€',
        'Manager': 'ä¸»ç®¡',
    }
    
    for en_name, zh_name in field_map.items():
        val = row.get(en_name, '')
        if val and str(val).strip() and str(val).lower() != 'nan':
            parts.append(f"**{zh_name}**: {str(val).strip()}")
    
    content = "\n".join(parts)
    
    # å»ºæ§‹ metadata
    metadata = {
        'doc_type': DocumentType.BUSINESS.value,
        'source': 'business_csv',
        'record_id': record_id,
        'date': str(row.get('Date', '')),
        'worker': str(row.get('Worker', '')),
        'customer': str(row.get('Customer', '')),
        'class': str(row.get('Class', '')),
        'depart': str(row.get('Depart', '')),
    }
    
    return Document(page_content=content, metadata=metadata)


def apply_incremental_changes(
    vectordb,
    index: RecordIndex,
    to_add: List[dict],
    to_delete: List[str],
    to_update: List[dict],
    batch_size: int = 100
) -> dict:
    """
    æ‡‰ç”¨å¢é‡è®Šæ›´åˆ°å‘é‡åº«
    
    Returns:
        {'added': int, 'deleted': int, 'updated': int}
    """
    stats = {'added': 0, 'deleted': 0, 'updated': 0}
    
    # 1. åˆªé™¤è¨˜éŒ„
    if to_delete:
        print(f"   ğŸ—‘ï¸ åˆªé™¤ {len(to_delete)} ç­†èˆŠè¨˜éŒ„...")
        try:
            # ChromaDB ä½¿ç”¨ ids åƒæ•¸åˆªé™¤
            vectordb._collection.delete(ids=to_delete)
            for rid in to_delete:
                index.remove_record(rid)
            stats['deleted'] = len(to_delete)
        except Exception as e:
            print(f"   âš ï¸ åˆªé™¤å¤±æ•—: {e}")
    
    # 2. æ›´æ–°è¨˜éŒ„ï¼ˆå…ˆåˆªå¾ŒåŠ ï¼‰
    if to_update:
        print(f"   ğŸ”„ æ›´æ–° {len(to_update)} ç­†è¨˜éŒ„...")
        try:
            update_ids = [r['record_id'] for r in to_update]
            vectordb._collection.delete(ids=update_ids)
            
            # é‡æ–°åŠ å…¥
            docs = [build_document_from_row(r['row'], r['record_id']) for r in to_update]
            vectordb.add_documents(docs, ids=update_ids)
            
            for r in to_update:
                index.update_record(r['record_id'], r['content_hash'])
            stats['updated'] = len(to_update)
        except Exception as e:
            print(f"   âš ï¸ æ›´æ–°å¤±æ•—: {e}")
    
    # 3. æ–°å¢è¨˜éŒ„ï¼ˆåˆ†æ‰¹ï¼‰
    if to_add:
        print(f"   â• æ–°å¢ {len(to_add)} ç­†è¨˜éŒ„...")
        total = len(to_add)
        for i in range(0, total, batch_size):
            batch = to_add[i:i+batch_size]
            try:
                docs = [build_document_from_row(r['row'], r['record_id']) for r in batch]
                ids = [r['record_id'] for r in batch]
                vectordb.add_documents(docs, ids=ids)
                
                for r in batch:
                    index.add_record(r['record_id'], r['content_hash'], {
                        'date': str(r['row'].get('Date', '')),
                        'worker': str(r['row'].get('Worker', ''))
                    })
                
                if (i + batch_size) % 5000 < batch_size:
                    print(f"      ğŸ“¥ æ–°å¢é€²åº¦: {min(i + batch_size, total):,} / {total:,}")
            except Exception as e:
                print(f"   âš ï¸ æ‰¹æ¬¡æ–°å¢å¤±æ•—: {e}")
        
        stats['added'] = len(to_add)
    
    # å„²å­˜ç´¢å¼•
    index.save()
    
    return stats

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¸»è¦å…¥å£å‡½æ•¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def incremental_embed_business_csv(
    csv_path: str,
    vectordb,
    index_dir: str,
    force_rebuild: bool = False
) -> dict:
    """
    å¢é‡æ›´æ–°æ¥­å‹™ CSV çš„ embedding
    
    Args:
        csv_path: CSV æª”æ¡ˆè·¯å¾‘
        vectordb: ChromaDB å‘é‡åº«å¯¦ä¾‹
        index_dir: ç´¢å¼•æª”æ¡ˆå„²å­˜ç›®éŒ„
        force_rebuild: æ˜¯å¦å¼·åˆ¶å…¨éƒ¨é‡å»º
    
    Returns:
        {'added': int, 'deleted': int, 'updated': int, 'total': int, 'action': str}
    """
    index_path = os.path.join(index_dir, INDEX_FILE_NAME)
    index = RecordIndex(index_path)
    
    # å¼·åˆ¶é‡å»ºï¼šæ¸…ç©ºç´¢å¼•
    if force_rebuild:
        print("ğŸ”„ å¼·åˆ¶é‡å»ºæ¨¡å¼ï¼Œæ¸…ç©ºç¾æœ‰ç´¢å¼•...")
        index.records = {}
        index.save()
    
    # è¨ˆç®—å·®ç•°
    print("ğŸ“Š åˆ†æ CSV è®Šæ›´...")
    to_add, to_delete, to_update = compute_diff(csv_path, index)
    
    total_changes = len(to_add) + len(to_delete) + len(to_update)
    
    if total_changes == 0:
        print("âœ… æ¥­å‹™è³‡æ–™ç„¡è®Šæ›´")
        return {
            'added': 0, 'deleted': 0, 'updated': 0,
            'total': len(index.records),
            'action': 'no_change'
        }
    
    print(f"ğŸ“‹ è®Šæ›´æ‘˜è¦: æ–°å¢ {len(to_add)} | åˆªé™¤ {len(to_delete)} | æ›´æ–° {len(to_update)}")
    
    # æ‡‰ç”¨è®Šæ›´
    stats = apply_incremental_changes(
        vectordb, index,
        to_add, to_delete, to_update,
        batch_size=100
    )
    
    stats['total'] = len(index.records)
    stats['action'] = 'incremental_update'
    
    print(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: å…± {stats['total']:,} ç­†è¨˜éŒ„")
    
    return stats


def get_index_stats(index_dir: str) -> dict:
    """å–å¾—ç´¢å¼•çµ±è¨ˆè³‡è¨Š"""
    index_path = os.path.join(index_dir, INDEX_FILE_NAME)
    
    if not os.path.exists(index_path):
        return {'exists': False, 'total_records': 0}
    
    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return {
            'exists': True,
            'total_records': data.get('total_records', 0),
            'updated_at': data.get('updated_at', 'unknown')
        }
    except Exception:
        return {'exists': False, 'total_records': 0}
