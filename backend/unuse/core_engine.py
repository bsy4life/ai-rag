# core_engine.py - SanShin AI æ ¸å¿ƒ RAG å¼•æ“ï¼ˆç”Ÿç”¢ç´šï¼‰
"""
å®Œæ•´çš„ RAG å¼•æ“å¯¦ä½œï¼š
1. ä¸‰å±¤æª¢ç´¢ï¼šç²¾ç¢ºåŒ¹é… + BM25 + å‘é‡
2. åˆ†å±¤ LLM é¸æ“‡
3. Reranker æ”¯æ´
4. ä¾†æºè¿½è¹¤
5. å¿«å–æ©Ÿåˆ¶
6. æˆæœ¬ä¼°ç®—
"""

import os
import re
import json
import hashlib
import logging
from typing import List, Dict, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from datetime import datetime
from threading import Lock
from functools import lru_cache

# é…ç½®
from config import (
    EMBEDDING_MODEL, LLM_CONFIGS, RETRIEVER_CONFIGS, CHUNK_CONFIGS,
    PROMPTS, RERANKER_CONFIG, CACHE_CONFIG, SOURCE_TRACKING,
    KEYWORD_PATTERNS, CHINESE_STOPWORDS, COMPLEXITY_THRESHOLDS,
    COMPARISON_KEYWORDS, ANALYSIS_KEYWORDS,
    VECTOR_DB_DIR, DATA_DIR, TECHNICAL_DATA_DIR,
    get_llm_config, get_retriever_config,
)

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è³‡æ–™çµæ§‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class SearchResult:
    """æœå°‹çµæœ"""
    content: str
    source: str
    doc_type: str
    score: float = 0.0
    metadata: Dict = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}

@dataclass
class QueryResult:
    """æŸ¥è©¢çµæœ"""
    answer: str
    sources: List[str]
    source_type: str  # technical, business, personal, mixed
    images: List[Dict] = None
    metadata: Dict = None
    cost_estimate: Dict = None
    from_cache: bool = False
    
    def __post_init__(self):
        if self.images is None:
            self.images = []
        if self.metadata is None:
            self.metadata = {}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# é—œéµå­—ç´¢å¼•
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class KeywordIndex:
    """å€’æ’é—œéµå­—ç´¢å¼•"""
    
    def __init__(self, index_path: str = None):
        self.index: Dict[str, List[Tuple[str, float]]] = {}  # keyword -> [(doc_id, weight), ...]
        self.doc_keywords: Dict[str, List[str]] = {}
        self.index_path = index_path
        self._lock = Lock()
        
        if index_path and os.path.exists(index_path):
            self.load()
    
    def add_document(self, doc_id: str, text: str, weight: float = 1.0):
        """æ·»åŠ æ–‡ä»¶åˆ°ç´¢å¼•"""
        keywords = self._extract_keywords(text)
        
        with self._lock:
            self.doc_keywords[doc_id] = keywords
            
            for kw in keywords:
                kw_lower = kw.lower()
                if kw_lower not in self.index:
                    self.index[kw_lower] = []
                
                # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = [i for i, (d, _) in enumerate(self.index[kw_lower]) if d == doc_id]
                if existing:
                    self.index[kw_lower][existing[0]] = (doc_id, weight)
                else:
                    self.index[kw_lower].append((doc_id, weight))
    
    def _extract_keywords(self, text: str) -> List[str]:
        """æå–é—œéµå­—"""
        keywords = set()
        
        # ä½¿ç”¨æ­£å‰‡æ¨¡å¼
        for pattern in KEYWORD_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            keywords.update(m.upper() if len(m) <= 10 else m for m in matches)
        
        # ä¸­æ–‡è©å½™
        chinese = re.findall(r'[\u4e00-\u9fa5]{2,6}', text)
        keywords.update(w for w in chinese if w not in CHINESE_STOPWORDS)
        
        return list(keywords)
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[str, float]]:
        """æœå°‹é—œéµå­—"""
        query_keywords = self._extract_keywords(query)
        query_words = set(query.lower().split())
        all_terms = set(kw.lower() for kw in query_keywords) | query_words
        
        doc_scores: Dict[str, float] = {}
        
        for term in all_terms:
            # ç²¾ç¢ºåŒ¹é…ï¼ˆæ¬Šé‡ 2ï¼‰
            if term in self.index:
                for doc_id, weight in self.index[term]:
                    doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 2 * weight
            
            # éƒ¨åˆ†åŒ¹é…ï¼ˆæ¬Šé‡ 1ï¼‰
            for indexed_kw, doc_list in self.index.items():
                if len(term) >= 3 and (term in indexed_kw or indexed_kw in term):
                    for doc_id, weight in doc_list:
                        doc_scores[doc_id] = doc_scores.get(doc_id, 0) + weight
        
        # æ’åº
        results = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def save(self):
        """å„²å­˜ç´¢å¼•"""
        if self.index_path:
            with self._lock:
                data = {
                    "index": {k: list(v) for k, v in self.index.items()},
                    "doc_keywords": self.doc_keywords,
                }
                with open(self.index_path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False)
    
    def load(self):
        """è¼‰å…¥ç´¢å¼•"""
        if self.index_path and os.path.exists(self.index_path):
            with open(self.index_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                self.index = {k: [tuple(x) for x in v] for k, v in data.get("index", {}).items()}
                self.doc_keywords = data.get("doc_keywords", {})

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æŸ¥è©¢å¿«å–
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QueryCache:
    """æŸ¥è©¢å¿«å–"""
    
    def __init__(self, config=CACHE_CONFIG):
        self.config = config
        self.cache: Dict[str, Tuple[Any, float]] = {}
        self._lock = Lock()
        
        if config.backend == "file" and config.file_path:
            self._load_file_cache()
    
    def _make_key(self, query: str, mode: str) -> str:
        """ç”Ÿæˆå¿«å–éµ"""
        return hashlib.md5(f"{query}:{mode}".encode()).hexdigest()
    
    def get(self, query: str, mode: str) -> Optional[QueryResult]:
        """å–å¾—å¿«å–"""
        if not self.config.enabled:
            return None
        
        key = self._make_key(query, mode)
        
        with self._lock:
            if key in self.cache:
                result, timestamp = self.cache[key]
                # æª¢æŸ¥æ˜¯å¦éæœŸ
                if datetime.now().timestamp() - timestamp < self.config.ttl:
                    result.from_cache = True
                    return result
                else:
                    del self.cache[key]
        
        return None
    
    def set(self, query: str, mode: str, result: QueryResult):
        """è¨­å®šå¿«å–"""
        if not self.config.enabled:
            return
        
        key = self._make_key(query, mode)
        
        with self._lock:
            # æª¢æŸ¥å¤§å°é™åˆ¶
            if len(self.cache) >= self.config.max_size:
                # ç§»é™¤æœ€èˆŠçš„
                oldest = min(self.cache.items(), key=lambda x: x[1][1])
                del self.cache[oldest[0]]
            
            self.cache[key] = (result, datetime.now().timestamp())
        
        # ç•°æ­¥ä¿å­˜åˆ°æª”æ¡ˆ
        if self.config.backend == "file":
            self._save_file_cache()
    
    def _load_file_cache(self):
        """è¼‰å…¥æª”æ¡ˆå¿«å–"""
        if os.path.exists(self.config.file_path):
            try:
                with open(self.config.file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for key, (result_dict, ts) in data.items():
                        self.cache[key] = (QueryResult(**result_dict), ts)
            except:
                pass
    
    def _save_file_cache(self):
        """ä¿å­˜æª”æ¡ˆå¿«å–"""
        try:
            data = {
                key: (asdict(result), ts)
                for key, (result, ts) in self.cache.items()
            }
            with open(self.config.file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
        except:
            pass
    
    def clear(self):
        """æ¸…ç©ºå¿«å–"""
        with self._lock:
            self.cache.clear()
        if self.config.backend == "file" and os.path.exists(self.config.file_path):
            os.remove(self.config.file_path)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Reranker
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Reranker:
    """é‡æ’åºå™¨"""
    
    _instance = None
    _lock = Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.config = RERANKER_CONFIG
        self.model = None
        
        if self.config.enabled:
            self._load_model()
        
        self._initialized = True
    
    def _load_model(self):
        """è¼‰å…¥æ¨¡å‹"""
        if self.config.type == "cohere":
            try:
                from langchain_cohere import CohereRerank
                api_key = os.getenv(self.config.cohere_api_key_env)
                if api_key:
                    self.model = CohereRerank(
                        model=self.config.cohere_model,
                        top_n=self.config.top_n
                    )
                    logger.info("âœ… Cohere Reranker å·²è¼‰å…¥")
            except ImportError:
                logger.warning("éœ€è¦å®‰è£ langchain-cohere")
        
        elif self.config.type == "local":
            try:
                from sentence_transformers import CrossEncoder
                self.model = CrossEncoder(self.config.local_model)
                logger.info(f"âœ… æœ¬åœ° Reranker å·²è¼‰å…¥: {self.config.local_model}")
            except ImportError:
                logger.warning("éœ€è¦å®‰è£ sentence-transformers")
    
    def rerank(self, query: str, documents: List[SearchResult]) -> List[SearchResult]:
        """é‡æ’åº"""
        if not self.config.enabled or not self.model or not documents:
            return documents[:self.config.top_n]
        
        try:
            if self.config.type == "local":
                pairs = [(query, doc.content) for doc in documents]
                scores = self.model.predict(pairs)
                
                for doc, score in zip(documents, scores):
                    doc.score = float(score)
                
                documents.sort(key=lambda x: x.score, reverse=True)
                return documents[:self.config.top_n]
            
            elif self.config.type == "cohere":
                # Cohere éœ€è¦ LangChain Document
                from langchain_core.documents import Document
                lc_docs = [Document(page_content=d.content, metadata=d.metadata) for d in documents]
                reranked = self.model.compress_documents(lc_docs, query)
                
                results = []
                for doc in reranked[:self.config.top_n]:
                    results.append(SearchResult(
                        content=doc.page_content,
                        source=doc.metadata.get("source", ""),
                        doc_type=doc.metadata.get("doc_type", "unknown"),
                        metadata=doc.metadata,
                    ))
                return results
        
        except Exception as e:
            logger.warning(f"Rerank å¤±æ•—: {e}")
        
        return documents[:self.config.top_n]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¤‡é›œåº¦è©•ä¼°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def estimate_complexity(query: str, doc_count: int = 0) -> str:
    """è©•ä¼°æŸ¥è©¢è¤‡é›œåº¦"""
    score = 0
    
    # å•é¡Œé•·åº¦
    if len(query) > COMPLEXITY_THRESHOLDS["query_length"]:
        score += 1
    
    # ç”¢å“å‹è™Ÿæ•¸é‡
    model_count = sum(len(re.findall(p, query)) for p in KEYWORD_PATTERNS[:3])
    if model_count >= COMPLEXITY_THRESHOLDS["model_count"]:
        score += 1
    
    # æ¯”è¼ƒé¡å•é¡Œ
    if any(kw in query for kw in COMPARISON_KEYWORDS):
        score += 1
    
    # åˆ†æé¡å•é¡Œ
    if any(kw in query for kw in ANALYSIS_KEYWORDS):
        score += 1
    
    # æ–‡æª”æ•¸é‡
    if doc_count > COMPLEXITY_THRESHOLDS["doc_count"]:
        score += 1
    
    return "complex" if score >= 2 else "simple"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æˆæœ¬ä¼°ç®—
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def estimate_cost(input_tokens: int, output_tokens: int, model: str) -> Dict:
    """ä¼°ç®— API æˆæœ¬"""
    from config import TOKEN_PRICES
    
    prices = TOKEN_PRICES.get(model, TOKEN_PRICES["gpt-4o-mini"])
    
    input_cost = (input_tokens / 1_000_000) * prices.get("input", 0)
    output_cost = (output_tokens / 1_000_000) * prices.get("output", 0)
    
    return {
        "model": model,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "input_cost_usd": round(input_cost, 6),
        "output_cost_usd": round(output_cost, 6),
        "total_cost_usd": round(input_cost + output_cost, 6),
    }

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ ¸å¿ƒå¼•æ“
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RAGEngine:
    """RAG æ ¸å¿ƒå¼•æ“"""
    
    def __init__(self):
        self.keyword_index = KeywordIndex(
            os.path.join(VECTOR_DB_DIR, "keyword_index.json")
        )
        self.cache = QueryCache()
        self.reranker = Reranker()
        
        self._vectordb = None
        self._bm25 = None
        self._llm = None
        self._embedding = None
        
        self._lock = Lock()
        self._initialized = False
    
    def initialize(self):
        """åˆå§‹åŒ–å¼•æ“"""
        if self._initialized:
            return
        
        with self._lock:
            if self._initialized:
                return
            
            logger.info("ğŸš€ åˆå§‹åŒ– RAG å¼•æ“...")
            
            # è¼‰å…¥ Embedding
            self._load_embedding()
            
            # è¼‰å…¥å‘é‡åº«
            self._load_vectordb()
            
            # è¼‰å…¥ BM25
            self._load_bm25()
            
            self._initialized = True
            logger.info("âœ… RAG å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    def _load_embedding(self):
        """è¼‰å…¥ Embedding æ¨¡å‹"""
        try:
            from langchain_openai import OpenAIEmbeddings
            self._embedding = OpenAIEmbeddings(model=EMBEDDING_MODEL)
            logger.info(f"âœ… Embedding æ¨¡å‹: {EMBEDDING_MODEL}")
        except Exception as e:
            logger.error(f"âŒ Embedding è¼‰å…¥å¤±æ•—: {e}")
    
    def _load_vectordb(self):
        """è¼‰å…¥å‘é‡åº«"""
        try:
            from langchain_chroma import Chroma
            import chromadb
            
            client = chromadb.PersistentClient(path=VECTOR_DB_DIR)
            self._vectordb = Chroma(
                client=client,
                collection_name="technical_docs",
                embedding_function=self._embedding
            )
            
            # å–å¾—æ–‡ä»¶æ•¸é‡
            try:
                count = self._vectordb._collection.count()
                logger.info(f"âœ… å‘é‡åº«å·²è¼‰å…¥: {count} å€‹æ–‡ä»¶")
            except:
                pass
        except Exception as e:
            logger.error(f"âŒ å‘é‡åº«è¼‰å…¥å¤±æ•—: {e}")
    
    def _load_bm25(self):
        """è¼‰å…¥ BM25 ç´¢å¼•"""
        try:
            from langchain_community.retrievers import BM25Retriever
            
            # å¾å‘é‡åº«å–å¾—æ‰€æœ‰æ–‡ä»¶
            if self._vectordb:
                docs = self._vectordb.get()
                if docs and docs.get("documents"):
                    from langchain_core.documents import Document
                    lc_docs = [
                        Document(
                            page_content=content,
                            metadata={"id": id}
                        )
                        for content, id in zip(docs["documents"], docs["ids"])
                    ]
                    self._bm25 = BM25Retriever.from_documents(lc_docs)
                    self._bm25.k = 10
                    logger.info(f"âœ… BM25 ç´¢å¼•å·²å»ºç«‹: {len(lc_docs)} å€‹æ–‡ä»¶")
        except Exception as e:
            logger.warning(f"âš ï¸ BM25 è¼‰å…¥å¤±æ•—: {e}")
    
    def _get_llm(self, config):
        """å–å¾— LLM"""
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(**config.to_dict())
    
    def search(
        self,
        query: str,
        doc_type: str = "technical",
        top_k: int = 10,
        use_rerank: bool = True,
    ) -> List[SearchResult]:
        """
        ä¸‰å±¤æ··åˆæœå°‹
        
        1. é—œéµå­—ç²¾ç¢ºåŒ¹é…
        2. BM25 é—œéµå­—æœå°‹
        3. å‘é‡èªç¾©æœå°‹
        """
        self.initialize()
        
        results: List[SearchResult] = []
        seen_ids = set()
        
        # 1. é—œéµå­—ç²¾ç¢ºåŒ¹é…ï¼ˆæœ€é«˜å„ªå…ˆï¼‰
        kw_results = self.keyword_index.search(query, top_k=5)
        for doc_id, score in kw_results:
            if doc_id not in seen_ids:
                # å¾å‘é‡åº«å–å¾—å…§å®¹
                try:
                    doc = self._vectordb._collection.get(ids=[doc_id])
                    if doc and doc.get("documents"):
                        results.append(SearchResult(
                            content=doc["documents"][0],
                            source=doc.get("metadatas", [{}])[0].get("source", ""),
                            doc_type=doc_type,
                            score=score * 2,  # åŠ æ¬Š
                            metadata={"match_type": "keyword", "doc_id": doc_id}
                        ))
                        seen_ids.add(doc_id)
                except:
                    pass
        
        # 2. BM25 æœå°‹
        if self._bm25:
            try:
                bm25_docs = self._bm25.invoke(query)
                for doc in bm25_docs[:top_k]:
                    doc_id = doc.metadata.get("id", "")
                    if doc_id and doc_id not in seen_ids:
                        results.append(SearchResult(
                            content=doc.page_content,
                            source=doc.metadata.get("source", ""),
                            doc_type=doc_type,
                            score=1.5,
                            metadata={"match_type": "bm25", "doc_id": doc_id}
                        ))
                        seen_ids.add(doc_id)
            except Exception as e:
                logger.warning(f"BM25 æœå°‹å¤±æ•—: {e}")
        
        # 3. å‘é‡æœå°‹
        if self._vectordb:
            try:
                config = get_retriever_config(doc_type)
                vector_docs = self._vectordb.similarity_search_with_score(
                    query, k=config.k
                )
                for doc, score in vector_docs:
                    doc_id = doc.metadata.get("id", hash(doc.page_content))
                    if doc_id not in seen_ids:
                        results.append(SearchResult(
                            content=doc.page_content,
                            source=doc.metadata.get("source", ""),
                            doc_type=doc_type,
                            score=1.0 / (1.0 + score),  # è½‰æ›è·é›¢ç‚ºåˆ†æ•¸
                            metadata={"match_type": "vector", **doc.metadata}
                        ))
                        seen_ids.add(doc_id)
            except Exception as e:
                logger.warning(f"å‘é‡æœå°‹å¤±æ•—: {e}")
        
        # 4. Rerank
        if use_rerank and results:
            results = self.reranker.rerank(query, results)
        else:
            # æŒ‰åˆ†æ•¸æ’åº
            results.sort(key=lambda x: x.score, reverse=True)
            results = results[:top_k]
        
        return results
    
    def generate_answer(
        self,
        query: str,
        context: List[SearchResult],
        doc_type: str = "technical",
    ) -> Tuple[str, Dict]:
        """ç”Ÿæˆå›ç­”"""
        # è©•ä¼°è¤‡é›œåº¦
        complexity = estimate_complexity(query, len(context))
        
        # é¸æ“‡ LLM
        llm_config = get_llm_config(doc_type, complexity)
        llm = self._get_llm(llm_config)
        
        # æº–å‚™ä¸Šä¸‹æ–‡
        context_text = "\n\n---\n\n".join([
            f"ã€ä¾†æº: {r.source}ã€‘\n{r.content}"
            for r in context
        ])
        
        # é¸æ“‡ Prompt
        prompt_template = PROMPTS.get(doc_type, PROMPTS["technical"])
        prompt = prompt_template.format(context=context_text, input=query)
        
        # ç”Ÿæˆå›ç­”
        try:
            response = llm.invoke(prompt)
            answer = response.content if hasattr(response, 'content') else str(response)
        except Exception as e:
            logger.error(f"LLM ç”Ÿæˆå¤±æ•—: {e}")
            answer = f"ç”Ÿæˆå›ç­”æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"
        
        # æ·»åŠ ä¾†æº
        if SOURCE_TRACKING.enabled and SOURCE_TRACKING.show_in_response:
            sources = list(set(r.source for r in context if r.source))[:SOURCE_TRACKING.max_sources]
            if sources:
                answer += "\n\n---\nğŸ“š **åƒè€ƒä¾†æº**ï¼š" + "ã€".join(sources)
        
        # æˆæœ¬ä¼°ç®—
        input_tokens = len(prompt) // 2
        output_tokens = len(answer) // 2
        cost = estimate_cost(input_tokens, output_tokens, llm_config.model)
        
        return answer, cost
    
    def query(
        self,
        query: str,
        mode: str = "smart",
        use_cache: bool = True,
    ) -> QueryResult:
        """
        å®Œæ•´æŸ¥è©¢æµç¨‹
        
        Args:
            query: æŸ¥è©¢å…§å®¹
            mode: æ¨¡å¼ (smart, technical, business, personal)
            use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
        
        Returns:
            QueryResult
        """
        if not query or not query.strip():
            return QueryResult(
                answer="è«‹è¼¸å…¥æœ‰æ•ˆçš„å•é¡Œã€‚",
                sources=[],
                source_type="error",
            )
        
        query = query.strip()
        
        # æª¢æŸ¥å¿«å–
        if use_cache:
            cached = self.cache.get(query, mode)
            if cached:
                return cached
        
        # åˆ¤æ–·æŸ¥è©¢é¡å‹
        if mode == "smart":
            doc_type = self._classify_query(query)
        else:
            doc_type = mode if mode in ["technical", "business", "personal"] else "technical"
        
        # æœå°‹
        search_results = self.search(query, doc_type)
        
        if not search_results:
            return QueryResult(
                answer="æœªæ‰¾åˆ°ç›¸é—œè³‡æ–™ã€‚è«‹å˜—è©¦ä¸åŒçš„é—œéµå­—æˆ–æŸ¥è©¢æ–¹å¼ã€‚",
                sources=[],
                source_type=doc_type,
            )
        
        # ç”Ÿæˆå›ç­”
        answer, cost = self.generate_answer(query, search_results, doc_type)
        
        # çµ„è£çµæœ
        result = QueryResult(
            answer=answer,
            sources=[r.source for r in search_results if r.source],
            source_type=doc_type,
            metadata={
                "search_count": len(search_results),
                "complexity": estimate_complexity(query, len(search_results)),
            },
            cost_estimate=cost,
        )
        
        # å­˜å…¥å¿«å–
        if use_cache:
            self.cache.set(query, mode, result)
        
        return result
    
    def _classify_query(self, query: str) -> str:
        """åˆ†é¡æŸ¥è©¢é¡å‹"""
        # æ¥­å‹™é—œéµå­—
        business_keywords = [
            'å®¢æˆ¶', 'æ¥­å‹™', 'æ‹œè¨ª', 'é€è²¨', 'è¨‚å–®', 'ç‡Ÿæ¥­æ‰€',
            'æ´»å‹•', 'æ—¥å ±', 'çµ±è¨ˆ', 'åˆ†æ', 'æ¥­ç¸¾',
        ]
        
        # æŠ€è¡“é—œéµå­—
        technical_keywords = [
            'è¦æ ¼', 'å‹è™Ÿ', 'ç”¢å“', 'å®‰è£', 'ç¶­ä¿®', 'æ•…éšœ',
            'æ°£ç¼¸', 'æ²¹å£“', 'å¢Šç‰‡', 'é›»ç£é–¥', 'SMC', 'YUKEN',
        ]
        
        query_lower = query.lower()
        
        biz_score = sum(1 for kw in business_keywords if kw in query)
        tech_score = sum(1 for kw in technical_keywords if kw in query_lower)
        
        if biz_score > tech_score:
            return "business"
        else:
            return "technical"
    
    def reload(self):
        """é‡æ–°è¼‰å…¥"""
        with self._lock:
            self._initialized = False
            self._vectordb = None
            self._bm25 = None
        
        self.initialize()
        self.cache.clear()
        logger.info("âœ… RAG å¼•æ“å·²é‡æ–°è¼‰å…¥")
    
    def get_stats(self) -> Dict:
        """å–å¾—çµ±è¨ˆè³‡è¨Š"""
        stats = {
            "initialized": self._initialized,
            "vectordb_loaded": self._vectordb is not None,
            "bm25_loaded": self._bm25 is not None,
            "reranker_enabled": RERANKER_CONFIG.enabled,
            "cache_enabled": CACHE_CONFIG.enabled,
            "cache_size": len(self.cache.cache),
        }
        
        if self._vectordb:
            try:
                stats["vectordb_count"] = self._vectordb._collection.count()
            except:
                pass
        
        return stats

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…¨åŸŸå¯¦ä¾‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_engine: Optional[RAGEngine] = None
_engine_lock = Lock()

def get_engine() -> RAGEngine:
    """å–å¾— RAG å¼•æ“å¯¦ä¾‹"""
    global _engine
    
    if _engine is None:
        with _engine_lock:
            if _engine is None:
                _engine = RAGEngine()
    
    return _engine

def query(query: str, mode: str = "smart") -> QueryResult:
    """ä¾¿æ·æŸ¥è©¢å‡½æ•¸"""
    return get_engine().query(query, mode)

def reload_engine():
    """é‡æ–°è¼‰å…¥å¼•æ“"""
    get_engine().reload()
