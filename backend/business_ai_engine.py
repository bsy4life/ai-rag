# business_ai_engine.py - ç´” AI é©…å‹•çš„æ¥­å‹™æ™ºèƒ½æŸ¥è©¢å¼•æ“
"""
åŠŸèƒ½ï¼š
1. AI æ„åœ–è§£æ - ä¸å†ç¡¬ç·¨ç¢¼è¦å‰‡ï¼Œç”± LLM ç†è§£æŸ¥è©¢æ„åœ–
2. å‹•æ…‹ä»£ç¢¼ç”Ÿæˆ - AI ç”Ÿæˆ Pandas æŸ¥è©¢ä»£ç¢¼
3. BI åˆ†æå±¤ - è¶¨å‹¢åˆ†æã€ç•°å¸¸åµæ¸¬ã€æ™ºèƒ½å»ºè­°
4. è‡ªç„¶èªè¨€å›è¦† - å°‡æ•¸æ“šè½‰ç‚ºäººé¡æ˜“è®€çš„æ´å¯Ÿ
5. æ”¯æ´å¾ PostgreSQL è³‡æ–™åº«è®€å–ï¼ˆä¸å†ä¾è³´ CSVï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
    from business_ai_engine import BusinessAIEngine
    
    engine = BusinessAIEngine()
    result = engine.query("å°å—ç‡Ÿæ¥­æ‰€æœ€è¿‘ä¸€å€‹æœˆçš„æ¥­ç¸¾å¦‚ä½•ï¼Ÿæœ‰ä»€éº¼å€¼å¾—æ³¨æ„çš„è¶¨å‹¢ï¼Ÿ")
    print(result['answer'])
"""

import os
import re
import json
import logging
import traceback
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from enum import Enum

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾è³´æª¢æŸ¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    import pandas as pd
    _HAS_PANDAS = True
except ImportError:
    _HAS_PANDAS = False
    logger.warning("pandas æœªå®‰è£ï¼Œæ¥­å‹™ AI å¼•æ“å°‡ç„¡æ³•é‹ä½œ")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è³‡æ–™åº«é€£æ¥ï¼ˆPostgreSQLï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
    _HAS_PSYCOPG2 = True
except ImportError:
    _HAS_PSYCOPG2 = False
    logger.warning("psycopg2 æœªå®‰è£ï¼Œç„¡æ³•å¾è³‡æ–™åº«è®€å–")


def get_db_connection():
    """å–å¾—è³‡æ–™åº«é€£æ¥"""
    if not _HAS_PSYCOPG2:
        return None
    
    try:
        conn = psycopg2.connect(
            host=os.getenv('PG_HOST', '192.168.0.227'),
            port=os.getenv('PG_PORT', '5432'),
            database=os.getenv('PG_NAME', 'ai_db'),
            user=os.getenv('PG_USER', 'ai_user'),
            password=os.getenv('PG_PASSWORD', 'sanshin2025'),
            cursor_factory=RealDictCursor
        )
        return conn
    except Exception as e:
        logger.error(f"è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ•¸æ“šçµæ§‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class QueryIntent(Enum):
    """æŸ¥è©¢æ„åœ–é¡å‹"""
    AGGREGATE = "aggregate"      # èšåˆçµ±è¨ˆï¼ˆç¸½æ•¸ã€å¹³å‡ç­‰ï¼‰
    LIST = "list"                # åˆ—å‡ºæ˜ç´°
    TREND = "trend"              # è¶¨å‹¢åˆ†æ
    COMPARE = "compare"          # æ¯”è¼ƒåˆ†æ
    ANOMALY = "anomaly"          # ç•°å¸¸åµæ¸¬
    FORECAST = "forecast"        # é æ¸¬
    RANKING = "ranking"          # æ’å
    SEARCH = "search"            # æœå°‹ç‰¹å®šè¨˜éŒ„


@dataclass
class ParsedIntent:
    """è§£æå¾Œçš„æŸ¥è©¢æ„åœ–"""
    intent: QueryIntent
    time_range: Optional[Dict] = None      # {"start": date, "end": date}
    filters: Dict = field(default_factory=dict)  # {"branch": "å°å—ç‡Ÿæ¥­æ‰€", "worker": "å¼µä¸‰"}
    metrics: List[str] = field(default_factory=list)  # ["æ‹œè¨ªæ¬¡æ•¸", "å®¢æˆ¶æ•¸"]
    group_by: List[str] = field(default_factory=list)  # ["Worker", "Customer"]
    sort_by: Optional[str] = None
    limit: Optional[int] = None
    raw_query: str = ""


@dataclass
class AnalysisResult:
    """åˆ†æçµæœ"""
    answer: str                           # è‡ªç„¶èªè¨€å›ç­”
    data_summary: Dict                    # æ•¸æ“šæ‘˜è¦
    insights: List[str]                   # BI æ´å¯Ÿ
    recommendations: List[str]            # å»ºè­°è¡Œå‹•
    visualizations: List[Dict] = field(default_factory=list)  # åœ–è¡¨å»ºè­°
    raw_data: Optional[Any] = None        # åŸå§‹æ•¸æ“šï¼ˆå¯é¸ï¼‰
    code_executed: str = ""               # åŸ·è¡Œçš„ä»£ç¢¼
    metadata: Dict = field(default_factory=dict)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Schema å®šç¾©ï¼ˆè®“ AI ç†è§£æ•¸æ“šçµæ§‹ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

BUSINESS_DATA_SCHEMA = """
æ¥­å‹™æ—¥å ±æ•¸æ“šçµæ§‹ï¼ˆä¾†æºï¼šPostgreSQL business.reportsï¼‰ï¼š

æ¬„ä½èªªæ˜ï¼š
- Date: æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)
- Worker: æ¥­å‹™å“¡å§“å
- Customer: å®¢æˆ¶åç¨±
- Class: æ´»å‹•é¡å‹ (å¦‚: æ¥­å‹™æ‹œè¨ª, é€è²¨, å ±åƒ¹, é›»è©±è¯ç¹«, æœƒè­°, ç¶­ä¿®æœå‹™)
- Content: æ´»å‹•å…§å®¹æè¿°
- Depart: ç‡Ÿæ¥­æ‰€/éƒ¨é–€ (å¦‚: å°å—ç‡Ÿæ¥­æ‰€, å°ä¸­ç‡Ÿæ¥­æ‰€, é«˜é›„ç‡Ÿæ¥­æ‰€, å°åŒ—ç‡Ÿæ¥­æ‰€)
- Level: ç­‰ç´š (A/B/C)
- Doc_Status: æ–‡ä»¶ç‹€æ…‹

åŸ·è¡Œç’°å¢ƒå·²æä¾›çš„è®Šæ•¸ï¼ˆä¸éœ€è¦ importï¼‰ï¼š
- df: æ¥­å‹™æ•¸æ“š DataFrame
- pd: pandas æ¨¡çµ„
- datetime: datetime é¡åˆ¥  
- timedelta: timedelta é¡åˆ¥
- re: æ­£å‰‡è¡¨é”å¼æ¨¡çµ„

å¸¸è¦‹æŸ¥è©¢æ¨¡å¼ï¼š
1. æ™‚é–“ç¯„åœéæ¿¾: df[df['_Date'] >= start_date]
2. ç‡Ÿæ¥­æ‰€éæ¿¾: df[df['Depart'].str.contains('å°å—')]
3. æ¥­å‹™å“¡éæ¿¾: df[df['Worker'] == 'å¼µä¸‰']
4. å®¢æˆ¶éæ¿¾: df[df['Customer'].str.contains('æ±å°')]
5. æ´»å‹•é¡å‹éæ¿¾: df[df['Class'].str.contains('æ¥­å‹™æ‹œè¨ª')]

é‡è¦æé†’ï¼š
- æ—¥æœŸæ¬„ä½ '_Date' æ˜¯ datetime é¡å‹ï¼Œå·²ç¶“åœ¨æ•¸æ“šé è™•ç†æ™‚å»ºç«‹
- ä½¿ç”¨ .str.contains() é€²è¡Œæ¨¡ç³ŠåŒ¹é…
- ä½¿ç”¨ pd.Timestamp è™•ç†æ—¥æœŸæ¯”è¼ƒ
- ä¸è¦ä½¿ç”¨ import èªå¥ï¼
"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Prompt æ¨¡æ¿
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

INTENT_PARSING_PROMPT = """ä½ æ˜¯ä¸€å€‹æ¥­å‹™æ•¸æ“šåˆ†æåŠ©æ‰‹ã€‚è«‹åˆ†æç”¨æˆ¶çš„æŸ¥è©¢æ„åœ–ï¼Œä¸¦æå–é—œéµä¿¡æ¯ã€‚

ç”¨æˆ¶æŸ¥è©¢ï¼š{query}

ç•¶å‰æ—¥æœŸï¼š{today}

è«‹ä»¥ JSON æ ¼å¼å›ç­”ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
{{
    "intent": "aggregate|list|trend|compare|anomaly|ranking|search",
    "time_range": {{
        "type": "relative|absolute|none",
        "value": "æœ€è¿‘30å¤©|2024å¹´1æœˆ|2024/01/01-2024/01/31",
        "start": "YYYY-MM-DD æˆ– null",
        "end": "YYYY-MM-DD æˆ– null"
    }},
    "filters": {{
        "branch": "ç‡Ÿæ¥­æ‰€åç¨±æˆ–null",
        "worker": "æ¥­å‹™å“¡åç¨±æˆ–null",
        "customer": "å®¢æˆ¶åç¨±æˆ–null",
        "activity_type": "æ´»å‹•é¡å‹æˆ–null"
    }},
    "metrics": ["è¦è¨ˆç®—çš„æŒ‡æ¨™ï¼Œå¦‚æ‹œè¨ªæ¬¡æ•¸ã€å®¢æˆ¶æ•¸ç­‰"],
    "group_by": ["åˆ†çµ„æ¬„ä½ï¼Œå¦‚Workerã€Customerã€Departç­‰"],
    "analysis_focus": "ç”¨æˆ¶é—œæ³¨çš„åˆ†æé‡é»æè¿°"
}}

åªå›ç­” JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"""


CODE_GENERATION_PROMPT = """ä½ æ˜¯ä¸€å€‹ Python/Pandas å°ˆå®¶ã€‚è«‹æ ¹æ“šç”¨æˆ¶æ„åœ–ç”ŸæˆæŸ¥è©¢ä»£ç¢¼ã€‚

{schema}

ç”¨æˆ¶æŸ¥è©¢ï¼š{query}
è§£æå¾Œçš„æ„åœ–ï¼š{intent_json}
ç•¶å‰æ—¥æœŸï¼š{today}

è«‹ç”Ÿæˆ Python ä»£ç¢¼ï¼Œä½¿ç”¨è®Šæ•¸ `df` ä½œç‚ºè¼¸å…¥ DataFrameã€‚

ã€é‡è¦é™åˆ¶ã€‘
1. ä¸è¦ä½¿ç”¨ import èªå¥ï¼ä»¥ä¸‹è®Šæ•¸å·²å¯ç”¨ï¼šdf, pd, datetime, timedelta, re
2. å¿…é ˆæŒ‰ç…§ä»¥ä¸‹é †åºç·¨å¯«ä»£ç¢¼ï¼š
   - ç¬¬ä¸€æ­¥ï¼šå»ºç«‹éæ¿¾æ¢ä»¶ mask
   - ç¬¬äºŒæ­¥ï¼šéæ¿¾æ•¸æ“šå¾—åˆ° filtered = df[mask]
   - ç¬¬ä¸‰æ­¥ï¼šåŸºæ–¼ filtered é€²è¡Œçµ±è¨ˆåˆ†æï¼Œå­˜å…¥ result
   - ç¬¬å››æ­¥ï¼šå»ºç«‹ summary å­—å…¸
3. æ‰€æœ‰å° filtered çš„å¼•ç”¨å¿…é ˆåœ¨ filtered = df[mask] ä¹‹å¾Œ

ã€ä»£ç¢¼æ¨¡æ¿ - å¿…é ˆéµå¾ªæ­¤çµæ§‹ã€‘
```python
# ç¬¬ä¸€æ­¥ï¼šå»ºç«‹éæ¿¾æ¢ä»¶
mask = pd.Series([True] * len(df))

# æ™‚é–“éæ¿¾ï¼ˆæ ¹æ“šéœ€è¦èª¿æ•´æ—¥æœŸï¼‰
mask = mask & (df['_Date'] >= pd.Timestamp('2024-10-01'))
mask = mask & (df['_Date'] <= pd.Timestamp('2024-10-31'))

# å…¶ä»–éæ¿¾ï¼ˆå®¢æˆ¶ã€æ¥­å‹™å“¡ç­‰ï¼‰
mask = mask & (df['Customer'].str.contains('å°å¡‘', na=False))

# ç¬¬äºŒæ­¥ï¼šåŸ·è¡Œéæ¿¾
filtered = df[mask]

# ç¬¬ä¸‰æ­¥ï¼šçµ±è¨ˆåˆ†æï¼ˆå¿…é ˆåœ¨ filtered å®šç¾©ä¹‹å¾Œï¼‰
result = filtered[['Date', 'Worker', 'Customer', 'Class', 'Content']].copy()

# æˆ–è€…åšåˆ†çµ„çµ±è¨ˆ
# result = filtered.groupby('Worker').agg({{
#     'Customer': 'nunique',
#     'Date': 'count'
# }})

# ç¬¬å››æ­¥ï¼šå»ºç«‹æ‘˜è¦
summary = {{
    'total_records': len(filtered),
    'unique_workers': filtered['Worker'].nunique() if len(filtered) > 0 else 0,
    'unique_customers': filtered['Customer'].nunique() if len(filtered) > 0 else 0
}}
```

åªè¼¸å‡ºå¯åŸ·è¡Œçš„ Python ä»£ç¢¼ï¼Œä¸è¦æœ‰ markdown æ¨™è¨˜ï¼Œä¸è¦æœ‰ import èªå¥ã€‚
ç¢ºä¿ filtered è®Šæ•¸åœ¨è¢«å¼•ç”¨å‰å·²ç¶“å®šç¾©ï¼"""


ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ¥­å‹™åˆ†æé¡§å•ã€‚è«‹æ ¹æ“šæŸ¥è©¢çµæœæä¾›æ·±å…¥çš„ BI åˆ†æã€‚

åŸå§‹æŸ¥è©¢ï¼š{query}
æ•¸æ“šæ‘˜è¦ï¼š{summary}
è©³ç´°çµæœï¼š{result_preview}

è«‹æä¾›ï¼š
1. **ç›´æ¥å›ç­”**ï¼šç”¨è‡ªç„¶èªè¨€å›ç­”ç”¨æˆ¶çš„å•é¡Œï¼ˆ2-3 å¥è©±ï¼‰
2. **é—œéµæ´å¯Ÿ**ï¼šå¾æ•¸æ“šä¸­ç™¼ç¾çš„ 3-5 å€‹é‡è¦ç™¼ç¾
3. **è¶¨å‹¢åˆ†æ**ï¼šå¦‚æœæ•¸æ“šæ¶‰åŠæ™‚é–“ï¼Œåˆ†æè®ŠåŒ–è¶¨å‹¢
4. **ç•°å¸¸åµæ¸¬**ï¼šæ¨™å‡ºä»»ä½•ç•°å¸¸æˆ–å€¼å¾—æ³¨æ„çš„æ•¸æ“šé»
5. **å»ºè­°è¡Œå‹•**ï¼šåŸºæ–¼åˆ†æçµæœï¼Œæå‡º 2-3 å€‹å…·é«”çš„è¡Œå‹•å»ºè­°

è«‹ä»¥ JSON æ ¼å¼å›ç­”ï¼š
{{
    "direct_answer": "ç›´æ¥å›ç­”ç”¨æˆ¶å•é¡Œ...",
    "insights": ["æ´å¯Ÿ1", "æ´å¯Ÿ2", "æ´å¯Ÿ3"],
    "trends": ["è¶¨å‹¢æè¿°1", "è¶¨å‹¢æè¿°2"],
    "anomalies": ["ç•°å¸¸1", "ç•°å¸¸2"],
    "recommendations": ["å»ºè­°1", "å»ºè­°2", "å»ºè­°3"],
    "visualization_suggestions": [
        {{"type": "bar", "title": "åœ–è¡¨æ¨™é¡Œ", "x": "æ¬„ä½", "y": "æ¬„ä½"}},
        {{"type": "line", "title": "è¶¨å‹¢åœ–", "x": "Date", "y": "count"}}
    ]
}}

åªå›ç­” JSONï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚"""


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LLM å®¢æˆ¶ç«¯
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class LLMClient:
    """LLM å®¢æˆ¶ç«¯ï¼ˆæ”¯æ´ OpenAI å’Œ Anthropicï¼‰"""
    
    def __init__(self):
        self.provider = os.getenv("LLM_PROVIDER", "openai").strip().lower()
        self.openai_key = os.getenv("OPENAI_API_KEY", "")
        self.anthropic_key = os.getenv("ANTHROPIC_API_KEY", "")
        
        # è™•ç† auto æ¨¡å¼ï¼šæ ¹æ“š LLM_PRIMARY æ±ºå®šå„ªå…ˆä½¿ç”¨å“ªå€‹
        if self.provider == "auto":
            primary = os.getenv("LLM_PRIMARY", "anthropic").strip().lower()
            if primary in ("anthropic", "claude"):
                self.provider = "anthropic"
            else:
                self.provider = "openai"
            logger.info(f"ğŸ”„ Auto æ¨¡å¼ï¼Œé¸æ“‡ primary: {self.provider}")
        
        # æ¥­å‹™åˆ†æå°ˆç”¨æ¨¡å‹
        self.model = os.getenv("LLM_MODEL_BUSINESS", "gpt-4o")
        
        self._client = None
        self._init_client()
    
    def _init_client(self):
        """åˆå§‹åŒ–å®¢æˆ¶ç«¯"""
        if self.provider in ("anthropic", "claude") and self.anthropic_key:
            try:
                from anthropic import Anthropic
                self._client = Anthropic(api_key=self.anthropic_key)
                self.provider = "anthropic"
                self.model = os.getenv("ANTHROPIC_MODEL_BUSINESS", "claude-sonnet-4-20250514")
                logger.info(f"âœ… æ¥­å‹™ AI å¼•æ“ä½¿ç”¨ Anthropic: {self.model}")
            except ImportError:
                logger.warning("anthropic å¥—ä»¶æœªå®‰è£ï¼Œé™ç´šåˆ° OpenAI")
                self._init_openai()
        else:
            self._init_openai()
    
    def _init_openai(self):
        """åˆå§‹åŒ– OpenAI"""
        if self.openai_key:
            try:
                from openai import OpenAI
                self._client = OpenAI(api_key=self.openai_key)
                self.provider = "openai"
                self.model = os.getenv("OPENAI_MODEL_BUSINESS", "gpt-4o")
                logger.info(f"âœ… æ¥­å‹™ AI å¼•æ“ä½¿ç”¨ OpenAI: {self.model}")
            except ImportError:
                raise RuntimeError("éœ€è¦å®‰è£ openai æˆ– anthropic å¥—ä»¶")
    
    def chat(self, prompt: str, system: str = None, temperature: float = 0.1) -> str:
        """ç™¼é€èŠå¤©è«‹æ±‚"""
        try:
            if self.provider == "anthropic":
                return self._chat_anthropic(prompt, system, temperature)
            else:
                return self._chat_openai(prompt, system, temperature)
        except Exception as e:
            error_msg = str(e)
            if "usage limits" in error_msg or "quota" in error_msg.lower():
                logger.warning(f"âš ï¸ {self.provider} é™é¡ï¼Œå˜—è©¦ fallback: {e}")
                return self._fallback_chat(prompt, system, temperature)
            else:
                raise
    
    def _chat_openai(self, prompt: str, system: str = None, temperature: float = 0.1) -> str:
        """OpenAI èŠå¤©"""
        messages = []
        if system:
            messages.append({"role": "system", "content": system})
        messages.append({"role": "user", "content": prompt})
        
        response = self._client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=temperature,
            max_tokens=4000,
        )
        return response.choices[0].message.content
    
    def _chat_anthropic(self, prompt: str, system: str = None, temperature: float = 0.1) -> str:
        """Anthropic èŠå¤©"""
        kwargs = {
            "model": self.model,
            "max_tokens": 4000,
            "messages": [{"role": "user", "content": prompt}],
        }
        if system:
            kwargs["system"] = system
        
        response = self._client.messages.create(**kwargs)
        return response.content[0].text
    
    def _fallback_chat(self, prompt: str, system: str = None, temperature: float = 0.1) -> str:
        """Fallback åˆ°å¦ä¸€å€‹æä¾›å•†"""
        if self.provider == "anthropic" and self.openai_key:
            from openai import OpenAI
            fallback_client = OpenAI(api_key=self.openai_key)
            fallback_model = os.getenv("OPENAI_MODEL_BUSINESS", "gpt-4o")
            
            messages = []
            if system:
                messages.append({"role": "system", "content": system})
            messages.append({"role": "user", "content": prompt})
            
            response = fallback_client.chat.completions.create(
                model=fallback_model,
                messages=messages,
                temperature=temperature,
                max_tokens=4000,
            )
            return response.choices[0].message.content
        raise RuntimeError("ç„¡å¯ç”¨çš„ LLM æä¾›å•†")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¥­å‹™ AI å¼•æ“
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class BusinessAIEngine:
    """ç´” AI é©…å‹•çš„æ¥­å‹™æ™ºèƒ½æŸ¥è©¢å¼•æ“"""
    
    def __init__(self, csv_path: str = None, use_database: bool = True):
        """
        åˆå§‹åŒ–å¼•æ“
        
        Args:
            csv_path: æ¥­å‹™ CSV æª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼Œæœƒè‡ªå‹•åµæ¸¬ï¼‰
            use_database: æ˜¯å¦å„ªå…ˆå¾è³‡æ–™åº«è®€å–ï¼ˆé è¨­ Trueï¼‰
        """
        if not _HAS_PANDAS:
            raise RuntimeError("éœ€è¦å®‰è£ pandas: pip install pandas")
        
        self.use_database = use_database and _HAS_PSYCOPG2
        self.csv_path = csv_path or self._detect_csv_path()
        self.llm = LLMClient()
        self.df = None
        self.data_source = None  # 'database' æˆ– 'csv'
        self._load_data()
    
    def _detect_csv_path(self) -> Optional[str]:
        """è‡ªå‹•åµæ¸¬ CSV è·¯å¾‘"""
        candidates = [
            os.environ.get("BUSINESS_CSV_FILE"),
            "/app/data/business/clean_business.csv",
            "./data/business/clean_business.csv",
            "./business/clean_business.csv",
            "clean_business.csv",
        ]
        for p in candidates:
            if p and os.path.exists(p):
                return p
        return None
    
    def _load_data(self):
        """è¼‰å…¥ä¸¦é è™•ç†æ•¸æ“šï¼ˆå„ªå…ˆå¾è³‡æ–™åº«ï¼‰"""
        # å˜—è©¦å¾è³‡æ–™åº«è¼‰å…¥
        if self.use_database:
            if self._load_data_from_database():
                return
            logger.warning("è³‡æ–™åº«è¼‰å…¥å¤±æ•—ï¼Œå˜—è©¦ CSV fallback")
        
        # Fallback åˆ° CSV
        self._load_data_from_csv()
    
    def _load_data_from_database(self) -> bool:
        """å¾ PostgreSQL è³‡æ–™åº«è¼‰å…¥æ¥­å‹™æ—¥å ±"""
        conn = get_db_connection()
        if not conn:
            return False
        
        try:
            cursor = conn.cursor()
            
            # æŸ¥è©¢æ¥­å‹™æ—¥å ±ï¼ˆæœ€è¿‘ 12 å€‹æœˆï¼‰
            cursor.execute('''
                SELECT 
                    r.report_date as "Date",
                    u.name as "Worker",
                    r.customer_name as "Customer",
                    r.activity_class as "Class",
                    r.content as "Content",
                    u.department as "Depart",
                    r.level as "Level",
                    r.status as "Doc_Status",
                    r.created_at as "TimeCreated"
                FROM business.reports r
                JOIN public.users u ON r.user_id = u.id
                WHERE r.report_date >= CURRENT_DATE - INTERVAL '12 months'
                ORDER BY r.report_date DESC
            ''')
            
            rows = cursor.fetchall()
            
            if not rows:
                logger.warning("è³‡æ–™åº«ä¸­æ²’æœ‰æ¥­å‹™æ—¥å ±è³‡æ–™")
                return False
            
            # è½‰æ›ç‚º DataFrame
            self.df = pd.DataFrame([dict(row) for row in rows])
            
            # é è™•ç†æ—¥æœŸ
            self.df['Date'] = pd.to_datetime(self.df['Date']).dt.strftime('%Y/%m/%d')
            self.df['_Date'] = pd.to_datetime(self.df['Date'], errors='coerce')
            
            # æ¸…ç†ç©ºå€¼
            for col in ['Worker', 'Customer', 'Class', 'Depart', 'Content']:
                if col in self.df.columns:
                    self.df[col] = self.df[col].fillna('').astype(str)
            
            self.data_source = 'database'
            logger.info(f"âœ… å¾è³‡æ–™åº«è¼‰å…¥æ¥­å‹™æ•¸æ“š: {len(self.df)} ç­†è¨˜éŒ„")
            return True
            
        except Exception as e:
            logger.error(f"å¾è³‡æ–™åº«è¼‰å…¥å¤±æ•—: {e}")
            return False
        finally:
            conn.close()
    
    def _load_data_from_csv(self):
        """å¾ CSV è¼‰å…¥æ•¸æ“šï¼ˆfallbackï¼‰"""
        if not self.csv_path or not os.path.exists(self.csv_path):
            logger.warning(f"æ¥­å‹™ CSV ä¸å­˜åœ¨: {self.csv_path}")
            return
        
        try:
            self.df = pd.read_csv(self.csv_path, encoding='utf-8')
            self.df = self.df.dropna(how='all')
            
            # é è™•ç†æ—¥æœŸ
            self.df['_Date'] = pd.to_datetime(self.df['Date'], errors='coerce')
            
            # æ¸…ç†ç©ºå€¼
            for col in ['Worker', 'Customer', 'Class', 'Depart', 'Content']:
                if col in self.df.columns:
                    self.df[col] = self.df[col].fillna('').astype(str)
            
            self.data_source = 'csv'
            logger.info(f"âœ… å¾ CSV è¼‰å…¥æ¥­å‹™æ•¸æ“š: {len(self.df)} ç­†è¨˜éŒ„")
        except Exception as e:
            logger.error(f"è¼‰å…¥æ¥­å‹™æ•¸æ“šå¤±æ•—: {e}")
            self.df = None
    
    def reload_data(self):
        """é‡æ–°è¼‰å…¥æ•¸æ“š"""
        self._load_data()
    
    def _parse_intent(self, query: str) -> Dict:
        """ä½¿ç”¨ AI è§£ææŸ¥è©¢æ„åœ–"""
        prompt = INTENT_PARSING_PROMPT.format(
            query=query,
            today=datetime.now().strftime("%Y-%m-%d")
        )
        
        try:
            response = self.llm.chat(prompt, temperature=0.0)
            response = response.strip()
            if response.startswith("```"):
                response = re.sub(r'^```\w*\n?', '', response)
                response = re.sub(r'\n?```$', '', response)
            
            return json.loads(response)
        except json.JSONDecodeError as e:
            logger.warning(f"æ„åœ–è§£æ JSON éŒ¯èª¤: {e}")
            return {"intent": "search", "filters": {}, "metrics": []}
        except Exception as e:
            logger.error(f"æ„åœ–è§£æå¤±æ•—: {e}")
            return {"intent": "search", "filters": {}, "metrics": []}
    
    def _generate_code(self, query: str, intent: Dict) -> str:
        """ä½¿ç”¨ AI ç”ŸæˆæŸ¥è©¢ä»£ç¢¼"""
        prompt = CODE_GENERATION_PROMPT.format(
            schema=BUSINESS_DATA_SCHEMA,
            query=query,
            intent_json=json.dumps(intent, ensure_ascii=False, indent=2),
            today=datetime.now().strftime("%Y-%m-%d")
        )
        
        try:
            response = self.llm.chat(prompt, temperature=0.0)
            code = response.strip()
            
            # æ¸…ç† markdown
            if code.startswith("```"):
                code = re.sub(r'^```\w*\n?', '', code)
                code = re.sub(r'\n?```$', '', code)
            
            # ç§»é™¤ import
            code = re.sub(r'^import\s+.*$', '', code, flags=re.MULTILINE)
            code = re.sub(r'^from\s+.*import\s+.*$', '', code, flags=re.MULTILINE)
            
            return code.strip()
        except Exception as e:
            logger.error(f"ä»£ç¢¼ç”Ÿæˆå¤±æ•—: {e}")
            return ""
    
    def _execute_code(self, code: str) -> Tuple[Any, Dict, Optional[str]]:
        """åŸ·è¡Œç”Ÿæˆçš„ä»£ç¢¼"""
        if self.df is None or self.df.empty:
            return None, {}, "æ•¸æ“šæœªè¼‰å…¥"
        
        local_vars = {
            'df': self.df.copy(),
            'pd': pd,
            'datetime': datetime,
            'timedelta': timedelta,
            're': re,
        }
        
        # Provide safe built-in functions for code execution
        safe_builtins = {
            'len': len,
            'int': int,
            'str': str,
            'float': float,
            'bool': bool,
            'list': list,
            'dict': dict,
            'tuple': tuple,
            'set': set,
            'sum': sum,
            'min': min,
            'max': max,
            'abs': abs,
            'round': round,
            'sorted': sorted,
            'enumerate': enumerate,
            'zip': zip,
            'range': range,
            'True': True,
            'False': False,
            'None': None,
        }
        
        try:
            exec(code, {"__builtins__": safe_builtins}, local_vars)
            
            result = local_vars.get('result')
            summary = local_vars.get('summary', {})
            
            if result is None:
                result = local_vars.get('filtered')
            
            return result, summary, None
            
        except Exception as e:
            logger.error(f"ä»£ç¢¼åŸ·è¡ŒéŒ¯èª¤: {e}\nä»£ç¢¼:\n{code}")
            return None, {}, str(e)
    
    def _fallback_query(self, query: str, intent: Dict) -> Tuple[Any, Dict, Optional[str]]:
        """ç°¡å–®çš„ fallback æŸ¥è©¢"""
        if self.df is None or self.df.empty:
            return None, {}, "æ•¸æ“šæœªè¼‰å…¥"
        
        try:
            df = self.df.copy()
            mask = pd.Series([True] * len(df))
            
            filters = intent.get('filters', {})
            
            if filters.get('branch'):
                mask = mask & df['Depart'].str.contains(filters['branch'], na=False)
            
            if filters.get('worker'):
                mask = mask & df['Worker'].str.contains(filters['worker'], na=False)
            
            if filters.get('customer'):
                mask = mask & df['Customer'].str.contains(filters['customer'], na=False)
            
            time_range = intent.get('time_range', {})
            if time_range.get('start'):
                start = pd.Timestamp(time_range['start'])
                mask = mask & (df['_Date'] >= start)
            if time_range.get('end'):
                end = pd.Timestamp(time_range['end'])
                mask = mask & (df['_Date'] <= end)
            
            filtered = df[mask]
            
            summary = {
                'total_records': len(filtered),
                'unique_workers': filtered['Worker'].nunique() if len(filtered) > 0 else 0,
                'unique_customers': filtered['Customer'].nunique() if len(filtered) > 0 else 0,
            }
            
            result = filtered[['Date', 'Worker', 'Customer', 'Class', 'Content']].head(100)
            
            return result, summary, None
            
        except Exception as e:
            return None, {}, str(e)
    
    def _analyze_result(self, query: str, result: Any, summary: Dict) -> Dict:
        """ä½¿ç”¨ AI åˆ†æçµæœ"""
        if isinstance(result, pd.DataFrame):
            result_preview = result.head(30).to_string() if len(result) > 0 else "ï¼ˆç„¡æ•¸æ“šï¼‰"
        else:
            result_preview = str(result)[:2000]
        
        prompt = ANALYSIS_PROMPT.format(
            query=query,
            summary=json.dumps(summary, ensure_ascii=False, default=str),
            result_preview=result_preview
        )
        
        try:
            response = self.llm.chat(prompt, temperature=0.3)
            response = response.strip()
            if response.startswith("```"):
                response = re.sub(r'^```\w*\n?', '', response)
                response = re.sub(r'\n?```$', '', response)
            
            return json.loads(response)
        except:
            return {
                "direct_answer": "æŸ¥è©¢å®Œæˆï¼Œè«‹æŸ¥çœ‹ä¸‹æ–¹æ•¸æ“šã€‚",
                "insights": [],
                "trends": [],
                "anomalies": [],
                "recommendations": [],
            }
    
    def _format_output(self, query: str, result: Any, summary: Dict, analysis: Dict, code: str) -> AnalysisResult:
        """æ ¼å¼åŒ–æœ€çµ‚è¼¸å‡º"""
        answer_parts = []
        
        # ç›´æ¥å›ç­”
        if analysis.get("direct_answer"):
            answer_parts.append(f"ğŸ“Š {analysis['direct_answer']}")
        
        # æ•¸æ“šæ‘˜è¦
        answer_parts.append(f"\n\n**æ•¸æ“šæ‘˜è¦**")
        answer_parts.append(f"- ç¸½è¨˜éŒ„æ•¸ï¼š{summary.get('total_records', 0)}")
        answer_parts.append(f"- æ¥­å‹™å“¡æ•¸ï¼š{summary.get('unique_workers', 0)}")
        answer_parts.append(f"- å®¢æˆ¶æ•¸ï¼š{summary.get('unique_customers', 0)}")
        answer_parts.append(f"- è³‡æ–™ä¾†æºï¼š{self.data_source or 'unknown'}")
        
        # æ´å¯Ÿ
        if analysis.get("insights"):
            answer_parts.append("\n\nğŸ’¡ **é—œéµæ´å¯Ÿ**")
            for insight in analysis["insights"]:
                answer_parts.append(f"- {insight}")
        
        # è¶¨å‹¢
        if analysis.get("trends"):
            answer_parts.append("\n\nğŸ“ˆ **è¶¨å‹¢åˆ†æ**")
            for trend in analysis["trends"]:
                answer_parts.append(f"- {trend}")
        
        # å»ºè­°
        if analysis.get("recommendations"):
            answer_parts.append("\n\nâœ… **å»ºè­°è¡Œå‹•**")
            for rec in analysis["recommendations"]:
                answer_parts.append(f"- {rec}")
        
        # æ•¸æ“šè¡¨æ ¼
        if isinstance(result, pd.DataFrame) and len(result) > 0 and len(result) <= 50:
            answer_parts.append("\n\nğŸ“‹ **è©³ç´°æ•¸æ“š**")
            answer_parts.append(self._df_to_markdown(result.head(30)))
        
        answer_parts.append("\n\nğŸ“‹ åƒè€ƒè³‡æ–™ä¾†æºï¼šbusiness")
        
        return AnalysisResult(
            answer="\n".join(answer_parts),
            data_summary=summary,
            insights=analysis.get("insights", []),
            recommendations=analysis.get("recommendations", []),
            visualizations=analysis.get("visualization_suggestions", []),
            raw_data=result if isinstance(result, (dict, list)) else None,
            code_executed=code,
            metadata={
                "query": query,
                "llm_model": self.llm.model,
                "data_source": self.data_source,
                "timestamp": datetime.now().isoformat(),
            }
        )
    
    def _df_to_markdown(self, df: pd.DataFrame, max_rows: int = 30) -> str:
        """å°‡ DataFrame è½‰ç‚º Markdown è¡¨æ ¼"""
        if df.empty:
            return ""
        
        df_show = df.head(max_rows)
        cols = list(df_show.columns)
        
        header = "| " + " | ".join(str(c) for c in cols) + " |"
        sep = "|" + "|".join(["---"] * len(cols)) + "|"
        
        rows = []
        for _, r in df_show.iterrows():
            row_vals = []
            for c in cols:
                val = str(r.get(c, ""))[:60]
                val = val.replace("|", "ï½œ").replace("\n", " ")
                row_vals.append(val)
            rows.append("| " + " | ".join(row_vals) + " |")
        
        return header + "\n" + sep + "\n" + "\n".join(rows)
    
    def query(self, query: str) -> Dict:
        """
        ä¸»æŸ¥è©¢å…¥å£
        
        Args:
            query: è‡ªç„¶èªè¨€æŸ¥è©¢
        
        Returns:
            {
                "answer": "è‡ªç„¶èªè¨€å›ç­”",
                "success": True/False,
                "data_summary": {...},
                "data_source": "database" | "csv",
                ...
            }
        """
        if not query or not query.strip():
            return {"answer": "è«‹è¼¸å…¥æœ‰æ•ˆçš„æŸ¥è©¢ã€‚", "success": False}
        
        if self.df is None or self.df.empty:
            return {"answer": "æ¥­å‹™æ•¸æ“šæœªè¼‰å…¥ã€‚è«‹ç¢ºèªè³‡æ–™åº«é€£æ¥æˆ– CSV æª”æ¡ˆã€‚", "success": False}
        
        try:
            # Step 1: AI è§£ææ„åœ–
            logger.info(f"ğŸ” è§£ææŸ¥è©¢æ„åœ–: {query[:50]}...")
            intent = self._parse_intent(query)
            
            # Step 2: AI ç”Ÿæˆä»£ç¢¼
            logger.info("ğŸ”§ ç”ŸæˆæŸ¥è©¢ä»£ç¢¼...")
            code = self._generate_code(query, intent)
            
            if not code:
                return {"answer": "ç„¡æ³•ç”ŸæˆæŸ¥è©¢ä»£ç¢¼ã€‚", "success": False}
            
            # Step 3: åŸ·è¡Œä»£ç¢¼
            logger.info("âš¡ åŸ·è¡ŒæŸ¥è©¢...")
            result, summary, error = self._execute_code(code)
            
            if error:
                logger.warning(f"ä»£ç¢¼åŸ·è¡Œå¤±æ•—ï¼Œå˜—è©¦ fallback: {error}")
                result, summary, fallback_error = self._fallback_query(query, intent)
                
                if fallback_error or result is None:
                    return {"answer": f"æŸ¥è©¢åŸ·è¡ŒéŒ¯èª¤ï¼š{error[:500]}", "success": False}
            
            if result is None or (isinstance(result, pd.DataFrame) and len(result) == 0):
                return {
                    "answer": "æŸ¥è©¢å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æ•¸æ“šã€‚",
                    "success": True,
                    "data_summary": summary,
                    "data_source": self.data_source,
                }
            
            # Step 4: AI åˆ†æçµæœ
            logger.info("ğŸ“Š åˆ†æçµæœ...")
            analysis = self._analyze_result(query, result, summary)
            
            # Step 5: æ ¼å¼åŒ–è¼¸å‡º
            output = self._format_output(query, result, summary, analysis, code)
            
            return {
                "answer": output.answer,
                "success": True,
                "data_summary": output.data_summary,
                "data_source": self.data_source,
                "insights": output.insights,
                "recommendations": output.recommendations,
                "visualizations": output.visualizations,
                "metadata": output.metadata,
            }
            
        except Exception as e:
            logger.error(f"æŸ¥è©¢å¤±æ•—: {e}\n{traceback.format_exc()}")
            return {"answer": f"æŸ¥è©¢éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}", "success": False}
    
    def get_schema_info(self) -> Dict:
        """ç²å–æ•¸æ“š schema ä¿¡æ¯"""
        if self.df is None:
            return {"loaded": False}
        
        return {
            "loaded": True,
            "data_source": self.data_source,
            "total_records": len(self.df),
            "columns": list(self.df.columns),
            "date_range": {
                "min": self.df['_Date'].min().isoformat() if not self.df['_Date'].isna().all() else None,
                "max": self.df['_Date'].max().isoformat() if not self.df['_Date'].isna().all() else None,
            },
            "unique_values": {
                "workers": self.df['Worker'].nunique() if 'Worker' in self.df.columns else 0,
                "customers": self.df['Customer'].nunique() if 'Customer' in self.df.columns else 0,
                "branches": self.df['Depart'].unique().tolist() if 'Depart' in self.df.columns else [],
            },
        }
    
    def get_quick_stats(self) -> Dict:
        """ç²å–å¿«é€Ÿçµ±è¨ˆ"""
        if self.df is None:
            return {}
        
        today = datetime.now().date()
        last_30_days = today - timedelta(days=30)
        
        recent = self.df[self.df['_Date'].dt.date >= last_30_days] if '_Date' in self.df.columns else self.df
        
        return {
            "data_source": self.data_source,
            "total_records": len(self.df),
            "recent_30_days": len(recent),
            "active_workers": recent['Worker'].nunique() if 'Worker' in recent.columns else 0,
            "active_customers": recent['Customer'].nunique() if 'Customer' in recent.columns else 0,
            "top_activities": recent['Class'].value_counts().head(5).to_dict() if 'Class' in recent.columns else {},
            "by_branch": recent.groupby('Depart').size().to_dict() if 'Depart' in recent.columns else {},
        }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾¿æ·å‡½æ•¸ï¼ˆå‘å¾Œå…¼å®¹ï¼‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_engine: Optional[BusinessAIEngine] = None

def get_business_ai_engine() -> BusinessAIEngine:
    """ç²å–æ¥­å‹™ AI å¼•æ“å–®ä¾‹"""
    global _engine
    if _engine is None:
        _engine = BusinessAIEngine()
    return _engine

def ai_business_query(query: str) -> str:
    """
    AI æ¥­å‹™æŸ¥è©¢ï¼ˆç°¡åŒ–æ¥å£ï¼Œå‘å¾Œå…¼å®¹ï¼‰
    """
    engine = get_business_ai_engine()
    result = engine.query(query)
    return result.get("answer", "æŸ¥è©¢å¤±æ•—")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI æ¸¬è©¦
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    engine = BusinessAIEngine()
    
    print("\n" + "="*60)
    print("ğŸ¤– æ¥­å‹™ AI æŸ¥è©¢å¼•æ“ - äº’å‹•æ¨¡å¼")
    print("="*60)
    print(f"æ•¸æ“šç‹€æ…‹: {engine.get_schema_info()}")
    print("\nè¼¸å…¥ 'quit' é€€å‡º\n")
    
    while True:
        try:
            query = input("ğŸ“ æ‚¨çš„å•é¡Œ: ").strip()
            if query.lower() in ('quit', 'exit', 'q'):
                break
            if not query:
                continue
            
            print("\nâ³ è™•ç†ä¸­...\n")
            result = engine.query(query)
            print(result["answer"])
            print("\n" + "-"*60 + "\n")
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"éŒ¯èª¤: {e}")
    
    print("\nğŸ‘‹ å†è¦‹ï¼")
