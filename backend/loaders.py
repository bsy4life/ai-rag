# loaders.py - æ–‡ä»¶è¼‰å…¥å™¨æ¨¡çµ„
"""
å„ç¨®æ–‡ä»¶æ ¼å¼çš„è¼‰å…¥å™¨ï¼šMarkdownã€CSVã€æ¥­å‹™å ±å‘Šã€åœ–ç‰‡
"""

import os
import re
from typing import List, Dict, Optional
from pathlib import Path

from langchain_community.document_loaders.base import BaseLoader
from langchain_core.documents import Document

from utils import DocumentType, _nfkc

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¾è³´æª¢æŸ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

try:
    import pandas as pd
    _HAS_PANDAS = True
except ImportError:
    _HAS_PANDAS = False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CSV æ¥­å‹™è³‡æ–™è™•ç†å™¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class CSVBusinessProcessor:
    """å¾ CSV æª”æ¡ˆè¼‰å…¥æ¥­å‹™è³‡æ–™"""
    
    @staticmethod
    def load_business_csv(csv_file: str) -> List[Document]:
        """è¼‰å…¥ CSV æ¥­å‹™è³‡æ–™ç‚º Document åˆ—è¡¨"""
        if not _HAS_PANDAS:
            return []
        
        if not os.path.exists(csv_file):
            return []
        
        try:
            df = pd.read_csv(csv_file, encoding='utf-8')
            
            # ğŸ”§ æ”¹é€²ï¼šéæ¿¾ç©ºè¡Œå’Œç„¡æ•ˆè³‡æ–™
            original_len = len(df)
            df = df.dropna(how='all')  # ç§»é™¤å…¨ç©ºè¡Œ
            df = df[df['Date'].notna()]  # ç¢ºä¿æœ‰æ—¥æœŸ
            df = df[df['Date'].astype(str).str.strip() != '']  # æ—¥æœŸä¸ç‚ºç©ºå­—ä¸²
            
            if original_len != len(df):
                print(f"   ğŸ§¹ éæ¿¾ç„¡æ•ˆè³‡æ–™ï¼š{original_len:,} â†’ {len(df):,} ç­†")
            
            required_columns = ['Date', 'Worker', 'Customer', 'Content']
            missing_columns = [col for col in required_columns if col not in df.columns]
            
            if missing_columns:
                print(f"âŒ CSV ç¼ºå°‘å¿…è¦æ¬„ä½ï¼š{missing_columns}")
                return []
            
            df = df.dropna(subset=required_columns)
            documents = []
            
            for idx, row in df.iterrows():
                content = CSVBusinessProcessor._build_content_from_row(row, idx + 1)
                
                if not content.strip():
                    continue
                
                metadata = {
                    'doc_type': DocumentType.BUSINESS.value,
                    'source': 'business_csv',
                    'record_id': idx + 1,
                    'date': str(row.get('Date', '')),
                    'worker': str(row.get('Worker', '')),
                    'customer': str(row.get('Customer', '')),
                    'class': str(row.get('Class', '')),
                    'depart': str(row.get('Depart', '')),
                    'manager': str(row.get('Manager', '')),
                }
                
                doc = Document(page_content=content, metadata=metadata)
                documents.append(doc)
                
                if (idx + 1) % 10000 == 0:
                    print(f"   ğŸ“Š å·²è™•ç† {idx + 1:,} / {len(df):,} ç­†è¨˜éŒ„...")
            
            return documents
            
        except Exception as e:
            print(f"CSV è¼‰å…¥å¤±æ•—ï¼š{e}")
            return []
    
    @staticmethod
    def _build_content_from_row(row, record_num: int) -> str:
        """å¾ DataFrame è¡Œå»ºæ§‹æ–‡æª”å…§å®¹"""
        parts = [f"**è¨˜éŒ„ç·¨è™Ÿ**: {record_num}"]
        
        field_map = {
            'Date': 'æ—¥æœŸ',
            'Worker': 'æ¥­å‹™äººå“¡',
            'Customer': 'å®¢æˆ¶',
            'Class': 'æ´»å‹•é¡å‹',
            'Content': 'æ´»å‹•å…§å®¹',
            'Depart': 'éƒ¨é–€',
            'Manager': 'ä¸»ç®¡',
        }
        
        for en_name, zh_name in field_map.items():
            val = row.get(en_name, '')
            if pd.notna(val) and str(val).strip():
                parts.append(f"**{zh_name}**: {str(val).strip()}")
        
        return "\n".join(parts)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ¥­å‹™å ±å‘Šè™•ç†å™¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class BusinessReportProcessor:
    """è™•ç†æ¥­å‹™æ—¥å ±æ ¼å¼çš„æ–‡ä»¶"""
    
    FIELD_MAPPING = {
        'Doc_Time:': 'æ—¥æœŸ',
        'TimeCreated:': 'å»ºç«‹æ™‚é–“',
        'Customer:': 'å®¢æˆ¶',
        'Worker:': 'æ¥­å‹™äººå“¡',
        'Content:': 'æ´»å‹•å…§å®¹',
        'Manager:': 'ä¸»ç®¡',
        'Depart:': 'éƒ¨é–€',
        'Doc_St:': 'ç‹€æ…‹',
        'Class:': 'æ´»å‹•é¡å‹',
    }
    
    @staticmethod
    def parse_report(text: str) -> List[Dict]:
        """è§£ææ¥­å‹™æ—¥å ±æ–‡æœ¬"""
        records = []
        
        # å˜—è©¦ç”¨ Doc_Time: åˆ†å‰²
        if 'Doc_Time:' in text:
            parts = re.split(r'(?=Doc_Time:)', text)
            parts = [p.strip() for p in parts if p.strip() and 'Doc_Time:' in p]
            
            for part in parts:
                record = BusinessReportProcessor._parse_single_record(part)
                if record and record.get('customer'):
                    records.append(record)
        
        return records
    
    @staticmethod
    def _parse_single_record(text: str) -> Dict:
        """è§£æå–®ç­†æ¥­å‹™è¨˜éŒ„"""
        record = {}
        
        for en_key, zh_key in BusinessReportProcessor.FIELD_MAPPING.items():
            pattern = rf'{re.escape(en_key)}\s*(.+?)(?=\n[A-Za-z_]+:|$)'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                value = match.group(1).strip()
                value = re.sub(r'\s+', ' ', value)
                
                field_name = en_key.replace(':', '').lower()
                if field_name == 'doc_time':
                    field_name = 'date'
                elif field_name == 'worker':
                    field_name = 'worker'
                elif field_name == 'customer':
                    field_name = 'customer'
                
                record[field_name] = value
        
        return record

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åœ–ç‰‡è™•ç†å™¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SimpleImageProcessor:
    """è™•ç† Markdown ä¸­çš„åœ–ç‰‡å¼•ç”¨"""
    
    def __init__(self, media_base_dir: str):
        self.media_base_dir = media_base_dir
    
    def process_images(self, text: str, source_file: str) -> tuple:
        """è™•ç†åœ–ç‰‡å¼•ç”¨ï¼Œè¿”å› (è™•ç†å¾Œæ–‡æœ¬, åœ–ç‰‡åˆ—è¡¨)"""
        images = []
        
        # åŒ¹é… Markdown åœ–ç‰‡èªæ³•
        pattern = r'!\[([^\]]*)\]\(([^)]+)\)'
        
        def replace_image(match):
            alt_text = match.group(1)
            img_path = match.group(2)
            
            # è™•ç†ç›¸å°è·¯å¾‘
            if not img_path.startswith(('http://', 'https://', '/')):
                source_dir = os.path.dirname(source_file)
                full_path = os.path.normpath(os.path.join(source_dir, img_path))
                
                if os.path.exists(full_path):
                    images.append({
                        'path': full_path,
                        'alt': alt_text,
                        'original': img_path
                    })
            
            return f"[åœ–ç‰‡: {alt_text or 'ç„¡æè¿°'}]"
        
        processed_text = re.sub(pattern, replace_image, text)
        return processed_text, images

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å¢å¼·å‹ Markdown è¼‰å…¥å™¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EnhancedMarkdownLoader(BaseLoader):
    """å¢å¼·å‹ Markdown è¼‰å…¥å™¨ï¼Œæ”¯æ´ç”¢å“è¦æ ¼ã€åœ–ç‰‡ã€è¡¨æ ¼"""
    
    # å®Œæ•´çš„ç”¢å“å‹è™Ÿæ¨¡å¼
    PRODUCT_PATTERNS = [
        # SMC ç”¢å“
        r'(?:^|\s)(MXJ\d+[A-Z]*)',              # MXJ ç³»åˆ—
        r'(?:^|\s)(MXH\d+[A-Z]*)',              # MXH ç³»åˆ—
        r'(?:^|\s)(MXP\d+[A-Z]*)',              # MXP ç³»åˆ—
        r'(?:^|\s)(LES[A-Z]*\d+)',              # LES ç³»åˆ—
        r'(?:^|\s)(LEHZ[A-Z]*\d*)',             # LEHZ ç³»åˆ—
        r'(?:^|\s)(LEHF\d+[A-Z]*)',             # LEHF ç³»åˆ—
        r'(?:^|\s)(ACG[A-Z]*\d*)',              # ACG ç³»åˆ—
        r'(?:^|\s)(ARG[A-Z]*\d*)',              # ARG ç³»åˆ—
        r'(?:^|\s)(AWG[A-Z]*\d*)',              # AWG ç³»åˆ—
        # VALQUA ç”¢å“
        r'(?:ãƒãƒ«ã‚«ãƒ¼\s*)?No\.\s*(\d{4}[A-Z]*)',  # No.6500, No.7010
        r'(?:ãƒãƒ«ã‚«ãƒ¼\s*)?No\.\s*([A-Z]+\d+)',    # No.GF300, No.SF300
        r'(?:ãƒãƒ«ã‚«ãƒ¼\s*)?No\.\s*(N\d{4})',       # No.N7030
        # ç–åŸº/å”é‹¼ç”¢å“
        r'(?:^|\s)(GF\d+[A-Z]*)',               # GF300
        r'(?:^|\s)(GFO[-\s]?\d*)',              # GFO
        r'(?:^|\s)(Gf[il]\d*[A-Z]*)',           # Gfil
        r'(?:^|\s)(Uf[il]\d*[A-Z]*)',           # Ufil
        # é€šç”¨å‹è™Ÿ
        r'(?:å‹è™Ÿ|è£½å“ç•ªå·)[:\s]*([A-Z0-9]+-?[A-Z0-9]+)',
    ]
    
    def __init__(self, file_path: str, **kwargs):
        self.file_path = file_path
        self.autodetect_encoding = kwargs.get('autodetect_encoding', True)
        self.media_base_dir = kwargs.get('media_base_dir', '')
    
    def load(self) -> List[Document]:
        """è¼‰å…¥ä¸¦è™•ç† Markdown æ–‡ä»¶"""
        text = self._read_file()
        if not text:
            return []
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ¥­å‹™å ±å‘Š
        if self._is_business_report(text):
            return self._process_as_business(text)
        
        # è™•ç†ç‚ºæŠ€è¡“æ–‡æª”
        return self._process_as_technical(text)
    
    def _read_file(self) -> str:
        """è®€å–æª”æ¡ˆå…§å®¹"""
        text = None
        encodings_to_try = ['utf-8', 'utf-8-sig', 'big5', 'gb18030', 'shift_jis']
        
        if self.autodetect_encoding:
            try:
                import chardet
                with open(self.file_path, "rb") as f:
                    raw_data = f.read()
                detected = chardet.detect(raw_data)
                if detected and detected['encoding']:
                    encodings_to_try.insert(0, detected['encoding'])
            except Exception:
                pass
        
        for encoding in encodings_to_try:
            try:
                with open(self.file_path, "r", encoding=encoding) as f:
                    text = f.read()
                break
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        if text is None:
            try:
                with open(self.file_path, "r", encoding="utf-8", errors="ignore") as f:
                    text = f.read()
            except Exception:
                return ""
        
        return _nfkc(text) if text else ""
    
    def _is_business_report(self, text: str) -> bool:
        """æª¢æŸ¥æ˜¯å¦ç‚ºæ¥­å‹™æ—¥å ±æ ¼å¼"""
        indicators = ['Doc_Time:', 'TimeCreated:', 'Customer:', 'Worker:', 
                      'Content:', 'Manager:', 'Depart:', 'Doc_St:']
        count = sum(1 for ind in indicators if ind in text)
        return count >= 3
    
    def _process_as_business(self, text: str) -> List[Document]:
        """è™•ç†æ¥­å‹™å ±å‘Š"""
        records = BusinessReportProcessor.parse_report(text)
        documents = []
        
        for i, record in enumerate(records):
            content = self._format_business_record(record, i + 1)
            
            metadata = {
                'source': self.file_path,
                'doc_type': DocumentType.BUSINESS.value,
                'record_index': i + 1,
                'date': record.get('date', ''),
                'worker': record.get('worker', ''),
                'customer': record.get('customer', ''),
            }
            
            documents.append(Document(page_content=content, metadata=metadata))
        
        return documents
    
    def _format_business_record(self, record: Dict, index: int) -> str:
        """æ ¼å¼åŒ–æ¥­å‹™è¨˜éŒ„"""
        parts = [f"**è¨˜éŒ„ {index}**"]
        
        field_map = {
            'date': 'æ—¥æœŸ',
            'worker': 'æ¥­å‹™äººå“¡',
            'customer': 'å®¢æˆ¶',
            'class': 'æ´»å‹•é¡å‹',
            'content': 'æ´»å‹•å…§å®¹',
            'depart': 'éƒ¨é–€',
        }
        
        for key, label in field_map.items():
            if record.get(key):
                parts.append(f"**{label}**: {record[key]}")
        
        return "\n".join(parts)
    
    def _process_as_technical(self, text: str) -> List[Document]:
        """è™•ç†æŠ€è¡“æ–‡æª”"""
        # æå–ç”¢å“ä»£ç¢¼
        product_codes = self._extract_product_codes(text)
        
        # è­˜åˆ¥ä¸»è¦å“ç‰Œ
        brand = self._identify_brand(text, product_codes)
        
        # è­˜åˆ¥æ–‡æª”é¡å‹
        doc_category = self._identify_doc_category(text)
        
        # è™•ç†åœ–ç‰‡
        images_count = len(re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', text))
        
        # æ¸…æ´—å…§å®¹
        cleaned_text = self._clean_technical_content(text)
        
        # è™•ç†è¡¨æ ¼
        cleaned_text = self._process_tables(cleaned_text)
        
        # ChromaDB åªæ¥å— str/int/float/boolï¼Œå°‡ list è½‰ç‚ºé€—è™Ÿåˆ†éš”å­—ä¸²
        metadata = {
            'source': self.file_path,
            'doc_type': DocumentType.TECHNICAL.value,
            'product_codes': ','.join(product_codes) if product_codes else '',
            'brand': brand,
            'doc_category': doc_category,
            'images_count': images_count,
            'file_name': os.path.basename(self.file_path),
        }
        
        return [Document(page_content=cleaned_text, metadata=metadata)]
    
    def _identify_brand(self, text: str, product_codes: List[str]) -> str:
        """è­˜åˆ¥ä¸»è¦å“ç‰Œ"""
        text_lower = text.lower()
        
        # å„ªå…ˆæ ¹æ“šç”¢å“ä»£ç¢¼åˆ¤æ–·
        for code in product_codes:
            code_upper = code.upper()
            if code_upper.startswith(('MXJ', 'MXH', 'MXP', 'LES', 'LEH', 'ACG', 'ARG', 'AWG')):
                return 'SMC'
            if 'NO.' in code_upper or code_upper.startswith('N'):
                return 'VALQUA'
            if code_upper.startswith('GF'):
                return 'ç–åŸº'
            if code_upper.startswith('UF'):
                return 'å”é‹¼'
        
        # æ ¹æ“šå…§å®¹åˆ¤æ–·
        if 'smc' in text_lower or 'é€Ÿç¦å–œ' in text:
            return 'SMC'
        if 'valqua' in text_lower or 'ãƒãƒ«ã‚«ãƒ¼' in text or 'è¯çˆ¾å¡' in text:
            return 'VALQUA'
        if 'ç–åŸº' in text:
            return 'ç–åŸº'
        if 'å”é‹¼' in text:
            return 'å”é‹¼'
        
        return 'Unknown'
    
    def _identify_doc_category(self, text: str) -> str:
        """è­˜åˆ¥æ–‡æª”é¡åˆ¥"""
        if 'è¦æ ¼' in text or 'ä»•æ§˜' in text or 'specification' in text.lower():
            return 'specification'
        if 'å–ä»˜' in text or 'å®‰è£' in text or 'install' in text.lower():
            return 'installation'
        if 'å¯¸æ³•' in text or 'å°ºå¯¸' in text or 'dimension' in text.lower():
            return 'dimension'
        if 'ã‚«ã‚¿ãƒ­ã‚°' in text or 'å‹éŒ„' in text or 'catalog' in text.lower():
            return 'catalog'
        return 'general'
    
    def _clean_technical_content(self, text: str) -> str:
        """æ¸…æ´—æŠ€è¡“æ–‡æª”å…§å®¹"""
        # ç§»é™¤åœ–ç‰‡ HTML æ¨™ç±¤ä½†ä¿ç•™æè¿°
        text = re.sub(r'<img[^>]*src="([^"]*)"[^>]*/>', r'[åœ–: \1]', text)
        
        # ç§»é™¤ç©ºçš„ blockquote
        text = re.sub(r'>\s*\n', '\n', text)
        
        # ç§»é™¤éå¤šç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # ç§»é™¤é ç¢¼
        text = re.sub(r'^\s*\d+\s*$', '', text, flags=re.MULTILINE)
        
        return text.strip()
    
    def _extract_product_codes(self, text: str) -> List[str]:
        """æå–ç”¢å“ä»£ç¢¼"""
        codes = set()
        for pattern in self.PRODUCT_PATTERNS:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for m in matches:
                if isinstance(m, tuple):
                    codes.update(c for c in m if c)
                else:
                    codes.add(m)
        return list(codes)[:20]
    
    def _process_tables(self, text: str) -> str:
        """è™•ç† HTML è¡¨æ ¼è½‰ç‚º Markdown"""
        # ç°¡åŒ–è™•ç†ï¼šç§»é™¤ HTML æ¨™ç±¤
        text = re.sub(r'</?table[^>]*>', '', text)
        text = re.sub(r'</?tr[^>]*>', '\n', text)
        text = re.sub(r'</?t[hd][^>]*>', ' | ', text)
        text = re.sub(r'</?tbody[^>]*>', '', text)
        text = re.sub(r'</?thead[^>]*>', '', text)
        return text
